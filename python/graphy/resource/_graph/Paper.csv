id|published|year|month|title|authors|summary|journal_ref|doi|primary_category|categories|bib|node_type
1605.08803v3|2017-03-01T01:19:42+00:00|2017|3|DENSITY ESTIMATION USING REAL NVP|['Laurent Dinh', 'Jascha Sohl-Dickstein', 'Samy Bengio']|Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.|||cs.LG|['cs.LG', 'cs.AI', 'cs.NE', 'stat.ML']|@article{1605.08803v3,\nAuthor        = {Laurent Dinh and Jascha Sohl-Dickstein and Samy Bengio},\nTitle         = {Density estimation using Real NVP},\nEprint        = {http://arxiv.org/abs/1605.08803v3},\nArchivePrefix = {arXiv},\nPrimaryClass  = {cs.LG},\nAbstract      = {Unsupervised learning of probabilistic models is a central yet challenging\nproblem in machine learning. Specifically, designing models with tractable\nlearning, sampling, inference and evaluation is crucial in solving this task.\nWe extend the space of such models using real-valued non-volume preserving\n(real NVP) transformations, a set of powerful invertible and learnable\ntransformations, resulting in an unsupervised learning algorithm with exact\nlog-likelihood computation, exact sampling, exact inference of latent\nvariables, and an interpretable latent space. We demonstrate its ability to\nmodel natural images on four datasets through sampling, log-likelihood\nevaluation and latent variable manipulations.},\nYear          = {2016},\nMonth         = {5},\nUrl           = {http://arxiv.org/pdf/1605.08803v3},\nFile          = {1605.08803v3.pdf}\n}|Fact
2202.01511v3|2023-01-30T02:15:38+00:00|2023|1|Challenging Common Assumptions in ConvexReinforcement Learning|['Mirco Mutti', 'Riccardo De Santi', 'Piersilvio De Bartolomeis', 'Marcello Restelli']|The classic Reinforcement Learning (RL) formulation concerns the maximization of a scalar reward function. More recently, convex RL has been introduced to extend the RL formulation to all the objectives that are convex functions of the state distribution induced by a policy. Notably, convex RL covers several relevant applications that do not fall into the scalar formulation, including imitation learning, risk-averse RL, and pure exploration. In classic RL, it is common to optimize an infinite trials objective, which accounts for the state distribution instead of the empirical state visitation frequencies, even though the actual number of trajectories is always finite in practice. This is theoretically sound since the infinite trials and finite trials objectives can be proved to coincide and thus lead to the same optimal policy. In this paper, we show that this hidden assumption does not hold in the convex RL setting. In particular, we show that erroneously optimizing the infinite trials objective in place of the actual finite trials one, as it is usually done, can lead to a significant approximation error. Since the finite trials setting is the default in both simulated and real-world RL, we believe shedding light on this issue will lead to better approaches and methodologies for convex RL, impacting relevant research areas such as imitation learning, risk-averse RL, and pure exploration among others.|||cs.LG|['cs.LG']|@article{2202.01511v3,\nAuthor        = {Mirco Mutti and Riccardo De Santi and Piersilvio De Bartolomeis and Marcello Restelli},\nTitle         = {Challenging Common Assumptions in Convex Reinforcement Learning},\nEprint        = {http://arxiv.org/abs/2202.01511v3},\nArchivePrefix = {arXiv},\nPrimaryClass  = {cs.LG},\nAbstract      = {The classic Reinforcement Learning (RL) formulation concerns the maximization\nof a scalar reward function. More recently, convex RL has been introduced to\nextend the RL formulation to all the objectives that are convex functions of\nthe state distribution induced by a policy. Notably, convex RL covers several\nrelevant applications that do not fall into the scalar formulation, including\nimitation learning, risk-averse RL, and pure exploration. In classic RL, it is\ncommon to optimize an infinite trials objective, which accounts for the state\ndistribution instead of the empirical state visitation frequencies, even though\nthe actual number of trajectories is always finite in practice. This is\ntheoretically sound since the infinite trials and finite trials objectives can\nbe proved to coincide and thus lead to the same optimal policy. In this paper,\nwe show that this hidden assumption does not hold in the convex RL setting. In\nparticular, we show that erroneously optimizing the infinite trials objective\nin place of the actual finite trials one, as it is usually done, can lead to a\nsignificant approximation error. Since the finite trials setting is the default\nin both simulated and real-world RL, we believe shedding light on this issue\nwill lead to better approaches and methodologies for convex RL, impacting\nrelevant research areas such as imitation learning, risk-averse RL, and pure\nexploration among others.},\nYear          = {2022},\nMonth         = {2},\nUrl           = {http://arxiv.org/pdf/2202.01511v3},\nFile          = {2202.01511v3.pdf}\n}|Fact
2001.11785v2|2020-02-04T01:38:42+00:00|2020|2|A Deep Reinforcement Learning Approach to Concurrent Bilateral Negotiation|['Pallavi Bagga', 'Nicola Paoletti', 'Bedour Alrayes', 'Kostas Stathis']|We present a novel negotiation model that allows an agent to learn how to negotiate during concurrent bilateral negotiations in unknown and dynamic e-markets. The agent uses an actor-critic architecture with model-free reinforcement learning to learn a strategy expressed as a deep neural network. We pre-train the strategy by supervision from synthetic market data, thereby decreasing the exploration time required for learning during negotiation. As a result, we can build automated agents for concurrent negotiations that can adapt to different e-market settings without the need to be pre-programmed. Our experimental evaluation shows that our deep reinforcement learning-based agents outperform two existing well-known negotiation strategies in one-to-many concurrent bilateral negotiations for a range of e-market settings.|||cs.MA|['cs.MA', 'cs.AI']|@article{2001.11785v2,\nAuthor        = {Pallavi Bagga and Nicola Paoletti and Bedour Alrayes and Kostas Stathis},\nTitle         = {A Deep Reinforcement Learning Approach to Concurrent Bilateral Negotiation},\nEprint        = {http://arxiv.org/abs/2001.11785v2},\nArchivePrefix = {arXiv},\nPrimaryClass  = {cs.MA},\nAbstract      = {We present a novel negotiation model that allows an agent to learn how to\nnegotiate during concurrent bilateral negotiations in unknown and dynamic\ne-markets. The agent uses an actor-critic architecture with model-free\nreinforcement learning to learn a strategy expressed as a deep neural network.\nWe pre-train the strategy by supervision from synthetic market data, thereby\ndecreasing the exploration time required for learning during negotiation. As a\nresult, we can build automated agents for concurrent negotiations that can\nadapt to different e-market settings without the need to be pre-programmed. Our\nexperimental evaluation shows that our deep reinforcement learning-based agents\noutperform two existing well-known negotiation strategies in one-to-many\nconcurrent bilateral negotiations for a range of e-market settings.},\nYear          = {2020},\nMonth         = {1},\nUrl           = {http://arxiv.org/pdf/2001.11785v2},\nFile          = {2001.11785v2.pdf}\n}|Fact
2407.00567v1|2024-07-02T13:37:10+00:00|2024|7|A Contextual Combinatorial Bandit Approach to Negotiation|['Yexin Li', 'Zhancun Mu', 'Siyuan Qi']|Learning effective negotiation strategies poses two key challenges: the exploration-exploitation dilemma and dealing with large action spaces. However, there is an absence of learning-based approaches that effectively address these challenges in negotiation. This paper introduces a comprehensive formulation to tackle various negotiation problems. Our approach leverages contextual combinatorial multi-armed bandits, with the bandits resolving the exploration-exploitation dilemma, and the combinatorial nature handles large action spaces. Building upon this formulation, we introduce NegUCB, a novel method that also handles common issues such as partial observations and complex reward functions in negotiation. NegUCB is contextual and tailored for full-bandit feedback without constraints on the reward functions. Under mild assumptions, it ensures a sub-linear regret upper bound. Experiments conducted on three negotiation tasks demonstrate the superiority of our approach.|||cs.AI|['cs.AI', 'cs.LG']|@article{2407.00567v1,\nAuthor        = {Yexin Li and Zhancun Mu and Siyuan Qi},\nTitle         = {A Contextual Combinatorial Bandit Approach to Negotiation},\nEprint        = {http://arxiv.org/abs/2407.00567v1},\nArchivePrefix = {arXiv},\nPrimaryClass  = {cs.AI},\nAbstract      = {Learning effective negotiation strategies poses two key challenges: the\nexploration-exploitation dilemma and dealing with large action spaces. However,\nthere is an absence of learning-based approaches that effectively address these\nchallenges in negotiation. This paper introduces a comprehensive formulation to\ntackle various negotiation problems. Our approach leverages contextual\ncombinatorial multi-armed bandits, with the bandits resolving the\nexploration-exploitation dilemma, and the combinatorial nature handles large\naction spaces. Building upon this formulation, we introduce NegUCB, a novel\nmethod that also handles common issues such as partial observations and complex\nreward functions in negotiation. NegUCB is contextual and tailored for\nfull-bandit feedback without constraints on the reward functions. Under mild\nassumptions, it ensures a sub-linear regret upper bound. Experiments conducted\non three negotiation tasks demonstrate the superiority of our approach.},\nYear          = {2024},\nMonth         = {6},\nUrl           = {http://arxiv.org/pdf/2407.00567v1},\nFile          = {2407.00567v1.pdf}\n}|Fact
-1452618514829550696|2023-01-24T01:58:25+00:00|2023|1|A Context-Integrated Transformer-Based Neural Network forAuction Design|[]|||||[]||Fact
neural2019durkan|2019-12-03T01:26:59+00:00|2019|12|Neural Spline Flows|['Durkan, Conor, Artur Bekasov, Iain Murray, and George Papamakarios. ']|… , we refer to the resulting class of normalizing flows as rational-quadratic neural spline flows  (RQ-NSF), … [22], and the universal approximation capabilities of neural networks in general. …|Advances in neural information processing systems, 2019||||@article{durkan2019neural,\n  title={Neural spline flows},\n  author={Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},\n  journal={Advances in neural information processing systems},\n  volume={32},\n  year={2019}\n}|Fact
2006.04678v2|2021-03-18T02:30:37+00:00|2021|3|PRIMAL WASSERSTEIN IMITATION LEARNING|['Robert Dadashi', 'Léonard Hussenot', 'Matthieu Geist', 'Olivier Pietquin']|Imitation Learning (IL) methods seek to match the behavior of an agent with that of an expert. In the present work, we propose a new IL method based on a conceptually simple algorithm: Primal Wasserstein Imitation Learning (PWIL), which ties to the primal form of the Wasserstein distance between the expert and the agent state-action distributions. We present a reward function which is derived offline, as opposed to recent adversarial IL algorithms that learn a reward function through interactions with the environment, and which requires little fine-tuning. We show that we can recover expert behavior on a variety of continuous control tasks of the MuJoCo domain in a sample efficient manner in terms of agent interactions and of expert interactions with the environment. Finally, we show that the behavior of the agent we train matches the behavior of the expert with the Wasserstein distance, rather than the commonly used proxy of performance.|||cs.LG|['cs.LG', 'stat.ML']|@article{2006.04678v2,\nAuthor        = {Robert Dadashi and Léonard Hussenot and Matthieu Geist and Olivier Pietquin},\nTitle         = {Primal Wasserstein Imitation Learning},\nEprint        = {http://arxiv.org/abs/2006.04678v2},\nArchivePrefix = {arXiv},\nPrimaryClass  = {cs.LG},\nAbstract      = {Imitation Learning (IL) methods seek to match the behavior of an agent with\nthat of an expert. In the present work, we propose a new IL method based on a\nconceptually simple algorithm: Primal Wasserstein Imitation Learning (PWIL),\nwhich ties to the primal form of the Wasserstein distance between the expert\nand the agent state-action distributions. We present a reward function which is\nderived offline, as opposed to recent adversarial IL algorithms that learn a\nreward function through interactions with the environment, and which requires\nlittle fine-tuning. We show that we can recover expert behavior on a variety of\ncontinuous control tasks of the MuJoCo domain in a sample efficient manner in\nterms of agent interactions and of expert interactions with the environment.\nFinally, we show that the behavior of the agent we train matches the behavior\nof the expert with the Wasserstein distance, rather than the commonly used\nproxy of performance.},\nYear          = {2020},\nMonth         = {6},\nUrl           = {http://arxiv.org/pdf/2006.04678v2},\nFile          = {2006.04678v2.pdf}\n}|Fact
2305.00303v1|2023-05-02T00:53:33+00:00|2023|5|A Coupled Flow Approach to Imitation Learning|['Gideon Freund', 'Elad Sarafian', 'Sarit Kraus']|In reinforcement learning and imitation learning, an object of central importance is the state distribution induced by the policy. It plays a crucial role in the policy gradient theorem, and references to it--along with the related state-action distribution--can be found all across the literature. Despite its importance, the state distribution is mostly discussed indirectly and theoretically, rather than being modeled explicitly. The reason being an absence of appropriate density estimation tools. In this work, we investigate applications of a normalizing flow-based model for the aforementioned distributions. In particular, we use a pair of flows coupled through the optimality point of the Donsker-Varadhan representation of the Kullback-Leibler (KL) divergence, for distribution matching based imitation learning. Our algorithm, Coupled Flow Imitation Learning (CFIL), achieves state-of-the-art performance on benchmark tasks with a single expert trajectory and extends naturally to a variety of other settings, including the subsampled and state-only regimes.|||cs.LG|['cs.LG', 'stat.ML']|@article{2305.00303v1,\nAuthor        = {Gideon Freund and Elad Sarafian and Sarit Kraus},\nTitle         = {A Coupled Flow Approach to Imitation Learning},\nEprint        = {http://arxiv.org/abs/2305.00303v1},\nArchivePrefix = {arXiv},\nPrimaryClass  = {cs.LG},\nAbstract      = {In reinforcement learning and imitation learning, an object of central\nimportance is the state distribution induced by the policy. It plays a crucial\nrole in the policy gradient theorem, and references to it--along with the\nrelated state-action distribution--can be found all across the literature.\nDespite its importance, the state distribution is mostly discussed indirectly\nand theoretically, rather than being modeled explicitly. The reason being an\nabsence of appropriate density estimation tools. In this work, we investigate\napplications of a normalizing flow-based model for the aforementioned\ndistributions. In particular, we use a pair of flows coupled through the\noptimality point of the Donsker-Varadhan representation of the Kullback-Leibler\n(KL) divergence, for distribution matching based imitation learning. Our\nalgorithm, Coupled Flow Imitation Learning (CFIL), achieves state-of-the-art\nperformance on benchmark tasks with a single expert trajectory and extends\nnaturally to a variety of other settings, including the subsampled and\nstate-only regimes.},\nYear          = {2023},\nMonth         = {4},\nUrl           = {http://arxiv.org/pdf/2305.00303v1},\nFile          = {2305.00303v1.pdf}\n}|Fact
1703.04691v5|2018-09-18T01:22:25+00:00|2018|9|Conditional time series forecasting with convolutional neural|['Anastasia Borovykh', 'Sander Bohte', 'Cornelis W. Oosterlee']|We present a method for conditional time series forecasting based on an adaptation of the recent deep convolutional WaveNet architecture. The proposed network contains stacks of dilated convolutions that allow it to access a broad range of history when forecasting, a ReLU activation function and conditioning is performed by applying multiple convolutional filters in parallel to separate time series which allows for the fast processing of data and the exploitation of the correlation structure between the multivariate time series. We test and analyze the performance of the convolutional network both unconditionally as well as conditionally for financial time series forecasting using the S&P500, the volatility index, the CBOE interest rate and several exchange rates and extensively compare it to the performance of the well-known autoregressive model and a long-short term memory network. We show that a convolutional network is well-suited for regression-type problems and is able to effectively learn dependencies in and between the series without the need for long historical time series, is a time-efficient and easy to implement alternative to recurrent-type networks and tends to outperform linear and recurrent models.|||stat.ML|['stat.ML']|@article{1703.04691v5,\nAuthor        = {Anastasia Borovykh and Sander Bohte and Cornelis W. Oosterlee},\nTitle         = {Conditional Time Series Forecasting with Convolutional Neural Networks},\nEprint        = {http://arxiv.org/abs/1703.04691v5},\nArchivePrefix = {arXiv},\nPrimaryClass  = {stat.ML},\nAbstract      = {We present a method for conditional time series forecasting based on an\nadaptation of the recent deep convolutional WaveNet architecture. The proposed\nnetwork contains stacks of dilated convolutions that allow it to access a broad\nrange of history when forecasting, a ReLU activation function and conditioning\nis performed by applying multiple convolutional filters in parallel to separate\ntime series which allows for the fast processing of data and the exploitation\nof the correlation structure between the multivariate time series. We test and\nanalyze the performance of the convolutional network both unconditionally as\nwell as conditionally for financial time series forecasting using the S&P500,\nthe volatility index, the CBOE interest rate and several exchange rates and\nextensively compare it to the performance of the well-known autoregressive\nmodel and a long-short term memory network. We show that a convolutional\nnetwork is well-suited for regression-type problems and is able to effectively\nlearn dependencies in and between the series without the need for long\nhistorical time series, is a time-efficient and easy to implement alternative\nto recurrent-type networks and tends to outperform linear and recurrent models.},\nYear          = {2017},\nMonth         = {3},\nUrl           = {http://arxiv.org/pdf/1703.04691v5},\nFile          = {1703.04691v5.pdf}\n}|Fact
1906.05274v3|2020-03-02T02:24:54+00:00|2020|3|Efﬁcient Exploration via State Marginal Matching|['Lisa Lee', 'Benjamin Eysenbach', 'Emilio Parisotto', 'Eric Xing', 'Sergey Levine', 'Ruslan Salakhutdinov']|Exploration is critical to a reinforcement learning agent's performance in its given environment. Prior exploration methods are often based on using heuristic auxiliary predictions to guide policy behavior, lacking a mathematically-grounded objective with clear properties. In contrast, we recast exploration as a problem of State Marginal Matching (SMM), where we aim to learn a policy for which the state marginal distribution matches a given target state distribution. The target distribution is a uniform distribution in most cases, but can incorporate prior knowledge if available. In effect, SMM amortizes the cost of learning to explore in a given environment. The SMM objective can be viewed as a two-player, zero-sum game between a state density model and a parametric policy, an idea that we use to build an algorithm for optimizing the SMM objective. Using this formalism, we further demonstrate that prior work approximately maximizes the SMM objective, offering an explanation for the success of these methods. On both simulated and real-world tasks, we demonstrate that agents that directly optimize the SMM objective explore faster and adapt more quickly to new tasks as compared to prior exploration methods.|||cs.LG|['cs.LG', 'cs.AI', 'cs.RO', 'stat.ML']|@article{1906.05274v3,\nAuthor        = {Lisa Lee and Benjamin Eysenbach and Emilio Parisotto and Eric Xing and Sergey Levine and Ruslan Salakhutdinov},\nTitle         = {Efficient Exploration via State Marginal Matching},\nEprint        = {http://arxiv.org/abs/1906.05274v3},\nArchivePrefix = {arXiv},\nPrimaryClass  = {cs.LG},\nAbstract      = {Exploration is critical to a reinforcement learning agent's performance in\nits given environment. Prior exploration methods are often based on using\nheuristic auxiliary predictions to guide policy behavior, lacking a\nmathematically-grounded objective with clear properties. In contrast, we recast\nexploration as a problem of State Marginal Matching (SMM), where we aim to\nlearn a policy for which the state marginal distribution matches a given target\nstate distribution. The target distribution is a uniform distribution in most\ncases, but can incorporate prior knowledge if available. In effect, SMM\namortizes the cost of learning to explore in a given environment. The SMM\nobjective can be viewed as a two-player, zero-sum game between a state density\nmodel and a parametric policy, an idea that we use to build an algorithm for\noptimizing the SMM objective. Using this formalism, we further demonstrate that\nprior work approximately maximizes the SMM objective, offering an explanation\nfor the success of these methods. On both simulated and real-world tasks, we\ndemonstrate that agents that directly optimize the SMM objective explore faster\nand adapt more quickly to new tasks as compared to prior exploration methods.},\nYear          = {2019},\nMonth         = {6},\nUrl           = {http://arxiv.org/pdf/1906.05274v3},\nFile          = {1906.05274v3.pdf}\n}|Fact
2006.08545v1|2020-06-16T00:39:41+00:00|2020|6|Why Normalizing Flows Fail to DetectOut-of-Distribution Data|['Polina Kirichenko', 'Pavel Izmailov', 'Andrew Gordon Wilson']|Detecting out-of-distribution (OOD) data is crucial for robust machine learning systems. Normalizing flows are flexible deep generative models that often surprisingly fail to distinguish between in- and out-of-distribution data: a flow trained on pictures of clothing assigns higher likelihood to handwritten digits. We investigate why normalizing flows perform poorly for OOD detection. We demonstrate that flows learn local pixel correlations and generic image-to-latent-space transformations which are not specific to the target image dataset. We show that by modifying the architecture of flow coupling layers we can bias the flow towards learning the semantic structure of the target data, improving OOD detection. Our investigation reveals that properties that enable flows to generate high-fidelity images can have a detrimental effect on OOD detection.|||stat.ML|['stat.ML', 'cs.LG']|@article{2006.08545v1,\nAuthor        = {Polina Kirichenko and Pavel Izmailov and Andrew Gordon Wilson},\nTitle         = {Why Normalizing Flows Fail to Detect Out-of-Distribution Data},\nEprint        = {http://arxiv.org/abs/2006.08545v1},\nArchivePrefix = {arXiv},\nPrimaryClass  = {stat.ML},\nAbstract      = {Detecting out-of-distribution (OOD) data is crucial for robust machine\nlearning systems. Normalizing flows are flexible deep generative models that\noften surprisingly fail to distinguish between in- and out-of-distribution\ndata: a flow trained on pictures of clothing assigns higher likelihood to\nhandwritten digits. We investigate why normalizing flows perform poorly for OOD\ndetection. We demonstrate that flows learn local pixel correlations and generic\nimage-to-latent-space transformations which are not specific to the target\nimage dataset. We show that by modifying the architecture of flow coupling\nlayers we can bias the flow towards learning the semantic structure of the\ntarget data, improving OOD detection. Our investigation reveals that properties\nthat enable flows to generate high-fidelity images can have a detrimental\neffect on OOD detection.},\nYear          = {2020},\nMonth         = {6},\nUrl           = {http://arxiv.org/pdf/2006.08545v1},\nFile          = {2006.08545v1.pdf}\n}|Fact
-199099092534379852|2024-04-19T00:02:28+00:00|2024|4|A DECODER-ONLY FOUNDATION MODEL FOR TIME-SERIESFORECASTING|[]|||||[]||Fact
