id|node_type|original|summary
dafaeaff-9ce6-59af-8ffb-b6136efaf754|Dimension|In this paper, we formalize the notion of a finite trials RL problem, in which the objective is a function of the empirical state distribution induced by the agent's policy over \( n \) trials rather than its expectation over infinite trials.|Formalization of finite trials RL problem.
ed3eebff-c8a2-5df1-8fed-2f980cd47bec|Dimension|We show that this property does not hold for the convex RL formulation: a policy optimized over infinite trials can be significantly sub-optimal when deployed over finite trials.|Demonstration of sub-optimality in convex RL under finite trials.
5e9e2736-05d1-5535-803d-9d0fad4cbab6|Dimension|In particular, we prove that erroneously optimizing the infinite trials objective in place of the actual finite trials one, as it is usually done, can lead to a significant approximation error.|Proof of significant approximation error in convex RL when using infinite trials objective.
d16ed1a2-5e9b-5601-8bee-39aa4b0b0fa6|Dimension|We provide the finite trial convex RL formulation, for which we prove an upper bound on the approximation error made by optimizing the infinite trials as a proxy of the finite trials objective.|Development of finite trial convex RL formulation and proof of approximation error bounds.
b354f118-899b-5d8b-95af-a073a9d76ae9|Dimension|We corroborate this result with an additional numerical analysis showing that the approximation bound is non-vacuous for relevant applications.|Numerical validation of the theoretical findings.
0063ae72-44ee-5839-ae5c-33bc7b38c5f7|Dimension|Finally, in Section 5 we include an in-depth analysis of the single trial convex RL, which suggests that other common assumptions in the convex RL literature, i.e., that 2 the problem is always computationally tractable and that 3 stationary policies are in general sufficient, should be reconsidered as well.|Reconsideration of common assumptions in convex RL based on single trial analysis.
50291aaa-dc80-53d3-b464-326584db4683|Dimension|we propose a novel agent model for one to many concurrent bilateral negotiations in open, dynamic and unknown e market settings.|Novel agent model for concurrent negotiations.
72857c81-2695-59b8-908f-b0eb58a05a59|Dimension|we extend the existing simulation environment Alrayes et al., 2016 to generate data and perform experiments that support agent learning for negotiation.|Extension of simulation environment for data generation and experiments.
4d8612b1-822a-5c8f-8286-8123ac512098|Dimension|we run an extensive experiments showing that our approach outperforms the existing strategies and produces adaptable agents that can transfer to a range of e market settings.|Extensive experiments demonstrating superior performance and adaptability.
0934c7d4-1883-5643-879a-13c038d2fb9a|Dimension|First, we provide a comprehensive formulation for diverse types of negotiation problems in 3.1.|Comprehensive formulation for various negotiation problems.
dc72f852-5161-58ee-910b-ce0dda55652e|Dimension|Second, we propose a contextual algorithm for full bandit feedback, named Negotiation UCB (NegUCB), to learn negotiation strategies and adeptly address the exploitation-exploration dilemma and the challenge of large action spaces. Moreover, NegUCB incorporates hidden states to tackle the issue of partial observations and handles diverse acceptance functions through kernel regression.|Proposal of NegUCB, a contextual algorithm for full bandit feedback, addressing key negotiation challenges.
cfbaf922-56d8-5fe4-904f-081da37783d0|Dimension|Under mild assumptions, NegUCB's regret upper bound is guaranteed to be sub-linear with respect to the number of negotiation steps and independent of the bid cardinality, distinguishing itself from existing works on either semi-bandit or full bandit feedback.|Theoretical guarantee of sub-linear regret for NegUCB, independent of bid cardinality.
75560cfe-e0e7-56c1-b687-4248d59dd90b|Dimension|Experiments conducted on three negotiation tasks highlight the advantages and effectiveness of our method.|Empirical validation of NegUCB's effectiveness through experiments on diverse negotiation tasks.
58e2889b-2ca3-591d-a644-d5a744517f6b|Dimension|We propose CITransNet, a context integrated transformer based neural network for optimal auction design, which maintains permutation equivariance over bids and contexts while being able to find asymmetric solutions.|Propose CITransNet, a permutation-equivariant neural network for optimal auction design.
b0270ad6-491a-50db-bf8a-5a7a019eed27|Dimension|We show by extensive experiments that CITransNet can recover the known optimal solutions in single item settings, outperform strong baselines in multi-item auctions, and generalize well to cases other than those in training.|Demonstrate CITransNet's effectiveness in recovering optimal solutions and outperforming baselines in various auction settings.
acc02b68-15f3-5e6a-bcc4-ed55ca0ff9af|Dimension|To overcome the aforementioned limitations of the previous works, we propose CITransNet: a Context Integrated Transformer based neural Network architecture as the parameterized mechanism to be optimized.|Address limitations of previous works by introducing CITransNet as a parameterized mechanism.
10c20c03-3c90-5412-9bb4-dfef6d5ceffd|Dimension|We formulate the contextual auction design as a learning problem and extend the learning framework to incorporate public contextual information of bidders and items into the auction learning framework.|Formulate contextual auction design as a learning problem and incorporate public contextual information.
0f5c1fda-4826-5851-80aa-f92eb251cb14|Dimension|Furthermore, we present a sample complexity result to bound the generalization error of the learned mechanism.|Provide a sample complexity result to bound the generalization error of the learned mechanism.
046398ae-22b8-5e21-afb8-49d26efd2917|Dimension|We propose a fully differentiable module based on monotonic rational quadratic splines, which enhances the flexibility of both coupling and autoregressive transforms while retaining analytic invertibility.|Introduced a new module using monotonic rational quadratic splines that increases model flexibility and maintains analytical invertibility.
8c4df0fc-4622-5242-8c2b-953b20ebe3d6|Dimension|We demonstrate that neural spline flows improve density estimation, variational inference, and generative modeling of images.|Showed empirical improvements in density estimation, variational inference, and image generation using neural spline flows.
79944cff-ce3e-5280-a13e-2da507325bc6|Dimension|The module acts as a drop-in replacement for the affine or additive transformations commonly found in coupling and autoregressive transforms.|Provided a direct replacement for existing transformations in coupling and autoregressive models, enhancing their capabilities without altering the overall architecture.
28acda9a-a135-5c1d-9e26-78e669b9b59c|Dimension|Monotonic rational quadratic splines naturally induce multi-modality when used to transform random variables.|Highlighted the ability of the proposed splines to introduce multi-modality in transformed distributions, improving the model's representational power.
ca926ac4-81b7-553b-af26-f2bb495aec84|Dimension|In this work, we use the Wasserstein distance as a measure between the state action distributions of the expert and of the agent. Contrary to f-divergences, the Wasserstein distance is a true distance, it is smooth and it is based on the geometry of the metric space it operates on.|Utilizes Wasserstein distance for measuring expert-agent similarity, emphasizing its properties over f-divergences.
4d8da9b3-6454-59e4-88a0-e0ba550bc678|Dimension|We propose a new IL method based on a conceptually simple algorithm: Primal Wasserstein Imitation Learning (PWIL), which ties to the primal form of the Wasserstein distance between the expert and the agent state-action distributions.|Introduces PWIL, a new imitation learning method based on the primal form of the Wasserstein distance.
7cf62f90-4f8e-52db-bfa5-dfab36ca105e|Dimension|We present a reward function which is derived offline, as opposed to recent adversarial IL algorithms that learn a reward function through interactions with the environment, and which requires little fine-tuning.|Develops an offline reward function, reducing the need for fine-tuning compared to adversarial IL methods.
7e4afdcf-aa78-5ae7-a140-ee86d7f1c857|Dimension|We show that we can recover expert behavior on a variety of continuous control tasks of the MuJoCo domain in a sample-efficient manner in terms of agent interactions and of expert interactions with the environment.|Demonstrates effectiveness in recovering expert behavior across various tasks with high sample efficiency.
239f87ef-8851-567c-9bfd-3bae04270bc7|Dimension|Finally, we show that the behavior of the agent we train matches the behavior of the expert with the Wasserstein distance, rather than the commonly used proxy of performance.|Validates that the trained agent's behavior closely matches the expert's, measured by the Wasserstein distance, not just performance metrics.
110bc106-52eb-5866-b254-1bae13bc575b|Dimension|In this work, we investigate applications of a normalizing flow-based model for the aforementioned distributions. In particular, we use a pair of flows coupled through the optimality point of the Donsker Varadhan representation of the Kullback-Leibler (KL) divergence, for distribution matching based imitation learning.|Propose a novel method using coupled normalizing flows for distribution matching in imitation learning.
f4bf81df-2a6e-5b45-9e66-0ad833f65ecc|Dimension|Our algorithm, Coupled Flow Imitation Learning (CFIL), achieves state-of-the-art performance on standard benchmark tasks with a single expert trajectory and extends naturally to a variety of other settings, including the subsampled and state-only regimes.|Demonstrate superior performance of CFIL on benchmarks and its adaptability to different learning scenarios.
76032743-8051-5703-9dda-797732801e50|Dimension|We show this in part by analyzing their respective BC graphs: a simple tool we present for gauging how well a proposed estimator captures the true distribution.|Introduce BC graphs as a tool to evaluate the effectiveness of distribution estimators.
b4bcf5d3-2a88-5965-bc2e-037c72c4bf8c|Dimension|While most IL works neglect analysis of their learned reward function, we think this can be a potential guiding tool for future IL researchers.|Highlight the importance of analyzing learned reward functions and suggest it as a valuable tool for future research.
456b740c-8584-544c-a016-1dc8580c2836|Dimension|This work also aims to inspire more research incorporating explicit modeling of the state-action distribution.|Encourage further research into explicit modeling of state-action distributions in imitation learning.
7fb8e8ef-73af-5aea-9eef-48b32409194f|Dimension|First of all, we present a CNN inspired by the WaveNet model, with a structure that is simplified and optimized for time series forecasting, i.e., using a ReLU activation and a novel and more optimal way of conditioning with parametrized skip connections.|Simplified and optimized CNN structure for time series forecasting.
436ceef7-cb4b-5672-b4cf-7f0d883eab0b|Dimension|Second, knowing the strong performance of CNNs on classification problems, our work is to the best of our knowledge the first to show that they can be applied successfully to forecasting financial time series of limited length.|First application of CNNs to forecasting short financial time series.
de5416e4-73e0-5da0-9119-ddc11ab89ae0|Dimension|By conducting an extensive analysis of the WaveNet model and comparing the performance to that of an LSTM, the current state of the art in forecasting, and an autoregressive model popular in econometrics our paper shows that the WaveNet model is a time efficient and easy to implement alternative to recurrent type networks.|Comprehensive performance comparison with LSTM and autoregressive models.
8dfcf897-8d70-548d-b783-6828a7ae5e6c|Dimension|Furthermore, the gated activation function from the original WaveNet model is replaced by a rectified linear unit (ReLU), simplifying the model and reducing the training time.|Replaced gated activation with ReLU for simplicity and efficiency.
ff67de69-f7b3-510f-a0be-aed94e7580d4|Dimension|Effectively, we use multiple financial time series as input in a neural network, thus conditioning the forecast of a time series on both its own history as well as that of multiple other time series.|Utilizes multiple time series for conditioning forecasts.
ea543eca-47c7-53c4-96e8-a6a1bdbdd4b1|Dimension|Lastly, we show using examples on artificial time series as well as the S&P500, VIX, CBOE interest rate, and five exchange rates that the efficient way of conditioning in the WaveNet model enables one to extract temporal relationships in between time series improving the forecast, while at the same time limiting the requirement for a long historical price series.|Demonstrates improved forecasting with limited historical data through efficient conditioning.
2f3087be-5334-5674-b4df-55558f5223ff|Dimension|We argue that the SMM objective is an effective way to learn a single, task agnostic exploration policy that can be used for solving many downstream tasks, amortizing the cost of learning to explore for each task.|Proposes a single, task-agnostic exploration policy using the SMM objective.
bcea6075-d366-5b4b-a868-2b0f53310e60|Dimension|By viewing the objective as a two player, zero sum game between a state density model and a parametric policy, we propose a practical algorithm to jointly learn the policy and the density by using fictitious play.|Develops a practical algorithm for optimizing the SMM objective using fictitious play.
02b1005e-e413-59a1-885e-65baf30c1b52|Dimension|We further decompose the SMM objective into a mixture of distributions, and derive an algorithm for learning a mixture of policies that resembles the mutual information objectives in recent work.|Decomposes the SMM objective and derives an algorithm for learning a mixture of policies.
cd10df18-376c-5852-b905-e866dc5d5efc|Dimension|On both simulated and real-world tasks, we demonstrate that agents that directly optimize the SMM objective explore faster and adapt more quickly to new tasks as compared to prior exploration methods.|Demonstrates superior exploration and adaptation performance of the SMM approach in various tasks.
2fc7f041-ae0d-569b-9241-13d9892bb38d|Dimension|The SMM objective can be viewed as a two player, zero sum game between a state density model and a parametric policy, an idea that we use to build an algorithm for optimizing the SMM objective.|Provides a game-theoretic perspective on the SMM objective and its optimization.
561afb8c-ea52-5cb4-8e23-b24186f6cdaa|Dimension|We show that exploration methods based on predictive error approximately maximize the SMM objective, offering an explanation for the success of these methods.|Explains the success of predictive error-based exploration methods through the lens of the SMM objective.
6b534ab8-5163-5422-86fb-09d8d96bd800|Dimension|We show that flows learn latent representations for images largely based on local pixel correlations, rather than semantic content, making it difficult to detect data with anomalous semantics.|Flows learn local pixel correlations over semantic content, hindering OOD detection.
3f1264e5-ab4d-556c-b941-a2a759dfcb33|Dimension|We identify mechanisms through which normalizing flows can simultaneously increase likelihood for all structured images.|Mechanisms are identified that allow flows to increase likelihood for structured images, affecting OOD detection.
ea41d384-0fb0-51f1-90be-d4da4d3ba974|Dimension|We demonstrate that flows learn local pixel correlations and generic image to latent space transformations which are not specific to the target image dataset.|Flows learn generic transformations, not specific to the target dataset.
d52419d4-c877-5e7a-990b-16267c112401|Dimension|We show that by modifying the architecture of flow coupling layers we can bias the flow towards learning the semantic structure of the target data, improving OOD detection.|Modifying coupling layers can improve OOD detection by learning semantic structures.
3726958a-9dc0-594d-a312-d6b69ddedf4c|Dimension|Our investigation reveals that properties that enable flows to generate high fidelity images can have a detrimental effect on OOD detection.|Properties enabling high-fidelity image generation can negatively impact OOD detection.
7f60bb00-8ae6-5ced-99e9-fe87475e454c|Dimension|We design a time series foundation model for forecasting that, when applied to a variety of previously unseen forecasting datasets across different domains, obtains close to state-of-the-art zero-shot accuracy compared to the best supervised models trained individually for these datasets.|Design of a time series foundation model (TimesFM) achieving near state-of-the-art zero-shot forecasting accuracy.
81e4b9f3-97fd-5835-809c-236f98a90f9d|Dimension|Our model is based on pretraining a decoder-style attention architecture with input patching, using a large time series corpus comprising both real-world and synthetic datasets.|Development of a decoder-style attention architecture with input patching, trained on a large and diverse time series corpus.
4e467d12-19f4-5717-837a-b04f6eac668b|Dimension|Compared to the latest large language models, our time series foundation model is much smaller in both parameter size (200M parameters) and pretraining data size (O(100B timepoints); yet we show that even at such scales, it is possible to pretrain a practical foundation model for forecasting whose zero-shot performance comes close to the accuracy of fully supervised approaches on a diverse set of time series data.|Demonstration that a smaller, more efficient model can achieve competitive zero-shot forecasting performance.
731fd560-6999-5370-b886-f5d1ca982907|Dimension|Our work also suggests that unlike recent work that recommends Large Language Models such as GPT-3 and LLama 2 as out-of-the-box zero-shot forecasters, foundation models trained from scratch exclusively on time series data can obtain much better zero-shot performance at a tiny fraction of its costs.|Evidence that specialized time series foundation models outperform general large language models in zero-shot forecasting with lower computational costs.
