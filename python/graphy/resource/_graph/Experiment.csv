id|node_type|name|settings|results
dafaeaff-9ce6-59af-8ffb-b6136efaf754|Dimension|Finite Trials vs. Infinite Trials Objective|Datasets: Simulated environments (e.g., GridWorld, CartPole) and real-world datasets (if applicable). Evaluation Metrics: Policy performance measured by the average reward, convergence rate, and stability. Baselines: Traditional RL algorithms (e.g., Q-learning, SARSA) and convex RL algorithms that assume infinite trials.|The finite trials objective showed a significant performance gap compared to the infinite trials objective, especially in environments with limited data. The policies optimized using the finite trials objective were more robust and had better generalization capabilities.
ed3eebff-c8a2-5df1-8fed-2f980cd47bec|Dimension|Impact of State Distribution on Policy Optimization|Datasets: A variety of environments with different state distributions (e.g., high-dimensional continuous spaces, discrete states). Evaluation Metrics: Policy performance, convergence speed, and robustness to changes in the environment. Baselines: Standard convex RL algorithms and non-convex RL algorithms.|Policies optimized using the state distribution-based objective showed improved performance and faster convergence in environments with complex state distributions. The improvement was particularly notable in high-dimensional and continuous state spaces.
5e9e2736-05d1-5535-803d-9d0fad4cbab6|Dimension|Application to Risk-Averse RL|Datasets: Environments with stochastic rewards and potential risks (e.g., financial trading, autonomous driving). Evaluation Metrics: Risk measures (e.g., Value at Risk, Conditional Value at Risk), reward stability, and policy robustness. Baselines: Traditional risk-averse RL algorithms and standard convex RL algorithms.|The proposed finite trials objective outperformed traditional risk-averse RL algorithms in terms of risk management and reward stability. The policies were more robust to unexpected changes in the environment, leading to better long-term performance.
50291aaa-dc80-53d3-b464-326584db4683|Dimension|Performance in One-to-Many Concurrent Bilateral Negotiations|Dataset: Synthetic market data used for pre-training and real-world e-market scenarios for testing. Environment: Simulated e-market environment where the agent engages in multiple concurrent bilateral negotiations. Baselines: Two existing well-known negotiation strategies: Concession-based Strategy, Tit-for-Tat Strategy.|The DRL-based agents achieved a higher negotiation success rate compared to both baseline strategies. The utility gain was significantly higher for the DRL agents, indicating better outcomes in terms of negotiation quality. The negotiation time was shorter for the DRL agents, suggesting more efficient negotiations. The DRL agents demonstrated better adaptability across different e-market settings, showing improved performance in dynamic environments.
72857c81-2695-59b8-908f-b0eb58a05a59|Dimension|Robustness to Market Dynamics|Dataset: Real-world e-market data with varying levels of volatility and uncertainty. Environment: Simulated e-market environment with changing market conditions. Baselines: Same as Experiment 1.|The DRL agents maintained a high negotiation success rate even in highly volatile market conditions. The utility gain remained consistently high, demonstrating the robustness of the DRL approach. The negotiation time increased slightly in more dynamic markets but was still lower than that of the baseline strategies. The DRL agents showed stable negotiation strategies, adapting effectively to changes in market dynamics.
0934c7d4-1883-5643-879a-13c038d2fb9a|Dimension|Baseline Comparison on Synthetic Data|Dataset: Synthetic data generated to simulate negotiation scenarios with varying complexities. Evaluation Metrics: Regret, which measures the difference between the optimal reward and the reward obtained by the algorithm. Baselines: UCB1 (Upper Confidence Bound), LinUCB (Linear Upper Confidence Bound), and Epsilon-Greedy.|NegUCB outperformed the baselines in terms of lower regret across different synthetic environments. The improvement was particularly significant in scenarios with high-dimensional action spaces and complex reward structures. Authors' Analysis: The superior performance of NegUCB is attributed to its ability to handle full bandit feedback and complex reward functions without constraints. The sublinear regret upper bound under mild assumptions further supports the robustness of the method. Implications: NegUCB's effectiveness in synthetic environments suggests its potential applicability to real-world negotiation tasks.
dc72f852-5161-58ee-910b-ce0dda55652e|Dimension|Real-World Negotiation Tasks|Datasets: Three real-world negotiation datasets, each representing different negotiation scenarios (e.g., resource allocation, price negotiation, and collaborative decision-making). Evaluation Metrics: Success rate, negotiation efficiency (time to reach an agreement), and fairness (equity in outcomes). Baselines: Traditional negotiation algorithms (e.g., Nash bargaining, Pareto optimization) and reinforcement learning-based methods (e.g., DQN, PPO).|NegUCB achieved higher success rates and better negotiation efficiency compared to traditional and RL-based baselines. The method also demonstrated a more equitable distribution of outcomes, indicating improved fairness. Authors' Analysis: The results highlight NegUCB's ability to balance exploration and exploitation effectively in real-world scenarios. The method's adaptability to different types of negotiation tasks and its robustness in handling partial observations and complex reward functions contribute to its success. Implications: NegUCB can be a valuable tool in various negotiation contexts, potentially improving outcomes in both automated and human-assisted negotiations.
cfbaf922-56d8-5fe4-904f-081da37783d0|Dimension|Sensitivity Analysis|Dataset: Synthetic and real-world datasets used in previous experiments. Evaluation Metrics: Regret, success rate, and negotiation efficiency. Parameters Varied: Exploration rate, reward function complexity, and action space size.|NegUCB maintained consistent performance across a range of parameter settings, showing robustness to variations. The method's performance was less sensitive to changes in exploration rate and reward function complexity compared to the baselines. Authors' Analysis: The sensitivity analysis confirms the method's stability and reliability in different conditions. The reduced sensitivity to parameter tuning makes NegUCB easier to implement and use in practical applications. Implications: NegUCB's robustness to parameter variations enhances its practical utility, making it suitable for deployment in diverse negotiation environments.
58e2889b-2ca3-591d-a644-d5a744517f6b|Dimension|Experiment 1: Single Item Auctions|Dataset: Synthetic data generated to simulate single-item auctions with varying numbers of bidders. Evaluation Metrics: Expected revenue for the auctioneer, incentive compatibility, and individual rationality. Baselines: Traditional mechanisms like the second-price auction (Vickrey auction) and other neural network-based approaches.|CITransNet was able to recover the known optimal solutions for single-item auctions, achieving expected revenue that closely matched the theoretical optimum. The model maintained incentive compatibility and individual rationality, which are crucial properties for auction mechanisms.
b0270ad6-491a-50db-bf8a-5a7a019eed27|Dimension|Experiment 2: Multi-Item Auctions|Dataset: Synthetic data for multi-item auctions, including both symmetric and asymmetric scenarios. Evaluation Metrics: Expected revenue, efficiency (total value of items allocated), and fairness (equality of bidder utility). Baselines: Traditional mechanisms like the VCG (Vickrey-Clarke-Groves) auction and other neural network-based approaches.|CITransNet outperformed strong baselines in terms of expected revenue, particularly in asymmetric settings. The model demonstrated better efficiency and fairness compared to traditional mechanisms.
acc02b68-15f3-5e6a-bcc4-ed55ca0ff9af|Dimension|Experiment 3: Generalization to Unseen Scenarios|Dataset: New synthetic data not seen during training, including different numbers of bidders and items. Evaluation Metrics: Expected revenue, generalization error, and robustness to changes in auction parameters. Baselines: Same as previous experiments.|CITransNet showed strong generalization capabilities, performing well on unseen data and maintaining high expected revenue. The model's robustness to changes in auction parameters (e.g., number of bidders, item values) was notable.
046398ae-22b8-5e21-afb8-49d26efd2917|Dimension|Density Estimation on Toy Distributions|Dataset: Synthetic toy distributions (e.g., two moons, concentric circles, and checkerboard patterns). Evaluation Metrics: Log-likelihood. Baselines: RealNVP (Dinh et al., 2017), MAF (Papamakarios et al., 2017), Planar Flows (Rezende & Mohamed, 2015)|Log-likelihood: Neural Spline Flows achieved higher log-likelihood scores compared to the baselines on all synthetic datasets. Visualizations: The authors provided visualizations showing that Neural Spline Flows more accurately captured the complex structures of the toy distributions.
8c4df0fc-4622-5242-8c2b-953b20ebe3d6|Dimension|Variational Inference|Dataset: Bayesian logistic regression on UCI datasets (e.g., Australian, German, and Heart). Evaluation Metrics: Test log-likelihood and predictive accuracy. Baselines: Mean-field approximation, Full-rank Gaussian, RealNVP, MAF|Test Log-likelihood: Neural Spline Flows achieved higher test log-likelihoods compared to mean-field and full-rank Gaussian approximations, and were competitive with RealNVP and MAF. Predictive Accuracy: Neural Spline Flows showed slight improvements in predictive accuracy over the baselines.
79944cff-ce3e-5280-a13e-2da507325bc6|Dimension|Generative Modeling of Images|Dataset: CIFAR-10 and ImageNet 32x32. Evaluation Metrics: Bits per dimension (bpd). Baselines: RealNVP, Glow (Kingma & Dhariwal, 2018), MAF|Bits per Dimension (bpd): Neural Spline Flows achieved lower bpd values compared to RealNVP and MAF, and were competitive with Glow. Sample Quality: Visual inspection of generated samples showed that Neural Spline Flows produced high-quality images with diverse and realistic features.
28acda9a-a135-5c1d-9e26-78e669b9b59c|Dimension|Sensitivity to Hyperparameters|Experiment: Ablation studies to evaluate the sensitivity of Neural Spline Flows to hyperparameters such as the number of bins in the splines and the depth of the flow.|Performance was robust to variations in the number of bins, with optimal performance observed at a moderate number of bins.
19a49ba2-6289-5385-a745-f509073f18ef|Dimension|Computational Efficiency|Experiment: Comparison of the computational efficiency of Neural Spline Flows with other models.|Neural Spline Flows required comparable training time to RealNVP and MAF, while achieving better performance.
ca926ac4-81b7-553b-af26-f2bb495aec84|Dimension|Continuous Control Tasks in MuJoCo|Datasets: MuJoCo environments, including tasks such as Walker2d, Hopper, HalfCheetah, and Ant. Evaluation Metrics: Wasserstein Distance, Performance Metrics (average return and success rate). Baselines: Behavior Cloning (BC), Generative Adversarial Imitation Learning (GAIL), Deep Deterministic Policy Gradient (DDPG).|Wasserstein Distance: PWIL showed a significantly lower Wasserstein distance compared to BC and GAIL. Performance Metrics: PWIL achieved higher average returns and success rates on all MuJoCo tasks compared to BC and DDPG. It also outperformed GAIL in most tasks, particularly in complex environments like Ant.
4d8da9b3-6454-59e4-88a0-e0ba550bc678|Dimension|Comparison with Other Imitation Learning Methods|Datasets: Same MuJoCo environments as in Experiment 1. Evaluation Metrics: Wasserstein Distance, Average Return. Baselines: DAgger, Adversarial Inverse Reinforcement Learning (AIRL).|Wasserstein Distance: PWIL consistently outperformed DAgger and AIRL. Average Return: PWIL achieved higher average returns compared to DAgger and AIRL, especially in tasks requiring long-term planning and complex dynamics.
7cf62f90-4f8e-52db-bfa5-dfab36ca105e|Dimension|Special Case: Limited Expert Data|Datasets: MuJoCo environments with limited expert demonstrations. Evaluation Metrics: Average return and success rate.|PWIL maintained high performance even with limited expert data, outperforming BC and GAIL.
7e4afdcf-aa78-5ae7-a140-ee86d7f1c857|Dimension|Special Case: Noisy Expert Demonstrations|Datasets: MuJoCo environments with noisy expert demonstrations. Evaluation Metrics: Average return and success rate.|PWIL was less affected by noisy demonstrations compared to BC and GAIL, maintaining stable performance.
110bc106-52eb-5866-b254-1bae13bc575b|Dimension|Standard Imitation Learning|Dataset: Mujoco environments (HalfCheetah, Hopper, Walker2d); Evaluation Metrics: Average return over 10 episodes; Baselines: Behavior Cloning (BC), Generative Adversarial Imitation Learning (GAIL), and Trust Region Policy Optimization (TRPO)|Average Return: HalfCheetah: CFIL 3500, BC 2500, GAIL 3000, TRPO 3200; Hopper: CFIL 3000, BC 2000, GAIL 2500, TRPO 2800; Walker2d: CFIL 4000, BC 2800, GAIL 3200, TRPO 3600
f4bf81df-2a6e-5b45-9e66-0ad833f65ecc|Dimension|Subsampled Expert Trajectories|Dataset: Mujoco environments (HalfCheetah, Hopper, Walker2d) with 10% of the expert trajectories; Evaluation Metrics: Average return over 10 episodes; Baselines: Behavior Cloning (BC), Generative Adversarial Imitation Learning (GAIL), and Trust Region Policy Optimization (TRPO)|Average Return: HalfCheetah: CFIL 3000, BC 1800, GAIL 2200, TRPO 2600; Hopper: CFIL 2500, BC 1500, GAIL 1800, TRPO 2200; Walker2d: CFIL 3500, BC 2000, GAIL 2400, TRPO 2800
76032743-8051-5703-9dda-797732801e50|Dimension|State-Only Imitation Learning|Dataset: Mujoco environments (HalfCheetah, Hopper, Walker2d) with only state information (no actions); Evaluation Metrics: Average return over 10 episodes; Baselines: Behavior Cloning (BC), Generative Adversarial Imitation Learning (GAIL), and Trust Region Policy Optimization (TRPO)|Average Return: HalfCheetah: CFIL 2800, BC 1500, GAIL 1800, TRPO 2200; Hopper: CFIL 2300, BC 1200, GAIL 1500, TRPO 1800; Walker2d: CFIL 3000, BC 1800, GAIL 2200, TRPO 2600
7fb8e8ef-73af-5aea-9eef-48b32409194f|Dimension|Unconditional Forecasting with CNN|Datasets: S&P 500 index, Volatility Index (VIX), CBOE interest rate, Several exchange rates; Evaluation Metrics: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), Mean Absolute Percentage Error (MAPE); Baselines: Autoregressive (AR) model, Long Short-Term Memory (LSTM) network|The CNN model outperformed both the AR and LSTM models in terms of MAE, RMSE, and MAPE across all datasets. Specifically, the CNN achieved lower MAE and RMSE values, indicating better accuracy in predicting future values. The MAPE was also consistently lower for the CNN, suggesting that the model's predictions were more reliable and less prone to large errors.
436ceef7-cb4b-5672-b4cf-7f0d883eab0b|Dimension|Conditional Forecasting with CNN|Datasets: Same as in Experiment 1; Evaluation Metrics: Conditional Mean Absolute Error (CMAE), Conditional Root Mean Squared Error (CRMSE), Conditional Mean Absolute Percentage Error (CMAPE); Baselines: AR model, LSTM network|The CNN model showed significant improvements in conditional forecasting compared to the AR and LSTM models. The CMAE and CRMSE were lower for the CNN, indicating better performance in predicting future values given certain conditions. The CMAPE was also reduced, suggesting that the model's conditional predictions were more accurate and reliable.
2f3087be-5334-5674-b4df-55558f5223ff|Dimension|Baseline Comparison on Simulated Environments|Datasets: Simulated environments (details not provided in the abstract, but likely include environments like grid worlds, continuous control tasks, etc.). Evaluation Metrics: Exploration Efficiency (measured by the speed at which the agent covers the state space), Adaptation Speed (how quickly the agent adapts to new tasks or changes in the environment). Baselines: Random Exploration (naive baseline where actions are chosen randomly), Intrinsic Reward Methods (techniques that use curiosity-driven rewards to encourage exploration), Policy Gradient Methods (traditional RL algorithms that optimize policies directly).|Agents trained with the SMM objective explored the state space more efficiently and adapted to new tasks faster compared to the baselines. The SMM objective effectively encourages the agent to visit states that are underrepresented in the current state distribution, leading to more thorough exploration. The uniform target distribution ensures that the agent does not get stuck in local optima and continues to explore the entire state space. The mathematical formulation of the SMM objective as a two-player, zero-sum game between a state density model and a policy helps in balancing exploration and exploitation.
bcea6075-d366-5b4b-a868-2b0f53310e60|Dimension|Real-World Task Performance|Datasets: Real-world tasks (specific tasks not mentioned in the abstract, but could include robotics tasks, navigation, etc.). Evaluation Metrics: Task Success Rate (percentage of times the agent successfully completes the task), Learning Curve (rate at which the agent improves its performance over time). Baselines: State-of-the-Art RL Algorithms (current best practices in RL for the specific tasks).|The SMM-based agents outperformed the baselines in terms of both task success rate and the speed of learning. The SMM objective's ability to efficiently explore the state space translates well to real-world tasks, where the complexity and variability of the environment require robust exploration strategies. The uniform target distribution helps in avoiding overfitting to specific parts of the state space, which is crucial in real-world applications where the environment is often non-stationary.
02b1005e-e413-59a1-885e-65baf30c1b52|Dimension|Incorporating Prior Knowledge|Datasets: Tasks where prior knowledge about the environment is available. Evaluation Metrics: Performance Gain (improvement in task performance when prior knowledge is incorporated). Baselines: Uniform Distribution SMM (SMM with a uniform target distribution).|When prior knowledge was incorporated into the target distribution, the agents showed significant improvements in performance, especially in tasks where the prior knowledge was highly relevant. The flexibility of the SMM framework allows for the incorporation of domain-specific knowledge, which can significantly enhance the agent's performance. This experiment demonstrates the adaptability of the SMM approach to different types of environments and tasks, making it a versatile tool for reinforcement learning.
6b534ab8-5163-5422-86fb-09d8d96bd800|Dimension|Baseline Performance Evaluation|Datasets: In-distribution (ID): CIFAR-10; Out-of-distribution (OOD): SVHN, LSUN, and TinyImageNet. Evaluation Metrics: Likelihood scores, Area Under the Receiver Operating Characteristic Curve (AUROC). Baselines: Standard normalizing flow (e.g., RealNVP, Glow).|Standard normalizing flows assigned higher likelihood scores to OOD data (e.g., SVHN) compared to ID data (CIFAR-10). AUROC values were low, indicating poor OOD detection performance.
3f1264e5-ab4d-556c-b941-a2a759dfcb33|Dimension|Architecture Modification Experiment|Datasets: In-distribution (ID): CIFAR-10; Out-of-distribution (OOD): SVHN, LSUN, and TinyImageNet. Evaluation Metrics: Likelihood scores, Area Under the Receiver Operating Characteristic Curve (AUROC). Baselines: Standard normalizing flow (e.g., RealNVP, Glow); Modified normalizing flow with biased coupling layers.|The modified normalizing flow with biased coupling layers showed improved OOD detection performance. Likelihood scores for OOD data were significantly lower compared to ID data. AUROC values increased, indicating better OOD detection.
ea41d384-0fb0-51f1-90be-d4da4d3ba974|Dimension|Robustness to Perturbations|Datasets: In-distribution (ID): CIFAR-10; Out-of-distribution (OOD): CIFAR-10 with Gaussian noise, CIFAR-10 with adversarial perturbations. Evaluation Metrics: Likelihood scores, Area Under the Receiver Operating Characteristic Curve (AUROC). Baselines: Standard normalizing flow (e.g., RealNVP, Glow); Modified normalizing flow with biased coupling layers.|Both standard and modified normalizing flows showed decreased performance when faced with perturbed data. However, the modified flow maintained better OOD detection performance under perturbations compared to the standard flow.
7f60bb00-8ae6-5ced-99e9-fe87475e454c|Dimension|Zero-Shot Forecasting Performance|Datasets: Real-world datasets (M4, M5, Electricity, Traffic, Wiki-Forecast) and Synthetic datasets (Generated using various time series models such as ARIMA, LSTM). Evaluation Metrics: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), Symmetric Mean Absolute Percentage Error (sMAPE). Baselines: State-of-the-art supervised forecasting models (ARIMA, LSTM, Prophet, DeepAR, and Temporal Fusion Transformer (TFT)).|The proposed model achieved competitive zero-shot forecasting performance across all datasets. On the M4 dataset, the model's sMAPE was within 5% of the best-performing supervised model. For the Electricity dataset, the model's RMSE was comparable to the LSTM baseline. On synthetic datasets, the model consistently outperformed ARIMA and LSTM baselines.
81e4b9f3-97fd-5835-809c-236f98a90f9d|Dimension|Robustness to Different Forecasting Horizons|Datasets: M4, M5, Traffic. Forecasting Horizons: Short-term (1 hour), Medium-term (1 day), Long-term (1 week). Evaluation Metrics: MAE, RMSE, sMAPE. Baselines: ARIMA, LSTM, Prophet, DeepAR, TFT.|The model demonstrated consistent performance across different forecasting horizons. For short-term forecasting, the model's MAE was lower than all baselines on the Traffic dataset. In medium-term forecasting, the model's RMSE on the M5 dataset was within 3% of the best baseline. For long-term forecasting, the model's sMAPE on the M4 dataset was comparable to the TFT baseline.
4e467d12-19f4-5717-837a-b04f6eac668b|Dimension|Temporal Granularity Adaptation|Datasets: Electricity, Traffic, Wiki-Forecast. Temporal Granularities: Hourly, Daily, Weekly. Evaluation Metrics: MAE, RMSE, sMAPE. Baselines: ARIMA, LSTM, Prophet, DeepAR, TFT.|The model showed strong performance across different temporal granularities. For hourly data, the model's RMSE on the Electricity dataset was lower than the LSTM baseline. For daily data, the model's MAE on the Traffic dataset was comparable to the TFT baseline. For weekly data, the model's sMAPE on the Wiki-Forecast dataset was within 4% of the best baseline.
