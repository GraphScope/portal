{
    "data": {
        "id": "2006.08545v1",
        "published": "2020-06-16T00:39:41+00:00",
        "year": 2020,
        "month": 6,
        "title": "Why Normalizing Flows Fail to DetectOut-of-Distribution Data",
        "authors": [
            "Polina Kirichenko",
            "Pavel Izmailov",
            "Andrew Gordon Wilson"
        ],
        "summary": "Detecting out-of-distribution (OOD) data is crucial for robust machine learning systems. Normalizing flows are flexible deep generative models that often surprisingly fail to distinguish between in- and out-of-distribution data: a flow trained on pictures of clothing assigns higher likelihood to handwritten digits. We investigate why normalizing flows perform poorly for OOD detection. We demonstrate that flows learn local pixel correlations and generic image-to-latent-space transformations which are not specific to the target image dataset. We show that by modifying the architecture of flow coupling layers we can bias the flow towards learning the semantic structure of the target data, improving OOD detection. Our investigation reveals that properties that enable flows to generate high-fidelity images can have a detrimental effect on OOD detection.",
        "journal_ref": null,
        "doi": null,
        "primary_category": "stat.ML",
        "categories": [
            "stat.ML",
            "cs.LG"
        ],
        "bib": "@article{2006.08545v1,\\nAuthor        = {Polina Kirichenko and Pavel Izmailov and Andrew Gordon Wilson},\\nTitle         = {Why Normalizing Flows Fail to Detect Out-of-Distribution Data},\\nEprint        = {http://arxiv.org/abs/2006.08545v1},\\nArchivePrefix = {arXiv},\\nPrimaryClass  = {stat.ML},\\nAbstract      = {Detecting out-of-distribution (OOD) data is crucial for robust machine\\nlearning systems. Normalizing flows are flexible deep generative models that\\noften surprisingly fail to distinguish between in- and out-of-distribution\\ndata: a flow trained on pictures of clothing assigns higher likelihood to\\nhandwritten digits. We investigate why normalizing flows perform poorly for OOD\\ndetection. We demonstrate that flows learn local pixel correlations and generic\\nimage-to-latent-space transformations which are not specific to the target\\nimage dataset. We show that by modifying the architecture of flow coupling\\nlayers we can bias the flow towards learning the semantic structure of the\\ntarget data, improving OOD detection. Our investigation reveals that properties\\nthat enable flows to generate high-fidelity images can have a detrimental\\neffect on OOD detection.},\\nYear          = {2020},\\nMonth         = {6},\\nUrl           = {http://arxiv.org/pdf/2006.08545v1},\\nFile          = {2006.08545v1.pdf}\\n}",
        "reference": [
            "[33] Jie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo, Joshua Dillon, and Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection.",
            "3 The original images are 3072 -dimensional, so the dimension of the embeddings is only two times smaller.",
            "[35] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision , 115 (3):211\u2013252, 2015.",
            "[16] Emiel Hoogeboom, Rianne van den Berg, and Max Welling. Emerging convolutions for generative normalizing \ufb02ows. arXiv preprint arXiv:1901.11137 , 2019.",
            "[6] Ricky TQ Chen, Jens Behrmann, David Duvenaud, and J\u00f6rn-Henrik Jacobsen. Residual \ufb02ows for invertible generative modeling. arXiv preprint arXiv:1906.02735 , 2019.",
            "[19] Pavel Izmailov, Polina Kirichenko, Marc Finzi, and Andrew Gordon Wilson. Semi- supervised learning with normalizing \ufb02ows. In International Conference on Machine Learning , 2020.",
            "[24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 , 2017.",
            "[28] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, and Balaji Lakshminarayanan. Detecting out-of-distribution inputs to deep generative models using a test for typicality.",
            "[25] Xuezhe Ma, Xiang Kong, Shanghang Zhang, and Eduard Hovy. Macow: Masked convolutional generative \ufb02ow. In Advances in Neural Information Processing Systems , pages 5891\u20135900, 2019.",
            "[26] Tom M Mitchell. The need for biases in learning generalizations . Department of Computer Science, Laboratory for Computer Science Research . . . , 1980.",
            "I Changing biases in \ufb02ow models for better OOD detec- tion I.1 Cycle-mask",
            "C Details of the experiments",
            "[27] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshmi- narayanan. Do deep generative models know what they don\u2019t know? arXiv preprint arXiv:1810.09136 , 2018.",
            "2 Background We brie\ufb02y introduce normalizing \ufb02ows based on coupling layers. For a more detailed introduction, see Papamakarios et al. [ 31 ] and Kobyzev et al. [ 23 ].",
            "[3] Jens Behrmann, David Duvenaud, and J\u00f6rn-Henrik Jacobsen. Invertible residual networks. arXiv preprint arXiv:1811.00995 , 2018.",
            "[30] George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive \ufb02ow for density estimation. In Advances in Neural Information Processing Systems , pages 2338\u20132347, 2017.",
            "[18] Sergey Io\ufb00e and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167 , 2015.",
            "[36] Joan Serr\u00e0, David \u00c1lvarez, Vicen\u00e7 G\u00f3mez, Olga Slizovskaia, Jos\u00e9 F N\u00fa\u00f1ez, and Jordi Luque. Input complexity and out-of-distribution detection with likelihood-based genera- tive models. arXiv preprint arXiv:1909.11480 , 2019.",
            "[32] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A \ufb02ow-based generative network for speech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 3617\u20133621. IEEE, 2019.",
            "[4] Apratim Bhattacharyya, Shweta Mahajan, Mario Fritz, Bernt Schiele, and Stefan Roth. Normalizing \ufb02ows with multi-scale autoregressive priors. In Proceedings of the Conference on Computer Vision and Pattern Recognition , pages 8415\u20138424, 2020.",
            "[13] Will Grathwohl, Ricky TQ Chen, Jesse Betterncourt, Ilya Sutskever, and David Duve- naud. Ffjord: Free-form continuous dynamics for scalable reversible generative models.",
            "[7] Hyunsun Choi, Eric Jang, and Alexander A Alemi. Waic, but why? generative ensembles for robust anomaly detection. arXiv preprint arXiv:1810.01392 , 2018.",
            "[40] Mingxing Tan and Quoc V Le. E\ufb03cientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946 , 2019.",
            "[9] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516 , 2014.",
            "[39] Esteban G Tabak and Cristina V Turner. A family of nonparametric density estimation algorithms. Communications on Pure and Applied Mathematics , 66(2):145\u2013164, 2013.",
            "[12] Marc Finzi, Pavel Izmailov, Wesley Maddox, Polina Kirichenko, and Andrew Gordon Wilson. Invertible convolutional networks. In Workshop on Invertible Neural Nets and Normalizing Flows, International Conference on Machine Learning , 2019.",
            "[10] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv preprint arXiv:1605.08803 , 2016.",
            "[22] Durk P Kingma and Prafulla Dhariwal. Glow: Generative \ufb02ow with invertible 1 \u00d7 1 convolutions. In Advances in Neural Information Processing Systems , pages 10215\u201310224, 2018.",
            "E Visualization implementation Normalizing \ufb02ows such as RealNVP and Glow consist of a sequence of coupling layers which change the content of the input and squeeze layers (see Figure 9 ) which reshape it. Due to the presence of squeeze layers, the latent representations of the \ufb02ow have a di\ufb00erent",
            "[21] Sungwon Kim, Sang-gil Lee, Jongyoon Song, Jaehyeon Kim, and Sungroh Yoon. Flowavenet: A generative \ufb02ow for raw audio. arXiv preprint arXiv:1811.02155 , 2018.",
            "[2] Pierre Baldi, Kyle Cranmer, Taylor Faucett, Peter Sadowski, and Daniel Whiteson. Pa- rameterized machine learning for high-energy physics. arXiv preprint arXiv:1601.07913 , 2016.",
            "[38] Yang Song, Chenlin Meng, and Stefano Ermon. Mintnet: Building invertible neural networks with masked convolutions. In Advances in Neural Information Processing Systems , pages 11002\u201311012, 2019.",
            "[29] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759 , 2016.",
            "[34] Byron P Roe, Hai-Jun Yang, Ji Zhu, Yong Liu, Ion Stancu, and Gordon McGregor. Boosted decision trees as an alternative to arti\ufb01cial neural networks for particle identi\ufb01- cation. Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment , 543(2-3):577\u2013584, 2005.",
            "[11] Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline \ufb02ows. In Advances in Neural Information Processing Systems , pages 7509\u20137520, 2019.",
            "[1] Andrei Atanov, Alexandra Volokhova, Arsenii Ashukha, Ivan Sosnovik, and Dmitry Vetrov. Semi-conditional normalizing \ufb02ows for semi-supervised learning. arXiv preprint arXiv:1905.00505 , 2019.",
            "[42] Antonio Torralba, Rob Fergus, and William T Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE transactions on pattern analysis and machine intelligence , 30(11):1958\u20131970, 2008.",
            "[23] Ivan Kobyzev, Simon Prince, and Marcus A Brubaker. Normalizing \ufb02ows: Introduction and ideas. arXiv preprint arXiv:1908.09257 , 2019.",
            "[44] Andrew Gordon Wilson and Pavel Izmailov. Bayesian deep learning and a probabilistic perspective of generalization. arXiv preprint arXiv:2002.08791 , 2020.",
            "Figure 7: Negative training. The histograms of log-likelihood for RealNVP when in training likelihood is maximized on one dataset and minimized on another dataset: (a) maximized on CIFAR, minimized on SVHN; (b) maximized on SVHN, minimized on CIFAR; (c) maximized on CIFAR, minimized on CelebA; (d) maximized on CelebA, minimized on CIFAR. (e) maximized on FashionMNIST, minimized on MNIST; (f) maximized on MNIST, minimized on FashionMNIST;",
            "5 Flow latent spaces Normalizing \ufb02ows learn highly non-linear image-to-latent-space mappings often using hun- dreds of millions of parameters. One could imagine that the learned latent representations have a complex structure, encoding high-level semantic information about the inputs. In",
            "[37] Jiaming Song, Yang Song, and Stefano Ermon. Unsupervised out-of-distribution detec- tion with batch normalization. arXiv preprint arXiv:1910.09115 , 2019.",
            "[45] Yufeng Zhang, Wanwei Liu, Zhenbang Chen, Ji Wang, Zhiming Liu, Kenli Li, Hongmei Wei, and Zuoning Chen. Out-of-distribution detection with distance guarantee in deep generative models. arXiv preprint arXiv:2002.03328 , 2020.",
            "[31] George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing \ufb02ows for probabilistic modeling and inference.",
            "[43] Benigno Uria, Iain Murray, and Hugo Larochelle. Rnade: The real-valued neural autoregressive density-estimator. In Advances in Neural Information Processing Systems , pages 2175\u20132183, 2013.",
            "6 Transformations learned by coupling layers To better understand the inductive biases of coupling-layer based \ufb02ows, we study the transformations learned by individual coupling layers.",
            "[17] Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville. Neural autoregressive \ufb02ows. arXiv preprint arXiv:1804.00779 , 2018.",
            "[14] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure. arXiv preprint arXiv:1812.04606 , 2018.",
            "[41] Lucas Theis, A\u00e4ron van den Oord, and Matthias Bethge. A note on the evaluation of generative models. arXiv preprint arXiv:1511.01844 , 2015.",
            "1 For the details of the visualization procedure and the training setup please see Appendices E and C .",
            "[8] Nicola De Cao, Ivan Titov, and Wilker Aziz. Block neural autoregressive \ufb02ow. arXiv preprint arXiv:1904.04676 , 2019.",
            "[15] Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving \ufb02ow-based generative models with variational dequantization and architecture design. arXiv preprint arXiv:1902.00275 , 2019.",
            "[5] Jianfei Chen, Cheng Lu, Biqi Chenli, Jun Zhu, and Tian Tian. V\ufb02ow: More expressive generative \ufb02ows with variational data augmentation. arXiv preprint arXiv:2002.09741 , 2020."
        ]
    }
}