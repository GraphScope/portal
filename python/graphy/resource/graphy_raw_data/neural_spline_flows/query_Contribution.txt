**************QUERY***************: 
 
You are a highly skilled academic AI assistant. Your role is to respond to user queries with structured,
clear, and well-organized answers, maintaining an academic tone and focusing on relevance and clarity.

User Query: **Question**: List all contributions of the paper. These contributions are always organized and listed with a head sentence like **our contributions are as follows**. For each contribution, output the **original representation** and use a few words to summarize it.

Guidance:
Research papers typically contain key components, including the problem definition, challenges,
contributions, solutions, and experimental results. These components are generally organized as follows:
- **Problem Definition, Challenges, and Contributions**: Usually found within the first few sections.
- **Solutions**: Typically located in the main body of the paper.
- **Experiment Results**: Usually appear toward the end in sections titled "Experiments" or "Empirical Studies."

The content is retrieved in annotated chunks, marked with **SECTION_X** (indicating the specific section)
or **POS_0.XX** (indicating the position within the paper, calculated as current page/total pages).
Use these annotations to identify and focus on the sections most relevant to the userâ€™s query,
ensuring a precise and targeted response.
                             **************MEMORY**************: 
 **SECTION_paper_meta**: Neural Spline Flows Conor Durkan Artur Bekasov Iain Murray George Papamakarios School of Informatics, University of Edinburgh conor.durkan, artur.bekasov, i.murray, g.papamakarios ed.ac.uk
**SECTION_1**: n log p x n with respect to the parameters of the transformation f. In recent years, normalizing ows have received widespread attention in the machine learning literature, seeing successful use in density estimation 10, 43 , variational inference 30, 36, 46, 57 , image, audio and video generation 26, 28, 32, 45 , likelihood free inference 44 , and learning maximum entropy distributions 34 . Equal contribution
**SECTION_1**: Intuitively, the function f compresses and expands the density of the noise distribution u , and this change is quanti ed by the determinant of the Jacobian of the transformation. The noise distribution u is typically chosen to be simple, such as a standard normal, whereas the transformation f and its inverse f 1 are often implemented by composing a series of invertible neural network modules. Given a dataset D x n N n 1, the ow is trained by maximizing the total log likelihood P
**SECTION_1**: this module signi cantly enhances the exibility of both classes of ows, and in some cases brings the performance of coupling transforms on par with the best known autoregressive ows. An illustration of our proposed transform is shown in g. 1.
**SECTION_1**: 1 Introduction Models that can reason about the joint distribution of high dimensional random variables are central to modern unsupervised machine learning. Explicit density evaluation is required in many statistical procedures, while synthesis of novel examples can enable agents to imagine and plan in an environment prior to choosing a action. In recent years, the variational autoencoder VAE, 29, 48
**SECTION_1**: 33rd Conference on Neural Information Processing Systems NeurIPS 2019 , Vancouver, Canada. arXiv:1906.04032v2 stat.ML 2 Dec 2019
**SECTION_abstract**: Abstract A normalizing ow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the exibility of these models. Building upon recent work, we propose a fully differentiable module based on monotonic rational quadratic splines,
**SECTION_1**: Flow based models present an alternative approach to the above methods, and in some cases provide both exact density evaluation and sampling in a single neural network pass. A normalizing ow models data x as the output of an invertible, differentiable transformation f of noise u: x f u where u u . 1 The probability density of x under the ow is obtained by a change of variables: p x f 1 x det f 1 x . 2
**SECTION_1**: B 0 B x B 0 B g x RQ Spline Inverse Knots B 0 B x 0 1 g x Figure 1: Monotonic rational quadratic transforms are drop in replacements for additive or af ne transformations in coupling or autoregressive layers, greatly enhancing their exibility while retaining exact invertibility. Left: A random monotonic rational quadratic transform with K 10 bins and linear tails is parameterized by a series of K 1 knot points in the plane, and the K 1 derivatives
**SECTION_1**: To train a density estimator, we need to be able to evaluate the Jacobian determinant and the inverse function f 1 quickly. We don t evaluate f, so the ow is usually de ned by specifying f 1. If we wish to draw samples using eq. 1 , we would like f to be available analytically, rather than having to invert f 1 with iterative or approximate methods. Ideally, we would like both f and f 1 to require only a single pass of a neural network to compute,
**SECTION_1**: at the internal knots. Right: Derivative of the transform on the left with respect to x. Monotonic rational quadratic splines naturally induce multi modality when used to transform random variables. A ow is de ned by specifying the bijective function f or its inverse f 1, usually with a neural network. Depending on the ow s intended use cases, there are practical constraints in addition to formal invertibility:
**SECTION_1**: so that both density evaluation and sampling can be performed quickly. Autoregressive ows such as inverse autoregressive ow IAF, 30 or masked autoregressive ow MAF, 43 are D times slower to invert than to evaluate, where D is the dimensionality of x. Subsequent work which enhances their exibility has resulted in models which do not have an analytic inverse, and require numerical optimization to invert 22 . Flows based on coupling layers
**SECTION_abstract**: which enhances the exibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline ows improve density estimation, variational inference, and generative modeling of images.
**SECTION_1**: NICE, RealNVP, 9, 10 have an analytic one pass inverse, but are often less exible than their autoregressive counterparts. In this work, we propose a fully differentiable module based on monotonic rational quadratic splines which has an analytic inverse. The module acts as a drop in replacement for the af ne or additive transformations commonly found in coupling and autoregressive transforms. We demonstrate that
**SECTION_1**: and generative adversarial network GAN, 15 have received particular attention in the generativemodeling community, and both are capable of sampling with a single forward pass of a neural network. However, these models do not offer exact density evaluation, and can be dif cult to train. On the other hand, autoregressive density estimators 13, 50, 56, 58, 59, 60 can be trained by maximum likelihood, but sampling requires a sequential loop over the output dimensions.
