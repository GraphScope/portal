**************QUERY***************: 
 
You are a highly skilled academic AI assistant. Your role is to respond to user queries with structured,
clear, and well-organized answers, maintaining an academic tone and focusing on relevance and clarity.

User Query: **Question**: List all contributions of the paper. These contributions are always organized and listed with a head sentence like **our contributions are as follows**. For each contribution, output the **original representation** and use a few words to summarize it.

Guidance:
Research papers typically contain key components, including the problem definition, challenges,
contributions, solutions, and experimental results. These components are generally organized as follows:
- **Problem Definition, Challenges, and Contributions**: Usually found within the first few sections.
- **Solutions**: Typically located in the main body of the paper.
- **Experiment Results**: Usually appear toward the end in sections titled "Experiments" or "Empirical Studies."

The content is retrieved in annotated chunks, marked with **SECTION_X** (indicating the specific section)
or **POS_0.XX** (indicating the position within the paper, calculated as current page/total pages).
Use these annotations to identify and focus on the sections most relevant to the userâ€™s query,
ensuring a precise and targeted response.
                             **************MEMORY**************: 
 **SECTION_1**: by reformulating it Kostrikov et al., 2019 , or by derivation of a surrogate objective Zhu et al., 2020; Dadashi et al., arXiv:2305.00303v1 cs.LG 29 Apr 2023
**SECTION_1**: normalizing ows, then training in an alternating fashion akin to other adversarial IL methods Ho Ermon, 2016; Kostrikov et al., 2018; Ghasemipour et al., 2020; Kostrikov et al., 2019; Sun et al., 2021 . This method proves far more accurate than estimating the log distribution ratio by naively training a pair of ows independently. We show this in part by analyzing their respective BC graphs: a simple tool we present for gauging how well a proposed estimator captures
**SECTION_1**: despite the competition being speci cally designed for that domain. This work also aims to inspire more research incorporating explicit modeling of the state action distribution.
**SECTION_abstract**: Abstract In reinforcement learning and imitation learning, an object of central importance is the state distribution induced by the policy. It plays a crucial role in the policy gradient theorem, and references to it along with the related state action distribution can be found all across the literature. Despite its importance, the state distribution is mostly discussed indirectly and theoretically, rather than being modeled explicitly. The
**SECTION_abstract**: benchmark tasks with a single expert trajectory and extends naturally to a variety of other settings, including the subsampled and state only regimes.
**SECTION_1**: Many approaches to distribution matching based imitation have been presented Ho Ermon, 2016; Fu et al., 2017; Kostrikov et al., 2018; Ke et al., 2020; Ghasemipour et al., 2020; Kostrikov et al., 2019; Sun et al., 2021; Kim et al., 2021b; Dadashi et al., 2020; Schroecker, 2020 . The common theme for such methods begins with the selection of a divergence, followed by the development of a unique approach. This may involve a direct attack on the objective
**SECTION_1**: only by the ability of the agent to reproduce the expert s behavior. Central to both RL and IL are the state and state action distributions induced by the policy. Their importance cannot be overstated, with the state distribution forming the basis of policy gradient methods through the policy gradient theorem Sutton et al., 2000 , and the state action distribution being core to the common distribution matching formulation of IL Ke et al., 2020; Ghasemipour et al., 2020 . They are
**SECTION_1**: In this work, we propose a unique approach to distribution matching based imitation, by coupling a pair of ows through the optimality point of the Donsker Varadhan Donsker Varadhan, 1976 representation of the KL. More speci cally, by noting this point occurs at the log distribution ratio, while the IL objective with the reverse KL can be seen as an RL problem with the ratio inverted. We propose setting the point of optimality as the difference of two
**SECTION_1**: A Coupled Flow Approach to Imitation Learning 2020; Kim et al., 2021b , with some utilizing mechanisms such as an inverse action model Zhu et al., 2020 and focusing on learning from states alone a setting our approach naturally lends to . Other methods rst derive estimates for the gradient of the state distribution with respect to the policy s parameters Schroecker, 2020 , while some devise unifying algorithms and frameworks encompassing previous
**SECTION_1**: the expert s behavior. While most IL works neglect analysis of their learned reward function, we think this can be a potential guiding tool for future IL researchers. Our resulting algorithm, Coupled Flow Imitation Learning CFIL shows strong performance on standard benchmark tasks, while extending naturally to the subsampled and stateonly regimes. In the state only regime in particular, CFIL exhibits signi cant advantage over prior state of the art work,
**SECTION_paper_meta**: A Coupled Flow Approach to Imitation Learning Gideon Freund 1 Elad Sara an 1 Sarit Kraus 1
**SECTION_1**: of the distributions is scarce. Instead, they mostly nd use as a theoretical tool for derivations. This is, of course, barring some approaches that do attempt to model them Hazan et al., 2019; Qin et al., 2021; Lee et al., 2019; Kim et al., 2021b or their ratios Nachum et al., 2019; Liu et al., 2018; Gangwani et al., 2020 , further discussion of which is delegated to the related work. The reason for this lack of modeling is due to the dif culty of density estimation,
**SECTION_abstract**: reason being an absence of appropriate density estimation tools. In this work, we investigate applications of a normalizing ow based model for the aforementioned distributions. In particular, we use a pair of ows coupled through the optimality point of the Donsker Varadhan representation of the Kullback Leibler KL divergence, for distribution matching based imitation learning. Our algorithm, Coupled Flow Imitation Learning CFIL , achieves state of the art performance on
**SECTION_1**: 1. Introduction Reinforcement learning RL Sutton Barto, 2018 concerns the optimization of an agent s behavior in an environment. Its characterizing dif culties of exploration vs exploitation and credit assignment, stem from its typical incarnations where the agent must learn from sparse feedback. In order to provoke desired behavior, one may need to craft a sophisticated reward function or provide demonstrations for the agent to imitate. Imitation Learning IL deals pre
**SECTION_1**: especially in the case of complex and high dimensional distributions. This is where normalizing ows NF Dinh et al., 2014; 2016; Papamakarios et al., 2017 , a recent approach to density estimation, will be of service. We believe there is no shortage of applications for such modeling, but we focus on imitation learning, a quite natural and suitable place, given its modern formulation as stateaction distribution matching Ghasemipour et al., 2020 .
**SECTION_1**: cisely with the latter: learning from expert demonstrations. Although RL and IL share the same ultimate goal of producing a good policy, they differ fundamentally in that RL is guided by the environment s feedback, while IL is guided 1Department of Computer Science, Bar Ilan University, Israel. Correspondence to: Gideon Freund gideonfreund gmail.com . Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author s .
**SECTION_1**: also foundational to other applications, like curiosity based exploration Pathak et al., 2017 , constrained RL Qin et al., 2021 and Batch RL Fujimoto et al., 2019 , some of which have recently been uni ed under the umbrella of convex RL Zhang et al., 2020; Zahavy et al., 2021; Mutti et al., 2022 , which studies objectives that are convex functions of the state distribution. Despite their ubiquity across the literature, explicit modeling
**SECTION_1**: approaches Ke et al., 2020; Ghasemipour et al., 2020 . The most popular divergence of choice is the reverse KL, which some favor due to its mode seeking behavior Ghasemipour et al., 2020 . Others attempt to get the best of both worlds, combining both mode seeking and modecovering elements Zhu et al., 2020 . A priori, it is dif cult to say which choice of divergence is advantageous, it s more about the ensuing approach to its minimization.
