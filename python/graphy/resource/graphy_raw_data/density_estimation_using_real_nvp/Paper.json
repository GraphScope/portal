{
    "data": {
        "id": "1605.08803v3",
        "published": "2017-03-01T01:19:42+00:00",
        "year": 2017,
        "month": 3,
        "title": "DENSITY ESTIMATION USING REAL NVP",
        "authors": [
            "Laurent Dinh",
            "Jascha Sohl-Dickstein",
            "Samy Bengio"
        ],
        "summary": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.",
        "journal_ref": null,
        "doi": null,
        "primary_category": "cs.LG",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.NE",
            "stat.ML"
        ],
        "bib": "@article{1605.08803v3,\\nAuthor        = {Laurent Dinh and Jascha Sohl-Dickstein and Samy Bengio},\\nTitle         = {Density estimation using Real NVP},\\nEprint        = {http://arxiv.org/abs/1605.08803v3},\\nArchivePrefix = {arXiv},\\nPrimaryClass  = {cs.LG},\\nAbstract      = {Unsupervised learning of probabilistic models is a central yet challenging\\nproblem in machine learning. Specifically, designing models with tractable\\nlearning, sampling, inference and evaluation is crucial in solving this task.\\nWe extend the space of such models using real-valued non-volume preserving\\n(real NVP) transformations, a set of powerful invertible and learnable\\ntransformations, resulting in an unsupervised learning algorithm with exact\\nlog-likelihood computation, exact sampling, exact inference of latent\\nvariables, and an interpretable latent space. We demonstrate its ability to\\nmodel natural images on four datasets through sampling, log-likelihood\\nevaluation and latent variable manipulations.},\\nYear          = {2016},\\nMonth         = {5},\\nUrl           = {http://arxiv.org/pdf/1605.08803v3},\\nFile          = {1605.08803v3.pdf}\\n}",
        "reference": [
            "",
            "[29] Aapo Hyv\u00e4rinen and Petteri Pajunen. Nonlinear independent component analysis: Existence and uniqueness results. Neural Networks , 12(3):429\u2013439, 1999.",
            "International Journal of Computer Vision , 115(3):211\u2013252, 2015. [53] Ruslan Salakhutdinov and Geoffrey E Hinton. Deep boltzmann machines. In International conference on arti\ufb01cial intelligence and statistics , pages 448\u2013455, 2009.",
            "arXiv preprint arXiv:1302.5125 , 2013. [51] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back- propagating errors. Cognitive modeling , 5(3):1, 1988.",
            "[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR , abs/1512.03385, 2015.",
            "[21] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada , pages 2672\u20132680, 2014.",
            "[44] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature , 518(7540):529\u2013533, 2015.",
            "[30] Daniel Jiwoong Im, Chris Dongjoo Kim, Hui Jiang, and Roland Memisevic. Generating images with recurrent adversarial networks. arXiv preprint arXiv:1602.05110 , 2016.",
            "Systems , pages 2728\u20132736, 2015. [68] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning , 8(3-4):229\u2013256, 1992.",
            "[43] Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. arXiv preprint arXiv:1402.0030 , 2014.",
            "[20] Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. MADE: masked autoencoder for distribution estimation. CoRR , abs/1502.03509, 2015.",
            "[36] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images, 2009. [37] Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In AISTATS , 2011.",
            "[33] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 2014.",
            "preprint arXiv:1511.06391 , 2015. [67] Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In Advances in Neural Information Processing Systems , pages 2728\u20132736, 2015.",
            "Journal of arti\ufb01cial intelligence research , 4(1):61\u201376, 1996. [57] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni- tion. arXiv preprint arXiv:1409.1556 , 2014.",
            "[22] Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards conceptual compression. arXiv preprint arXiv:1604.08772 , 2016.",
            "dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365 , 2015. [71] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. arXiv preprint arXiv:1603.08511 , 2016.",
            "[27] Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. The Journal of Machine Learning Research , 14(1):1303\u20131347, 2013.",
            "estimator. In Advances in Neural Information Processing Systems , pages 2175\u20132183, 2013. [65] Hado van Hasselt, Arthur Guez, Matteo Hessel, and David Silver. Learning functions across many orders of magnitudes. arXiv preprint arXiv:1602.07714 , 2016.",
            "Information Processing Systems , pages 1918\u20131926, 2015. [62] Lucas Theis, A\u00e4ron Van Den Oord, and Matthias Bethge. A note on the evaluation of generative models. CoRR , abs/1511.01844, 2015.",
            "Machine Learning, ICML 2015, Lille, France, 6-11 July 2015 , pages 2256\u20132265, 2015. [60] Sasha Targ, Diogo Almeida, and Kevin Lyman. Resnet in resnet: Generalizing residual architectures. CoRR , abs/1603.08029, 2016.",
            "mate inference in deep generative models. arXiv preprint arXiv:1401.4082 , 2014. [50] Oren Rippel and Ryan Prescott Adams. High-dimensional probability estimation with deep density models. arXiv preprint arXiv:1302.5125 , 2013.",
            "[3] Johannes Ball\u00e9, Valero Laparra, and Eero P Simoncelli. Density modeling of images using a generalized normalization transformation. arXiv preprint arXiv:1511.06281 , 2015.",
            "[41] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV) , December 2015.",
            "[38] Anders Boesen Lindbo Larsen, S\u00f8ren Kaae S\u00f8nderby, and Ole Winther. Autoencoding beyond pixels using a learned similarity metric. CoRR , abs/1512.09300, 2015.",
            "[2] Vijay Badrinarayanan, Bamdev Mishra, and Roberto Cipolla. Understanding symmetries in deep networks. arXiv preprint arXiv:1511.01029 , 2015.",
            "[35] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 , 2013.",
            "[18] Brendan J Frey. Graphical models for machine learning and digital communication . MIT press, 1998. [19] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. Texture synthesis using convolutional neural networks. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada , pages 262\u2013270,",
            "CoRR , abs/1603.08029, 2016. [61] Lucas Theis and Matthias Bethge. Generative image modeling using spatial lstms. In Advances in Neural Information Processing Systems , pages 1918\u20131926, 2015.",
            "[6] Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layer neural networks. In NIPS , volume 99, pages 400\u2013406, 1999.",
            "[7] Mathias Berglund and Tapani Raiko. Stochastic gradient estimate variance in contrastive divergence and persistent contrastive divergence. arXiv preprint arXiv:1312.6002 , 2013.",
            "training of deep neural networks. arXiv preprint arXiv:1602.07868 , 2016. [55] Tim Salimans, Diederik P Kingma, and Max Welling. Markov chain monte carlo and variational inference: Bridging the gap. arXiv preprint arXiv:1410.6460 , 2014.",
            "[40] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply-supervised nets. arXiv preprint arXiv:1409.5185 , 2014.",
            "tion. arXiv preprint arXiv:1409.1556 , 2014. [58] Paul Smolensky. Information processing in dynamical systems: Foundations of harmony theory. Technical report, DTIC Document, 1986.",
            "of magnitudes. arXiv preprint arXiv:1602.07714 , 2016. [66] Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets. arXiv preprint arXiv:1511.06391 , 2015.",
            "arXiv:1511.06499 , 2015. [64] Benigno Uria, Iain Murray, and Hugo Larochelle. Rnade: The real-valued neural autoregressive density- estimator. In Advances in Neural Information Processing Systems , pages 2175\u20132183, 2013.",
            "[14] Gustavo Deco and Wilfried Brauer. Higher order statistical decorrelation without information loss. In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, Advances in Neural Information Processing Systems 7 , pages 247\u2013254. MIT Press, 1995.",
            "[5] Yoshua Bengio. Arti\ufb01cial neural networks and their application to sequence recognition. 1991. [6] Yoshua Bengio and Samy Bengio. Modeling high-dimensional discrete data with multi-layer neural networks. In NIPS , volume 99, pages 400\u2013406, 1999.",
            "[16] Luc Devroye. Sample-based non-uniform random variate generation. In Proceedings of the 18th conference on Winter simulation , pages 260\u2013265. ACM, 1986.",
            "[37] Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In AISTATS , 2011. [38] Anders Boesen Lindbo Larsen, S\u00f8ren Kaae S\u00f8nderby, and Ole Winther. Autoencoding beyond pixels using a learned similarity metric. CoRR , abs/1512.09300, 2015.",
            "report, DTIC Document, 1986. [59] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015 , pages 2256\u20132265, 2015.",
            "learning. Machine learning , 8(3-4):229\u2013256, 1992. [69] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. arXiv preprint arXiv:1511.07122 , 2015.",
            "[9] Joan Bruna, Pablo Sprechmann, and Yann LeCun. Super-resolution with deep convolutional suf\ufb01cient statistics. arXiv preprint arXiv:1511.05666 , 2015.",
            "[42] Lars Maal\u00f8e, Casper Kaae S\u00f8nderby, S\u00f8ren Kaae S\u00f8nderby, and Ole Winther. Auxiliary deep generative models. arXiv preprint arXiv:1602.05473 , 2016.",
            "[39] Yann A LeCun, L\u00e9on Bottou, Genevieve B Orr, and Klaus-Robert M\u00fcller. Ef\ufb01cient backprop. In Neural networks: Tricks of the trade , pages 9\u201348. Springer, 2012.",
            "[10] Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv preprint arXiv:1509.00519 , 2015.",
            "[15] Emily L. Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus. Deep generative image models using a laplacian pyramid of adversarial networks. In Advances in Neural Information Processing Systems 28: 10",
            "[31] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167 , 2015.",
            "[13] Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The helmholtz machine. Neural computation , 7(5):889\u2013904, 1995.",
            "preprint arXiv:1505.05770 , 2015. [49] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approxi- mate inference in deep generative models. arXiv preprint arXiv:1401.4082 , 2014.",
            "[4] Anthony J Bell and Terrence J Sejnowski. An information-maximization approach to blind separation and blind deconvolution. Neural computation , 7(6):1129\u20131159, 1995.",
            "propagating errors. Cognitive modeling , 5(3):1, 1988. [52] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.",
            "Bridging the gap. arXiv preprint arXiv:1410.6460 , 2014. [56] Lawrence K Saul, Tommi Jaakkola, and Michael I Jordan. Mean \ufb01eld theory for sigmoid belief networks. Journal of arti\ufb01cial intelligence research , 4(1):61\u201376, 1996.",
            "[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. CoRR , abs/1603.05027, 2016.",
            "CoRR , abs/1511.01844, 2015. [63] Dustin Tran, Rajesh Ranganath, and David M Blei. Variational gaussian process. arXiv preprint arXiv:1511.06499 , 2015.",
            "[34] Diederik P Kingma, Tim Salimans, and Max Welling. Improving variational inference with inverse autoregressive \ufb02ow. arXiv preprint arXiv:1606.04934 , 2016.",
            "[17] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: non-linear independent components estimation. arXiv preprint arXiv:1410.8516 , 2014.",
            "[19] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. Texture synthesis using convolutional neural networks. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada , pages 262\u2013270, 2015.",
            "[12] Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. A recurrent latent variable model for sequential data. In Advances in neural information processing systems , pages 2962\u20132970, 2015.",
            "preprint arXiv:1601.06759 , 2016. [47] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. CoRR , abs/1511.06434, 2015.",
            "arti\ufb01cial intelligence and statistics , pages 448\u2013455, 2009. [54] Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. arXiv preprint arXiv:1602.07868 , 2016.",
            "[26] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation , 9(8):1735\u20131780, 1997.",
            "[8] Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349 , 2015.",
            "[45] Radford M Neal and Geoffrey E Hinton. A view of the em algorithm that justi\ufb01es incremental, sparse, and other variants. In Learning in graphical models , pages 355\u2013368. Springer, 1998.",
            "[32] Rafal J\u00f3zefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. CoRR , abs/1602.02410, 2016.",
            "[11] Scott Shaobing Chen and Ramesh A Gopinath. Gaussianization. In Advances in Neural Information Processing Systems , 2000.",
            "[1] Mart\u0131n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensor\ufb02ow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 , 2016.",
            "arXiv:1511.07122 , 2015. [70] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365 , 2015.",
            "[23] Shixiang Gu, Timothy Lillicrap, Ilya Sutskever, and Sergey Levine. Continuous deep q-learning with model-based acceleration. arXiv preprint arXiv:1603.00748 , 2016.",
            "convolutional generative adversarial networks. CoRR , abs/1511.06434, 2015. [48] Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing \ufb02ows. arXiv preprint arXiv:1505.05770 , 2015.",
            "[28] Aapo Hyv\u00e4rinen, Juha Karhunen, and Erkki Oja. Independent component analysis , volume 46. John Wiley & Sons, 2004."
        ]
    }
}