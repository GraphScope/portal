{
    "data": {
        "id": "2006.04678v2",
        "published": "2021-03-18T02:30:37+00:00",
        "year": 2021,
        "month": 3,
        "title": "PRIMAL WASSERSTEIN IMITATION LEARNING",
        "authors": [
            "Robert Dadashi",
            "L\u00e9onard Hussenot",
            "Matthieu Geist",
            "Olivier Pietquin"
        ],
        "summary": "Imitation Learning (IL) methods seek to match the behavior of an agent with that of an expert. In the present work, we propose a new IL method based on a conceptually simple algorithm: Primal Wasserstein Imitation Learning (PWIL), which ties to the primal form of the Wasserstein distance between the expert and the agent state-action distributions. We present a reward function which is derived offline, as opposed to recent adversarial IL algorithms that learn a reward function through interactions with the environment, and which requires little fine-tuning. We show that we can recover expert behavior on a variety of continuous control tasks of the MuJoCo domain in a sample efficient manner in terms of agent interactions and of expert interactions with the environment. Finally, we show that the behavior of the agent we train matches the behavior of the expert with the Wasserstein distance, rather than the commonly used proxy of performance.",
        "journal_ref": null,
        "doi": null,
        "primary_category": "cs.LG",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "bib": "@article{2006.04678v2,\\nAuthor        = {Robert Dadashi and L\u00e9onard Hussenot and Matthieu Geist and Olivier Pietquin},\\nTitle         = {Primal Wasserstein Imitation Learning},\\nEprint        = {http://arxiv.org/abs/2006.04678v2},\\nArchivePrefix = {arXiv},\\nPrimaryClass  = {cs.LG},\\nAbstract      = {Imitation Learning (IL) methods seek to match the behavior of an agent with\\nthat of an expert. In the present work, we propose a new IL method based on a\\nconceptually simple algorithm: Primal Wasserstein Imitation Learning (PWIL),\\nwhich ties to the primal form of the Wasserstein distance between the expert\\nand the agent state-action distributions. We present a reward function which is\\nderived offline, as opposed to recent adversarial IL algorithms that learn a\\nreward function through interactions with the environment, and which requires\\nlittle fine-tuning. We show that we can recover expert behavior on a variety of\\ncontinuous control tasks of the MuJoCo domain in a sample efficient manner in\\nterms of agent interactions and of expert interactions with the environment.\\nFinally, we show that the behavior of the agent we train matches the behavior\\nof the expert with the Wasserstein distance, rather than the commonly used\\nproxy of performance.},\\nYear          = {2020},\\nMonth         = {6},\\nUrl           = {http://arxiv.org/pdf/2006.04678v2},\\nFile          = {2006.04678v2.pdf}\\n}",
        "reference": [
            "OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research , 2020.",
            "Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforce- ment learning. International Conference on Learning Representations , 2018.",
            "Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems , 2016.",
            "Liyiming Ke, Matt Barnes, Wen Sun, Gilwoo Lee, Sanjiban Choudhury, and Siddhartha Srinivasa. Imitation learning as f -divergence minimization. arXiv preprint arXiv:1905.12888 , 2019.",
            "Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming . John Wiley & Sons, 2014.",
            "Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva Tb, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic policy gradients. International Conference on Learning Representations , 2018.",
            "B A BLATION STUDY We present the learning curves of PWIL in the presence of ablations.",
            "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems , 2014.",
            "Ruohan Wang, Carlo Ciliberto, Pierluigi Vito Amadori, and Yiannis Demiris. Random expert distillation: Imitation learning via expert policy support estimation. In International Conference on Machine Learning , 2019.",
            "Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable imitation learning from visual demonstrations. In Advances in Neural Information Processing Systems , 2017.",
            "Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905 , 2018b.",
            "Abdeslam Boularias, Jens Kober, and Jan Peters. Relative entropy inverse reinforcement learning. In International Conference on Arti\ufb01cial Intelligence and Statistics , 2011.",
            "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations , 2015.",
            "Yusuf Aytar, Tobias Pfaff, David Budden, Thomas Paine, Ziyu Wang, and Nando de Freitas. Playing hard exploration games by watching youtube. In Advances in Neural Information Processing Systems , 2018.",
            "Edouard Klein, Bilal Piot, Matthieu Geist, and Olivier Pietquin. A cascaded supervised learning approach to inverse reinforcement learning. In Joint European conference on machine learning and knowledge discovery in databases , 2013.",
            "V. Kumar and E. Todorov. Mujoco haptix: A virtual reality system for hand manipulation. In 2015 IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids) , 2015.",
            "St\u00e9phane Ross and Drew Bagnell. Ef\ufb01cient reductions for imitation learning. In International Conference on Arti\ufb01cial Intelligence and Statistics , 2010.",
            "Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband, et al. Deep q-learning from demonstrations. In AAAI Conference on Arti\ufb01cial Intelligence , 2018.",
            "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016.",
            "Djork-Arn\u00e9 Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). International Conference on Learning Representations , 2016.",
            "Seyed Kamyar Seyed Ghasemipour, Richard Zemel, and Shixiang Gu. A divergence minimization perspective on imitation learning methods. Conference on Robot Learning , 2019.",
            "Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In International Conference on Machine Learning , 2016.",
            "Jonathan Lacotte, Mohammad Ghavamzadeh, Yinlam Chow, and Marco Pavone. Risk-sensitive generative adversarial imitation learning. In International Conference on Arti\ufb01cial Intelligence and Statistics , 2019.",
            "Yannick Schroecker and Charles L Isbell. State aware imitation learning. In Advances in Neural Information Processing Systems , 2017.",
            "Daniel Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating beyond sub- optimal demonstrations via inverse reinforcement learning from observations. In International Conference on Machine Learning , 2019.",
            "Dean A Pomerleau. Ef\ufb01cient training of arti\ufb01cial neural networks for autonomous navigation. Neural computation , 1991.",
            "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016.",
            "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature , 2015.",
            "Gabriel Peyr\u00e9, Marco Cuturi, et al. Computational optimal transport. Foundations and Trends in Machine Learning , 2019.",
            "Edouard Klein, Matthieu Geist, Bilal Piot, and Olivier Pietquin. Inverse reinforcement learning through structured classi\ufb01cation. In Advances in Neural Information Processing Systems , 2012.",
            "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. International Conference on Learning Representations , 2016.",
            "Bilal Piot, Matthieu Geist, and Olivier Pietquin. Bridging the gap between imitation learning and inverse reinforcement learning. IEEE transactions on neural networks and learning systems , 2016.",
            "Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In International Conference on Intelligent Robots and Systems , 2012.",
            "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning , 2018a.",
            "Ashley Edwards, Himanshu Sahni, Yannick Schroecker, and Charles Isbell. Imitating latent policies from observation. In International Conference on Machine Learning , 2019.",
            "Huang Xiao, Michael Herman, Joerg Wagner, Sebastian Ziesche, Jalal Etesami, and Thai Hong Linh. Wasserstein adversarial imitation learning. arXiv preprint arXiv:1906.08113 , 2019.",
            "Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. Temporal cycle-consistency learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019.",
            "Bilal Piot, Matthieu Geist, and Olivier Pietquin. Learning from demonstrations: Is it worth estimating a reward function? In Joint European Conference on Machine Learning and Knowledge Discovery in Databases , 2013.",
            "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning , 2015.",
            "Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In International Joint Conference on Arti\ufb01cial Intelligence , 2018.",
            "Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement learning. In International Conference on Machine Learning , 2000.",
            "Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM , 1995. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.",
            "Fangchen Liu, Zhan Ling, Tongzhou Mu, and Hao Su. State alignment-based imitation learning. International Conference on Learning Representations , 2020.",
            "Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. ACM Transactions on Graphics (TOG) , 2018.",
            "Alexis Jacq, Matthieu Geist, Ana Paiva, and Olivier Pietquin. Learning from a learner. In International Conference on Machine Learning , 2019.",
            "JA Bagnell, Joel Chestnutt, David M Bradley, and Nathan D Ratliff. Boosting structured prediction for imitation learning. In Advances in Neural Information Processing Systems , 2007.",
            "Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In AAAI Conference on Arti\ufb01cial Intelligence , 2008.",
            "Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In International Conference on Machine Learning , 2017.",
            "Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In International Conference on Machine Learning , 2004.",
            "Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tompson. Discriminator-actor-critic: Addressing sample inef\ufb01ciency and reward bias in adversarial imitation learning. International Conference on Learning Representations , 2019.",
            "Siddharth Reddy, Anca D Dragan, and Sergey Levine. Sqil: imitation learning via regularized behavioral cloning. International Conference on Learning Representations , 2020.",
            "Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor- critic methods. In International Conference on Machine Learning , 2018.",
            "Bilal Piot, Matthieu Geist, and Olivier Pietquin. Boosted and reward-regularized classi\ufb01cation for apprenticeship learning. In International conference on autonomous agents and multi-agent systems , 2014.",
            "R\u00e9mi Flamary and Nicolas Courty. Pot python optimal transport library. GitHub: https://github. com/r\ufb02amary/POT , 2017.",
            "Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Krzysztof Choromanski, Anna Choromanska, and Michael Jordan. Learning to score behaviors for guided policy optimization. In International Conference on Machine Learning , 2020.",
            "C\u00e9dric Villani. Optimal transport: old and new . 2008. Ruohan Wang, Carlo Ciliberto, Pierluigi Vito Amadori, and Yiannis Demiris. Random expert distillation: Imitation learning via expert policy support estimation. In International Conference on Machine Learning , 2019.",
            "Sridhar Mahadevan and Mauro Maggioni. Proto-value functions: A laplacian framework for learning representation and control in markov decision processes. Journal of Machine Learning Research , 2007.",
            "Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . 2018. Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM , 1995.",
            "Kiant\u00e9 Brantley, Wen Sun, and Mikael Henaff. Disagreement-regularized imitation learning. In International Conference on Learning Representations , 2020.",
            "Kee-Eung Kim and Hyun Soo Park. Imitation learning via kernel mean embedding. In AAAI Conference on Arti\ufb01cial Intelligence , 2018.",
            "Nicolas Bonneel, Michiel Van De Panne, Sylvain Paris, and Wolfgang Heidrich. Displacement interpolation using lagrangian mass transport. In Proceedings of the 2011 SIGGRAPH Asia Conference , 2011.",
            "Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. In Proceedings of Robotics: Science and Systems (RSS) , 2018.",
            "Matt Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal Behbahani, Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, et al. Acme: A research framework for distributed reinforcement learning. arXiv preprint arXiv:2006.00979 , 2020.",
            "David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature , 2016.",
            "Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for \ufb01nite markov decision processes. In Conference on Uncertainty in Arti\ufb01cial Intelligence , 2004.",
            "Stuart Russell. Learning agents for uncertain environments. In Conference on Computational learning theory , 1998.",
            "Marc G Bellemare, Will Dabney, and R\u00e9mi Munos. A distributional perspective on reinforcement learning. In International Conference on Machine Learning , 2017."
        ]
    }
}