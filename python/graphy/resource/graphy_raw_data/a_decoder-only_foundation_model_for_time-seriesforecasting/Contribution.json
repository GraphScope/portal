{
    "data": [
        {
            "original": "We design a time series foundation model for forecasting that, when applied to a variety of previously unseen forecasting datasets across different domains, obtains close to state-of-the-art zero-shot accuracy compared to the best supervised models trained individually for these datasets.",
            "summary": "Design of a time series foundation model (TimesFM) achieving near state-of-the-art zero-shot forecasting accuracy."
        },
        {
            "original": "Our model is based on pretraining a decoder-style attention architecture with input patching, using a large time series corpus comprising both real-world and synthetic datasets.",
            "summary": "Development of a decoder-style attention architecture with input patching, trained on a large and diverse time series corpus."
        },
        {
            "original": "Compared to the latest large language models, our time series foundation model is much smaller in both parameter size (200M parameters) and pretraining data size (O(100B timepoints); yet we show that even at such scales, it is possible to pretrain a practical foundation model for forecasting whose zero-shot performance comes close to the accuracy of fully supervised approaches on a diverse set of time series data.",
            "summary": "Demonstration that a smaller, more efficient model can achieve competitive zero-shot forecasting performance."
        },
        {
            "original": "Our work also suggests that unlike recent work that recommends Large Language Models such as GPT-3 and LLama 2 as out-of-the-box zero-shot forecasters, foundation models trained from scratch exclusively on time series data can obtain much better zero-shot performance at a tiny fraction of its costs.",
            "summary": "Evidence that specialized time series foundation models outperform general large language models in zero-shot forecasting with lower computational costs."
        }
    ]
}