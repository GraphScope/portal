{
    "data": {
        "id": "2202.01511v3",
        "published": "2023-01-30T02:15:38+00:00",
        "year": 2023,
        "month": 1,
        "title": "Challenging Common Assumptions in ConvexReinforcement Learning",
        "authors": [
            "Mirco Mutti",
            "Riccardo De Santi",
            "Piersilvio De Bartolomeis",
            "Marcello Restelli"
        ],
        "summary": "The classic Reinforcement Learning (RL) formulation concerns the maximization of a scalar reward function. More recently, convex RL has been introduced to extend the RL formulation to all the objectives that are convex functions of the state distribution induced by a policy. Notably, convex RL covers several relevant applications that do not fall into the scalar formulation, including imitation learning, risk-averse RL, and pure exploration. In classic RL, it is common to optimize an infinite trials objective, which accounts for the state distribution instead of the empirical state visitation frequencies, even though the actual number of trajectories is always finite in practice. This is theoretically sound since the infinite trials and finite trials objectives can be proved to coincide and thus lead to the same optimal policy. In this paper, we show that this hidden assumption does not hold in the convex RL setting. In particular, we show that erroneously optimizing the infinite trials objective in place of the actual finite trials one, as it is usually done, can lead to a significant approximation error. Since the finite trials setting is the default in both simulated and real-world RL, we believe shedding light on this issue will lead to better approaches and methodologies for convex RL, impacting relevant research areas such as imitation learning, risk-averse RL, and pure exploration among others.",
        "journal_ref": null,
        "doi": null,
        "primary_category": "cs.LG",
        "categories": [
            "cs.LG"
        ],
        "bib": "@article{2202.01511v3,\\nAuthor        = {Mirco Mutti and Riccardo De Santi and Piersilvio De Bartolomeis and Marcello Restelli},\\nTitle         = {Challenging Common Assumptions in Convex Reinforcement Learning},\\nEprint        = {http://arxiv.org/abs/2202.01511v3},\\nArchivePrefix = {arXiv},\\nPrimaryClass  = {cs.LG},\\nAbstract      = {The classic Reinforcement Learning (RL) formulation concerns the maximization\\nof a scalar reward function. More recently, convex RL has been introduced to\\nextend the RL formulation to all the objectives that are convex functions of\\nthe state distribution induced by a policy. Notably, convex RL covers several\\nrelevant applications that do not fall into the scalar formulation, including\\nimitation learning, risk-averse RL, and pure exploration. In classic RL, it is\\ncommon to optimize an infinite trials objective, which accounts for the state\\ndistribution instead of the empirical state visitation frequencies, even though\\nthe actual number of trajectories is always finite in practice. This is\\ntheoretically sound since the infinite trials and finite trials objectives can\\nbe proved to coincide and thus lead to the same optimal policy. In this paper,\\nwe show that this hidden assumption does not hold in the convex RL setting. In\\nparticular, we show that erroneously optimizing the infinite trials objective\\nin place of the actual finite trials one, as it is usually done, can lead to a\\nsignificant approximation error. Since the finite trials setting is the default\\nin both simulated and real-world RL, we believe shedding light on this issue\\nwill lead to better approaches and methodologies for convex RL, impacting\\nrelevant research areas such as imitation learning, risk-averse RL, and pure\\nexploration among others.},\\nYear          = {2022},\\nMonth         = {2},\\nUrl           = {http://arxiv.org/pdf/2202.01511v3},\\nFile          = {2202.01511v3.pdf}\\n}",
        "reference": [
            "[9] Kiant \u00b4 e Brantley, Miro Dudik, Thodoris Lykouris, Sobhan Miryoose\ufb01, Max Simchowitz, Alek- sandrs Slivkins, and Wen Sun. Constrained episodic reinforcement learning in concave-convex and knapsack settings. In Advances in Neural Information Processing Systems , 2020.",
            "[31] Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in partially observable stochastic domains. Arti\ufb01cial Intelligence , 1998.",
            "[1] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In International Conference on Machine learning , 2004.",
            "[51] Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, and Shie Mannor. Policy gradient for coherent risk measures. In Advances in Neural Information Processing Systems , 2015.",
            "[5] Karl Johan  \u02da Astr \u00a8 om. Optimal control of Markov processes with incomplete state information. Journal of Mathematical Analysis and Applications , 10:174\u2013205, 1965.",
            "[29] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural Information Processing Systems , 2016.",
            "[7] Richard Bellman. Dynamic programming. Princeton University Press , 1957.",
            "[57] Tom Zahavy, Brendan O\u2019Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is enough for convex mdps. In Advances in Neural Information Processing Systems , 2021.",
            "[42] Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, Jan Peters, et al. An algorithmic perspective on imitation learning. Foundations and Trends \u00ae in Robotics , 7(1-2):1\u2013179, 2018.",
            "[24] Zhaohan Daniel Guo, Mohammad Gheshlagi Azar, Alaa Saade, Shantanu Thakoor, Bilal Piot, Bernardo Avila Pires, Michal Valko, Thomas Mesnard, Tor Lattimore, and R \u00b4 emi Munos.",
            "[11] V \u00b4 \u0131ctor Campos, Alexander Trott, Caiming Xiong, Richard Socher, Xavier Gir \u00b4 o-i Nieto, and Jordi Torres. Explore, discover and learn: Unsupervised discovery of state-covering skills. In International Conference on Machine Learning , 2020.",
            "[13] Wang Chi Cheung. Exploration-exploitation trade-off in reinforcement learning on online markov decision processes with global concave rewards. arXiv preprint arXiv:1905.06466 , 2019.",
            "[10] S \u00b4 ebastien Bubeck and R \u00b4 emi Munos. Open loop optimistic planning. In Conference on Learning Theory , 2010.",
            "[8] L Bisi, L Sabbioni, E Vittori, M Papini, and M Restelli. Risk-averse trust region optimization for reward-volatility reduction. In International Joint Conference on Arti\ufb01cial Intelligence , 2020.",
            "[52] Aviv Tamar and Shie Mannor. Variance adjusted actor critic algorithms. arXiv preprint arXiv:1310.3697 , 2013.",
            "[50] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 2018.",
            "[41] Mirco Mutti and Marcello Restelli. An intrinsically-motivated approach for learning highly exploring and fast mixing policies. In AAAI Conference on Arti\ufb01cial Intelligence , 2020.",
            "[28] Shuncheng He, Yuhang Jiang, Hongchang Zhang, Jianzhun Shao, and Xiangyang Ji. Wasserstein unsupervised reinforcement learning. In AAAI Conference on Arti\ufb01cial Intelligence , 2022.",
            "[15] Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained reinforcement learning with percentile risk criteria. Journal of Machine Learning Research , 18(1):6070\u20136120, 2017.",
            "[40] Mirco Mutti, Lorenzo Pratissoli, and Marcello Restelli. Task-agnostic exploration via policy gra- dient of a non-parametric state entropy estimate. In AAAI Conference on Arti\ufb01cial Intelligence , 2021.",
            "[48] Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics- aware unsupervised discovery of skills. In International Conference on Learning Representa- tions , 2020.",
            "[46] R Tyrrell Rockafellar and Stanislav Uryasev. Optimization of conditional value-at-risk. Journal of Risk , 2(3):21\u201341, 2000.",
            "[26] Milos Hauskrecht. Value-function approximations for partially observable markov decision processes. Journal of Arti\ufb01cial Intelligence Research , 13:33\u201394, 2000.",
            "[3] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In International Conference on Machine Learning , 2017.",
            "[60] Junyu Zhang, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, and Mengdi Wang. Variational policy gradient method for reinforcement learning with general utilities. In Advances in Neural Information Processing Systems , 2020.",
            "[22] Seyed Kamyar Seyed Ghasemipour, Richard Zemel, and Shixiang Gu. A divergence minimiza- tion perspective on imitation learning methods. In Conference on Robot Learning , 2020.",
            "[38] Mirco Mutti, Riccardo De Santi, and Marcello Restelli. The importance of non-markovianity in maximum state entropy exploration. In International Conference on Machine Learning , 2022.",
            "[55] Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Reinforcement learning with prototypical representations. In International Conference on Machine Learning , 2021.",
            "[23] Karol Gregor, Danilo Rezende, and Daan Wierstra. Variational intrinsic control. International Conference on Learning Representations, Workshop Track , 2017.",
            "[17] Robert Dadashi, Leonard Hussenot, Matthieu Geist, and Olivier Pietquin. Primal wasserstein imitation learning. In International Conference on Learning Representations , 2020.",
            "[12] Niladri Chatterji, Aldo Pacchiano, Peter Bartlett, and Michael Jordan. On the theory of reinforcement learning with once-per-episode feedback. In Advances in Neural Information Processing Systems , 2021.",
            "[35] Hao Liu and Pieter Abbeel. APS: Active pretraining with successor features. In International Conference on Machine Learning , 2021.",
            "[14] Wang Chi Cheung. Regret minimization for reinforcement learning with vectorial feedback and complex objectives. In Advances in Neural Information Processing Systems , 2019.",
            "[37] Sobhan Miryoose\ufb01, Kiant \u00b4 e Brantley, Hal Daume III, Miro Dudik, and Robert E Schapire. Reinforcement learning with convex constraints. In Advances in Neural Information Processing Systems , 2019.",
            "[18] Yonathan Efroni, Nadav Merlis, and Shie Mannor. Reinforcement learning with trajectory feedback. In AAAI Conference on Arti\ufb01cial Intelligence , 2021.",
            "[27] Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably ef\ufb01cient maximum entropy exploration. In International Conference on Machine Learning , 2019.",
            "[49] David Silver, Satinder Singh, Doina Precup, and Richard S Sutton. Reward is enough. Arti\ufb01cial Intelligence , 299:103535, 2021.",
            "[20] Javier Garc\u0131a and Fernando Fern \u00b4 andez. A comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research , 16(1):1437\u20131480, 2015.",
            "[43] LA Prashanth and Mohammad Ghavamzadeh. Actor-critic algorithms for risk-sensitive mdps. In Advances in Neural Information Processing Systems , 2013.",
            "[16] Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-sensitive and robust decision- making: a cvar optimization approach. In Advances in Neural Information Processing Systems , 2015.",
            "[34] Michael L Littman, Anthony R Cassandra, and Leslie Pack Kaelbling. Learning policies for partially observable environments: Scaling up. In Machine Learning , pages 362\u2013370. Elsevier, 1995.",
            "[36] Hao Liu and Pieter Abbeel. Behavior from the void: Unsupervised active pre-training. In Advances in Neural Information Processing Systems , 2021.",
            "[44] Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming . John Wiley & Sons, 2014.",
            "[61] Shangtong Zhang, Bo Liu, and Shimon Whiteson. Mean-variance policy iteration for risk-averse reinforcement learning. In AAAI Conference on Arti\ufb01cial Intelligence , 2021.",
            "[59] Chuheng Zhang, Yuanying Cai, Longbo Huang, and Jian Li. Exploration by maximizing R \u00b4 enyi entropy for reward-free RL framework. In AAAI Conference on Arti\ufb01cial Intelligence , 2021.",
            "[19] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. In International Conference on Learning Representations , 2018.",
            "[58] Tom Zahavy, Yannick Schroecker, Feryal Behbahani, Kate Baumli, Sebastian Flennerhag, Shaobo Hou, and Satinder Singh. Discovering policies with domino: Diversity optimization maintaining near optimality. arXiv preprint arXiv:2205.13521 , 2022.",
            "[6] Qinbo Bai, Amrit Singh Bedi, Mridul Agarwal, Alec Koppel, and Vaneet Aggarwal. Achieving zero constraint violation for constrained reinforcement learning via primal-dual approach. In AAAI Conference on Arti\ufb01cial Intelligence , 2022.",
            "[45] Zengyi Qin, Yuxiao Chen, and Chuchu Fan. Density constrained reinforcement learning. In International Conference on Machine Learning , 2021.",
            "[54] Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdu, and Marcelo J Weinberger. Inequalities for the l1 deviation of the empirical distribution. Hewlett-Packard Labs, Tech. Rep , 2003.",
            "[56] Tiancheng Yu, Yi Tian, Jingzhao Zhang, and Suvrit Sra. Provably ef\ufb01cient algorithms for multi-objective competitive rl. In International Conference on Machine Learning , 2021.",
            "[39] Mirco Mutti, Mattia Mancassola, and Marcello Restelli. Unsupervised reinforcement learning in multiple environments. In AAAI Conference on Arti\ufb01cial Intelligence , 2022.",
            "[47] Younggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee. State en- tropy maximization with random encoders for ef\ufb01cient exploration. In International Conference on Machine Learning , 2021.",
            "[30] Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne. Imitation learning: A survey of learning methods. ACM Computing Surveys (CSUR) , 50(2):1\u201335, 2017.",
            "[25] Steven Hansen, Will Dabney, Andre Barreto, David Warde-Farley, Tom Van de Wiele, and Volodymyr Mnih. Fast task inference with variational intrinsic successor features. In Interna- tional Conference on Learning Representations , 2019.",
            "[32] Ilya Kostrikov, O\ufb01r Nachum, and Jonathan Tompson. Imitation learning via off-policy distribu- tion matching. In International Conference on Learning Representations , 2019.",
            "[53] Jean Tarbouriech and Alessandro Lazaric. Active exploration in Markov decision processes. In International Conference on Arti\ufb01cial Intelligence and Statistics , 2019.",
            "[33] Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric Xing, Sergey Levine, and Rus- lan Salakhutdinov. Ef\ufb01cient exploration via state marginal matching. arXiv preprint arXiv:1906.05274 , 2019.",
            "[2] David Abel, Will Dabney, Anna Harutyunyan, Mark K Ho, Michael Littman, Doina Precup, and Satinder Singh. On the expressivity of Markov reward. In Advances in Neural Information Processing Systems , 2021."
        ]
    }
}