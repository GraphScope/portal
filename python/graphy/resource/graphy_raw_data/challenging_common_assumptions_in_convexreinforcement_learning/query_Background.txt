**************QUERY***************: 
 
You are a highly skilled academic AI assistant. Your role is to respond to user queries with structured,
clear, and well-organized answers, maintaining an academic tone and focusing on relevance and clarity.

User Query: **Question**: Please describe the problem studied in this paper, why the problem is worth studying, and what are the issues of existing solutions to this problem.

Guidance:
Research papers typically contain key components, including the problem definition, challenges,
contributions, solutions, and experimental results. These components are generally organized as follows:
- **Problem Definition, Challenges, and Contributions**: Usually found within the first few sections.
- **Solutions**: Typically located in the main body of the paper.
- **Experiment Results**: Usually appear toward the end in sections titled "Experiments" or "Empirical Studies."

The content is retrieved in annotated chunks, marked with **SECTION_X** (indicating the specific section)
or **POS_0.XX** (indicating the position within the paper, calculated as current page/total pages).
Use these annotations to identify and focus on the sections most relevant to the userâ€™s query,
ensuring a precise and targeted response.
                             **************MEMORY**************: 
 **SECTION_1**: discovery 11, 19, 23, 25, 28, 35, 48, 58 , constrained RL 3, 4, 6, 9, 37, 45, 56 , and others. All this large body of work has been recently uni ed into a unique framework, called convex RL 21, 57, 60 , which admits as an objective any convex function of the state distribution induced by the agent s policy. The convex RL problem has been showed to be largely tractable either computationally, as it
**SECTION_1**: the mentioned state distribution and a reward vector. However, not all the relevant objectives can be encoded through this linear representation 2 . Several works have thus extended the standard RL formulation to address non linear objectives of practical interest. These include imitation learning 30, 42 , or the problem of nding a policy that minimizes the distance between the induced state distribution and the state distribution provided by experts interactions 1, 17, 22, 29, 32, 33 ,
**SECTION_1**: In this paper, we formalize the notion of a nite trials RL problem, in which the objective is a function of the empirical state distribution induced by the agent s policy over n trials rather than its expectation over in nite trials. As an illustrative example, consider a nancial application, in which we aim to optimize a trading strategy. In the real world, we can only deploy the strategy over a single trial.
**SECTION_1**: over nite trials Figure 1 . In light of this observation, we reformulate the convex RL problem from a nite trials perspective, developing insights that can be used to partially rethink the way convex objectives have been previously addressed in RL, with potential ripple effects to research areas of signi cant interest, such as imitation learning, risk averse RL, pure exploration, and others.
**SECTION_1**: always computationally tractable and that 3 stationary policies are in general suf cient, should be reconsidered as well. The proofs of the reported results can be found in the Appendix.
**SECTION_paper_meta**: Challenging Common Assumptions in Convex Reinforcement Learning Mirco Mutti Politecnico di Milano Universit a di Bologna mirco.mutti polimi.it Riccardo De Santi ETH Zurich rdesanti ethz.ch Piersilvio De Bartolomeis ETH Zurich pdebartol ethz.ch Marcello Restelli Politecnico di Milano marcello.restelli polimi.it
**SECTION_1**: trials formulation, even if the setting is nite trial. We corroborate this result with an additional numerical analysis showing that the approximation bound is non vacuous for relevant applications Section 6 . Finally, in Section 5 we include an in depth analysis of the single trial convex RL, which suggests that other common assumptions in the convex RL literature, i.e., that 2 the problem is
**SECTION_1**: Section 3 , for which it is trivial to prove the equivalence with standard RL. In Section 4, we provide the nite trial convex RL formulation, for which we prove an upper bound on the approximation error made by optimizing the in nite trials as a proxy of the nite trials objective. In light of this nding, we challenge the hidden assumption that 1 convex RL can be equivalently addressed with an in nite
**SECTION_1**: Thus, we are only interested in the performance of the strategy in the real world realization, rather than the performance of the strategy when averaging different realizations. Similar considerations apply to other relevant real world applications, such as autonomous driving or treatment optimization in a healthcare domain. Following this intuition, we rst de ne the linear nite trial RL formulation
**SECTION_abstract**: in nite trials objective, which accounts for the state distribution instead of the empirical state visitation frequencies, even though the actual number of trajectories is always nite in practice. This is theoretically sound since the in nite trials and nite trials objectives are equivalent and thus lead to the same optimal policy. In this paper, we show that this hidden assumption does not hold in convex RL. In particular, we prove that erroneously optimizing the in nite trials objective in
**SECTION_1**: 1 Introduction Standard Reinforcement Learning RL 50 is concerned with sequential decision making problems in which the utility can be expressed through a linear combination of scalar reward terms. The coef cients of this linear combination are given by the state visitation distribution induced by the agent s policy. Thus, the objective function can be equivalently written as the inner product between
**SECTION_1**: ical state distribution from converging to its expectation. This has never been a problem in standard RL: due to the scalar objective, optimizing the policy over in nite trials or nite trials is equivalent, as it leads to the same optimal policy. Crucially, in this paper, we show that this property does not hold for the convex RL formulation: a policy optimized over in nite trials can be signi cantly sub optimal when deployed
**SECTION_1**: However, we note that the usual convex RL formulation makes an implicit in nite trials assumption which is rarely met in practice. Indeed, the objective is written as a function of the state distribution, which is an expectation over the empirical state distributions that are actually obtained by running the policy in a given episode. In practice, we always run our policy for a nite number of episodes or trials , which in general prevents the empir
**SECTION_abstract**: place of the actual nite trials one, as it is usually done, can lead to a signi cant approximation error. Since the nite trials setting is the default in both simulated and real world RL, we believe shedding light on this issue will lead to better approaches and methodologies for convex RL, impacting relevant research areas such as imitation learning, risk averse RL, and pure exploration among others.
**SECTION_1**: admits a dual formulation akin to standard RL 44 , or statistically, as principled algorithms achieving sub linear regret rates that are slightly worse than standard RL have been developed 57, 60 . Finite Trials In nite Trials RL Convex RL r d F d E dn p n r dn E dn p n F dn Figure 1: Summary of the main nding of this paper: the equivalence between nite and in nite trials objectives does not hold for the convex RL formulation.
**SECTION_abstract**: Abstract The classic Reinforcement Learning RL formulation concerns the maximization of a scalar reward function. More recently, convex RL has been introduced to extend the RL formulation to all the objectives that are convex functions of the state distribution induced by a policy. Notably, convex RL covers several relevant applications that do not fall into the scalar formulation, including imitation learning, risk averse RL, and pure exploration. In classic RL, it is common to optimize an
**SECTION_1**: risk averse RL 20 , in which the objective is sensitive to the tail behavior of the agent s policy 8, 15, 16, 43, 51, 52, 61 , pure exploration 27 , where the goal is to nd a policy that maximizes the entropy of the induced state distribution 24, 33, 36, 38, 39, 40, 41, 47, 53, 55, 59 , diverse skills Equal contribution 36th Conference on Neural Information Processing Systems NeurIPS 2022 . arXiv:2202.01511v3 cs.LG 27 Jan 2023
