**************QUERY***************: 
 
You are a highly skilled academic AI assistant. Your role is to respond to user queries with structured,
clear, and well-organized answers, maintaining an academic tone and focusing on relevance and clarity.

User Query: **Question**: Please describe the problem studied in this paper, why the problem is worth studying, and what are the issues of existing solutions to this problem.

Guidance:
Research papers typically contain key components, including the problem definition, challenges,
contributions, solutions, and experimental results. These components are generally organized as follows:
- **Problem Definition, Challenges, and Contributions**: Usually found within the first few sections.
- **Solutions**: Typically located in the main body of the paper.
- **Experiment Results**: Usually appear toward the end in sections titled "Experiments" or "Empirical Studies."

The content is retrieved in annotated chunks, marked with **SECTION_X** (indicating the specific section)
or **POS_0.XX** (indicating the position within the paper, calculated as current page/total pages).
Use these annotations to identify and focus on the sections most relevant to the userâ€™s query,
ensuring a precise and targeted response.
                             **************MEMORY**************: 
 **SECTION_paper_meta**: Ef cient Exploration via State Marginal Matching Lisa Lee a b Benjamin Eysenbach a b Emilio Parisotto a Eric Xing a Sergey Levine c b Ruslan Salakhutdinov a
**SECTION_1**: that these methods target the single task setting. Because these methods aim to converge to the optimal policy for a particular task, it is dif cult to repurpose these methods to solve multiple tasks. We address these shortcomings by recasting exploration as a problem of State Marginal Matching SMM : Given a target state distribution, we learn a policy for which the state marginal distribution matches this target distribution. Not only does the SMM problem provide a clear and explicit
**SECTION_1**: First, they lack an explicit objective to quantify good exploration, but rather argue that exploration arises implicitly through some iterative procedure. Lacking a well de ned optimization objective, it remains unclear what these methods are doing and why they work. Similarly, the lack of a metric to quantify exploration, even if only for evaluation, makes it dif cult to compare exploration methods and assess progress in this area. 2 The second limitation is
**SECTION_1**: objective for exploration, but it also provides a convenient mechanism to incorporate prior knowledge about the task through the target distribution whether in the form of safety constraints that the agent should obey; preferences for some states over other states; reward shaping; or the relative importance of each state dimension for a particular task. Without any prior information, the SMM objective reduces to maximizing the marginal state entropy H s , which
**SECTION_1**: 1. Introduction Reinforcement learning RL algorithms must be equipped with exploration mechanisms to effectively solve tasks with long horizons and limited or delayed reward signals. These tasks arise in many real world applications where providing human supervision is expensive. Exploration for RL has been studied in a wealth of prior work. The optimal exploration strategy is intractable to compute in most settings, motivating work on tractable heuristics
**SECTION_1**: encourages the policy to visit all states. In this work, we study state marginal matching as a metric for task agnostic exploration. While this class of objectives has been considered in Hazan et al. 2018 , we build on this prior work in a number of dimensions: 1 We argue that the SMM objective is an effective way to learn a single, task agnostic exploration policy that can be used for solving many downstream tasks, amortizing the cost of learning to explore for each task. Learning a single
**SECTION_abstract**: a given target state distribution. The target distribution is a uniform distribution in most cases, but can incorporate prior knowledge if available. In effect, SMM amortizes the cost of learning to explore in a given environment. The SMM objective can be viewed as a two player, zero sum game between a state density model and a parametric policy, an idea that we use to build an algorithm for optimizing the SMM objective. Using this formalism, we further demonstrate that prior
**SECTION_1**: approximately optimizes the same SMM objective, offering an explanation for the success of these methods. However, they omit a crucial historical averaging step, potentially explaining why they do not converge to an exploratory policy. 4 We demonstrate on complex RL tasks that optimizing the SMM objective allows for faster exploration and adaptation than prior state of the art exploration methods. In short, our paper contributes a method to measure, amortize, and understand exploration.
**SECTION_1**: for exploration Kolter Ng, 2009 . Exploration methods based on random actions have limited ability to cover a wide range of states. More sophisticated techniques, such Equal contribution aCarnegie Mellon University bGoogle Brain cUC Berkeley. Correspondence to: Lisa Lee lslee cs.cmu.edu . 1Videos and code: https: sites.google.com view state marginal matching as intrinsic motivation, accelerate learning in the single task setting. However, these methods have two limitations: 1
**SECTION_1**: of policies that resembles the mutual information objectives in recent work Achiam et al., 2018; Eysenbach et al., 2018; Co Reyes et al., 2018 . Thus, these prior work may be interpreted as also almost doing distribution matching, with the caveat that they omit the state entropy term. 3 Our analysis provides a unifying view of prior exploration methods as almost performing distribution matching. We show that exploration methods based on predictive error
**SECTION_abstract**: Abstract Exploration is critical to a reinforcement learning agent s performance in its given environment. Prior exploration methods are often based on using heuristic auxiliary predictions to guide policy behavior, lacking a mathematically grounded objective with clear properties. In contrast, we recast exploration as a problem of State Marginal Matching SMM , where we aim to learn a policy for which the state marginal distribution matches
**SECTION_1**: exploration policy is considerably more dif cult than doing exploration throughout the course of learning a single task. The latter is done by intrinsic motivation Pathak et al., 2017; Tang et al., 2017; Oudeyer et al., 2007 and count based exploration Bellemare et al., 2016 , which can effectively explore to nd states with high reward, at which point the agent can decrease exploration and increase exploitation arXiv:1906.05274v3 cs.LG 28 Feb 2020
**SECTION_1**: Ef cient Exploration via State Marginal Matching of those high reward states. While these methods perform ef cient exploration for learning a single task, we show in Sec. 4 that the policy at any particular iteration is not a good exploration policy. In contrast, maximizing H s produces a stochastic policy at convergence that visits states in proportion to their density under a target distribution. We use this policy as an exploration prior in our multi task experiments, and also prove
**SECTION_abstract**: work approximately maximizes the SMM objective, offering an explanation for the success of these methods. On both simulated and real world tasks, we demonstrate that agents that directly optimize the SMM objective explore faster and adapt more quickly to new tasks as compared to prior exploration methods. 1
**SECTION_1**: that this policy is optimal for a class of goal reaching tasks Appendix B . 2 We explain how to optimize the SMM objective properly. By viewing the objective as a two player, zero sum game between a state density model and a parametric policy, we propose a practical algorithm to jointly learn the policy and the density by using ctitious play Brown, 1951 . We further decompose the SMM objective into a mixture of distributions, and derive an algorithm for learning a mixture
