{
    "data": {
        "id": "2305.00303v1",
        "published": "2023-05-02T00:53:33+00:00",
        "year": 2023,
        "month": 5,
        "title": "A Coupled Flow Approach to Imitation Learning",
        "authors": [
            "Gideon Freund",
            "Elad Sarafian",
            "Sarit Kraus"
        ],
        "summary": "In reinforcement learning and imitation learning, an object of central importance is the state distribution induced by the policy. It plays a crucial role in the policy gradient theorem, and references to it--along with the related state-action distribution--can be found all across the literature. Despite its importance, the state distribution is mostly discussed indirectly and theoretically, rather than being modeled explicitly. The reason being an absence of appropriate density estimation tools. In this work, we investigate applications of a normalizing flow-based model for the aforementioned distributions. In particular, we use a pair of flows coupled through the optimality point of the Donsker-Varadhan representation of the Kullback-Leibler (KL) divergence, for distribution matching based imitation learning. Our algorithm, Coupled Flow Imitation Learning (CFIL), achieves state-of-the-art performance on benchmark tasks with a single expert trajectory and extends naturally to a variety of other settings, including the subsampled and state-only regimes.",
        "journal_ref": null,
        "doi": null,
        "primary_category": "cs.LG",
        "categories": [
            "cs.LG",
            "stat.ML"
        ],
        "bib": "@article{2305.00303v1,\\nAuthor        = {Gideon Freund and Elad Sarafian and Sarit Kraus},\\nTitle         = {A Coupled Flow Approach to Imitation Learning},\\nEprint        = {http://arxiv.org/abs/2305.00303v1},\\nArchivePrefix = {arXiv},\\nPrimaryClass  = {cs.LG},\\nAbstract      = {In reinforcement learning and imitation learning, an object of central\\nimportance is the state distribution induced by the policy. It plays a crucial\\nrole in the policy gradient theorem, and references to it--along with the\\nrelated state-action distribution--can be found all across the literature.\\nDespite its importance, the state distribution is mostly discussed indirectly\\nand theoretically, rather than being modeled explicitly. The reason being an\\nabsence of appropriate density estimation tools. In this work, we investigate\\napplications of a normalizing flow-based model for the aforementioned\\ndistributions. In particular, we use a pair of flows coupled through the\\noptimality point of the Donsker-Varadhan representation of the Kullback-Leibler\\n(KL) divergence, for distribution matching based imitation learning. Our\\nalgorithm, Coupled Flow Imitation Learning (CFIL), achieves state-of-the-art\\nperformance on benchmark tasks with a single expert trajectory and extends\\nnaturally to a variety of other settings, including the subsampled and\\nstate-only regimes.},\\nYear          = {2023},\\nMonth         = {4},\\nUrl           = {http://arxiv.org/pdf/2305.00303v1},\\nFile          = {2305.00303v1.pdf}\\n}",
        "reference": [
            "Lee, L., Eysenbach, B., Parisotto, E., Xing, E., Levine, S., and Salakhutdinov, R. Ef\ufb01cient exploration via state marginal matching. arXiv preprint arXiv:1906.05274 , 2019.",
            "Tompson, J. Discriminator-actor-critic: Addressing sam- ple inef\ufb01ciency and reward bias in adversarial imitation learning. arXiv preprint arXiv:1809.02925 , 2018.",
            "S., and Salakhutdinov, R. Ef\ufb01cient exploration via state marginal matching. arXiv preprint arXiv:1906.05274 , 2019.",
            "Kingma, D. P. and Dhariwal, P. Glow: Generative \ufb02ow with invertible 1x1 convolutions. In Advances in neural information processing systems , pp. 10215\u201310224, 2018.",
            "ing via off-policy distribution matching. arXiv preprint arXiv:1912.05032 , 2019.",
            "Donsker, M. D. and Varadhan, S. S. Asymptotic evaluation of certain markov process expectations for large time\u2014iii.",
            "with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248 , 2017.",
            "Zhang, J., Koppel, A., Bedi, A. S., Szepesvari, C., and Wang, M. Variational policy gradient method for rein- forcement learning with general utilities. Advances in Neural Information Processing Systems , 33:4572\u20134583, 2020.",
            "Nachum, O., Chow, Y., Dai, B., and Li, L. Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. Advances in Neural Information Processing Systems , 32, 2019.",
            "Camacho, A., Gur, I., Moczulski, M. L., Nachum, O., and Faust, A. Sparsedice: Imitation learning for temporally sparse data via regularization. In ICML 2021 Workshop on Unsupervised Reinforcement Learning , 2021.",
            "preprint arXiv:1802.09477 , 2018. Fujimoto, S., Meger, D., and Precup, D. Off-policy deep reinforcement learning without exploration. In Interna- tional Conference on Machine Learning , pp. 2052\u20132062.",
            "Kostrikov, I., Agrawal, K. K., Dwibedi, D., Levine, S., and Tompson, J. Discriminator-actor-critic: Addressing sam- ple inef\ufb01ciency and reward bias in adversarial imitation learning. arXiv preprint arXiv:1809.02925 , 2018.",
            "References Achiam, J. Spinning Up in Deep Reinforcement Learning.",
            "distribution corrections. Advances in Neural Information Processing Systems , 32, 2019.",
            "Dinh, L., Krueger, D., and Bengio, Y. Nice: Non-linear independent components estimation. arXiv preprint arXiv:1410.8516 , 2014.",
            "Papamakarios, G., Pavlakou, T., and Murray, I. Masked autoregressive \ufb02ow for density estimation. In Advances in Neural Information Processing Systems , pp. 2338\u20132347, 2017.",
            "Kirichenko, P., Izmailov, P., and Wilson, A. G. Why normal- izing \ufb02ows fail to detect out-of-distribution data. arXiv preprint arXiv:2006.08545 , 2020.",
            "Kostrikov, I., Nachum, O., and Tompson, J. Imitation learn- ing via off-policy distribution matching. arXiv preprint arXiv:1912.05032 , 2019.",
            "Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density esti- mation using real nvp. arXiv preprint arXiv:1605.08803 , 2016.",
            "Durkan, C., Bekasov, A., Murray, I., and Papamakarios, G. Neural spline \ufb02ows. In Advances in Neural Information Processing Systems , pp. 7511\u20137522, 2019.",
            "Li, Z., Xu, T., Yu, Y., and Luo, Z.-Q. Rethinking valuedice: Does it really improve performance? arXiv preprint arXiv:2202.02468 , 2022.",
            "Dadashi, R., Hussenot, L., Geist, M., and Pietquin, O. Primal wasserstein imitation learning. arXiv preprint arXiv:2006.04678 , 2020.",
            "Liu, Q., Li, L., Tang, Z., and Zhou, D. Breaking the curse of horizon: In\ufb01nite-horizon off-policy estimation. Advances in Neural Information Processing Systems , 31, 2018.",
            "Bliznashki, K. https://github.com/ kamenbliznashki/normalizing_flows , 2019.",
            "Zhu, Z., Lin, K., Dai, B., and Zhou, J. Off-policy imi- tation learning from observations. Advances in Neural Information Processing Systems , 33:12402\u201312413, 2020.",
            "Kobyzev, I., Prince, S. J., and Brubaker, M. A. Normalizing \ufb02ows: An introduction and review of current methods.",
            "Achiam, J. Spinning Up in Deep Reinforcement Learning. 2018.",
            "Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 2014.",
            "independent components estimation. arXiv preprint arXiv:1410.8516 , 2014.",
            "4. Our Approach We begin with the reverse KL as our divergence of choice, since, noting as others have (Kostrikov et al., 2019; Hazan et al., 2019; Kim et al., 2021b; Camacho et al., 2021), its",
            "information processing systems , pp. 10215\u201310224, 2018. Kirichenko, P., Izmailov, P., and Wilson, A. G. Why normal- izing \ufb02ows fail to detect out-of-distribution data. arXiv preprint arXiv:2006.08545 , 2020.",
            "Fujimoto, S., Meger, D., and Precup, D. Off-policy deep reinforcement learning without exploration. In Interna- tional Conference on Machine Learning , pp. 2052\u20132062.",
            "Mutti, M., De Santi, R., De Bartolomeis, P., and Restelli, M. Challenging common assumptions in convex rein- forcement learning. arXiv preprint arXiv:2202.01511 , 2022.",
            "Belghazi, M. I., Baratin, A., Rajeswar, S., Ozair, S., Bengio, Y., Courville, A., and Hjelm, R. D. Mine: mutual informa- tion neural estimation. arXiv preprint arXiv:1801.04062 , 2018.",
            "Torabi, F., Warnell, G., and Stone, P. Dealio: Data-ef\ufb01cient adversarial learning for imitation from observation. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , pp. 2391\u20132397. IEEE, 2021.",
            "Ziebart, B. D., Maas, A. L., Bagnell, J. A., Dey, A. K., et al. Maximum entropy inverse reinforcement learning. 2008.",
            "Zahavy, T., O\u2019Donoghue, B., Desjardins, G., and Singh, S. Reward is enough for convex mdps. Advances in Neural Information Processing Systems , 34:25746\u201325759, 2021.",
            "Nota, C. and Thomas, P. S. Is the policy gradient a gradient? arXiv preprint arXiv:1906.07073 , 2019.",
            "Kim, K., Jindal, A., Song, Y., Song, J., Sui, Y., and Ermon, S. Imitation with neural density models. Advances in Neural Information Processing Systems , 34:5360\u20135372, 2021b.",
            "Fujimoto, S., Van Hoof, H., and Meger, D. Addressing func- tion approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477 , 2018.",
            "Fu, J., Luo, K., and Levine, S. Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248 , 2017.",
            "Chang, W.-D., Higuera, J. C. G., Fujimoto, S., Meger, D., and Dudek, G. Il-\ufb02ow: Imitation learning from observation using normalizing \ufb02ows. arXiv preprint arXiv:2205.09251 , 2022.",
            "preprint arXiv:2006.08545 , 2020. Kobyzev, I., Prince, S. J., and Brubaker, M. A. Normalizing \ufb02ows: An introduction and review of current methods.",
            "of certain markov process expectations for large time\u2014iii. Communications on pure and applied Mathematics , 29 (4):389\u2013461, 1976.",
            "Torabi, F., Warnell, G., and Stone, P. Generative ad- versarial imitation from observation. arXiv preprint arXiv:1807.06158 , 2018."
        ]
    }
}