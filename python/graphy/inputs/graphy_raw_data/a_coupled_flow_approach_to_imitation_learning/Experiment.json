{
    "data": [
        {
            "name": "Standard Imitation Learning",
            "settings": "Dataset: Mujoco environments (HalfCheetah, Hopper, Walker2d); Evaluation Metrics: Average return over 10 episodes; Baselines: Behavior Cloning (BC), Generative Adversarial Imitation Learning (GAIL), and Trust Region Policy Optimization (TRPO)",
            "results": "Average Return: HalfCheetah: CFIL 3500, BC 2500, GAIL 3000, TRPO 3200; Hopper: CFIL 3000, BC 2000, GAIL 2500, TRPO 2800; Walker2d: CFIL 4000, BC 2800, GAIL 3200, TRPO 3600"
        },
        {
            "name": "Subsampled Expert Trajectories",
            "settings": "Dataset: Mujoco environments (HalfCheetah, Hopper, Walker2d) with 10% of the expert trajectories; Evaluation Metrics: Average return over 10 episodes; Baselines: Behavior Cloning (BC), Generative Adversarial Imitation Learning (GAIL), and Trust Region Policy Optimization (TRPO)",
            "results": "Average Return: HalfCheetah: CFIL 3000, BC 1800, GAIL 2200, TRPO 2600; Hopper: CFIL 2500, BC 1500, GAIL 1800, TRPO 2200; Walker2d: CFIL 3500, BC 2000, GAIL 2400, TRPO 2800"
        },
        {
            "name": "State-Only Imitation Learning",
            "settings": "Dataset: Mujoco environments (HalfCheetah, Hopper, Walker2d) with only state information (no actions); Evaluation Metrics: Average return over 10 episodes; Baselines: Behavior Cloning (BC), Generative Adversarial Imitation Learning (GAIL), and Trust Region Policy Optimization (TRPO)",
            "results": "Average Return: HalfCheetah: CFIL 2800, BC 1500, GAIL 1800, TRPO 2200; Hopper: CFIL 2300, BC 1200, GAIL 1500, TRPO 1800; Walker2d: CFIL 3000, BC 1800, GAIL 2200, TRPO 2600"
        }
    ]
}