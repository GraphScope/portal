**************QUERY***************: 
 
You are a highly skilled academic AI assistant. Your role is to respond to user queries with structured,
clear, and well-organized answers, maintaining an academic tone and focusing on relevance and clarity.

User Query: **Question**: List all contributions of the paper. These contributions are always organized and listed with a head sentence like **our contributions are as follows**. For each contribution, output the **original representation** and use a few words to summarize it.

Guidance:
Research papers typically contain key components, including the problem definition, challenges,
contributions, solutions, and experimental results. These components are generally organized as follows:
- **Problem Definition, Challenges, and Contributions**: Usually found within the first few sections.
- **Solutions**: Typically located in the main body of the paper.
- **Experiment Results**: Usually appear toward the end in sections titled "Experiments" or "Empirical Studies."

The content is retrieved in annotated chunks, marked with **SECTION_X** (indicating the specific section)
or **POS_0.XX** (indicating the position within the paper, calculated as current page/total pages).
Use these annotations to identify and focus on the sections most relevant to the userâ€™s query,
ensuring a precise and targeted response.
                             **************MEMORY**************: 
 **SECTION_1**: which comes with challenges see Section 5 . Our approach is novel in the fact that we consider the problem of minimizing the Wasserstein distance through its primal formulation. Crucially, the primal formulation prevents the minmax optimization problem, and requires little ne tuning. We introduce a reward function computed of ine based on an upper bound of the primal form of the Wasserstein distance. As the Wasserstein distance requires a distance between state action pairs, we
**SECTION_paper_meta**: Published as a conference paper at ICLR 2021 PRIMAL WASSERSTEIN IMITATION LEARNING Robert Dadashi 1, L onard Hussenot1,2, Matthieu Geist1, Olivier Pietquin1 1Google Research, Brain Team 2Univ. de Lille, CNRS, Inria Scool, UMR 9189 CRIStAL
**SECTION_1**: Published as a conference paper at ICLR 2021 In this work, we use the Wasserstein distance as a measure between the state action distributions of the expert and of the agent. Contrary to f divergences, the Wasserstein distance is a true distance, it is smooth and it is based on the geometry of the metric space it operates on. The Wasserstein distance has gained popularity in GAN approaches Arjovsky et al., 2017 through its dual formulation
**SECTION_1**: arXiv:2006.04678v2 cs.LG 17 Mar 2021
**SECTION_1**: that matches the expert behavior in some sense. In IRL, we assume that the demonstrations come from an agent that acts optimally with respect to an unknown reward function that we seek to recover, to subsequently train an agent on it. Although IRL methods introduce an intermediary problem i.e. recovering the environment s reward they are less sensitive to distributional shift Pomerleau, 1991 , they generalize to environments with different dynamics Piot et al., 2013 , and they can recover a
**SECTION_abstract**: ABSTRACT Imitation Learning IL methods seek to match the behavior of an agent with that of an expert. In the present work, we propose a new IL method based on a conceptually simple algorithm: Primal Wasserstein Imitation Learning PWIL , which ties to the primal form of the Wasserstein distance between the expert and the agent state action distributions. We present a reward function which is derived of ine, as opposed to recent adversarial IL algorithms that learn a reward function
**SECTION_1**: 2018 interleave the learning of the reward function with the learning process of the agent. Adversarial IL methods are based on an adversarial training paradigm similar to Generative Adversarial Networks GANs Goodfellow et al., 2014 , where the learned reward function can be thought of as the confusion of a discriminator that learns to differentiate expert transitions from non expert ones. These methods are well suited to the IL problem since they implicitly minimize an f divergence
**SECTION_1**: the true return of the task we consider as it is unknown in general . Our method recovers expert behaviour comparably to existing state of the art methods while being based on signi cantly fewer hyperparameters; it operates even in the extreme low data regime of demonstrations, and is the rst method that makes Humanoid run with a single subsampled demonstration.
**SECTION_1**: with hard to specify rewards: we seek to solve a task by learning a policy from a xed number of demonstrations generated by an expert. IL methods can typically be folded into two paradigms: Behavioral Cloning, or BC Pomerleau, 1991; Bagnell et al., 2007; Ross Bagnell, 2010 and Inverse Reinforcement Learning, or IRL Russell, 1998; Ng et al., 2000 . In BC, we seek to recover the expert s behavior by directly learning a policy
**SECTION_1**: show that it can be hand de ned for locomotion tasks, and that it can be learned from pixels for a hand manipulation task. The inferred reward function is non stationary, like adversarial IL methods, but it is not re evaluated as the agent interacts with the environment, therefore the reward function we de ne is computed of ine. We present a true distance to compare the behavior of the expert and the behavior of the agent, rather than using the common proxy of performance with respect to
**SECTION_abstract**: through interactions with the environment, and which requires little ne tuning. We show that we can recover expert behavior on a variety of continuous control tasks of the MuJoCo domain in a sample ef cient manner in terms of agent interactions and of expert interactions with the environment. Finally, we show that the behavior of the agent we train matches the behavior of the expert with the Wasserstein distance, rather than the commonly used proxy of performance.
**SECTION_1**: near optimal agent from suboptimal demonstrations Brown et al., 2019; Jacq et al., 2019 . However, IRL methods are usually based on an iterative process alternating between reward estimation and RL, which might result in poor sample ef ciency. Earlier IRL methods Ng et al., 2000; Abbeel Ng, 2004; Ziebart et al., 2008 require multiple calls to a Markov decision process solver Puterman, 2014 , whereas recent adversarial IL approaches Finn et al., 2016; Ho Ermon, 2016; Fu et al.,
**SECTION_1**: between the state action distribution of an expert and the state action distribution of the learning agent Ghasemipour et al., 2019; Ke et al., 2019 . However the interaction between a generator the policy and the discriminator the reward function makes it a minmax optimization problem, and therefore comes with practical challenges that might include training instability, sensitivity to hyperparameters and poor sample ef ciency. Correspondence to Robert Dadashi: dadashi google.com. 1
**SECTION_1**: 1 INTRODUCTION Reinforcement Learning RL has solved a number of dif cult tasks whether in games Tesauro, 1995; Mnih et al., 2015; Silver et al., 2016 or robotics Abbeel Ng, 2004; Andrychowicz et al., 2020 . However, RL relies on the existence of a reward function, that can be either hard to specify or too sparse to be used in practice. Imitation Learning IL is a paradigm that applies to these environments
