{
    "data": {
        "id": "1703.04691v5",
        "published": "2018-09-18T01:22:25+00:00",
        "year": 2018,
        "month": 9,
        "title": "Conditional time series forecasting with convolutional neural",
        "authors": [
            "Anastasia Borovykh",
            "Sander Bohte",
            "Cornelis W. Oosterlee"
        ],
        "summary": "We present a method for conditional time series forecasting based on an adaptation of the recent deep convolutional WaveNet architecture. The proposed network contains stacks of dilated convolutions that allow it to access a broad range of history when forecasting, a ReLU activation function and conditioning is performed by applying multiple convolutional filters in parallel to separate time series which allows for the fast processing of data and the exploitation of the correlation structure between the multivariate time series. We test and analyze the performance of the convolutional network both unconditionally as well as conditionally for financial time series forecasting using the S&P500, the volatility index, the CBOE interest rate and several exchange rates and extensively compare it to the performance of the well-known autoregressive model and a long-short term memory network. We show that a convolutional network is well-suited for regression-type problems and is able to effectively learn dependencies in and between the series without the need for long historical time series, is a time-efficient and easy to implement alternative to recurrent-type networks and tends to outperform linear and recurrent models.",
        "journal_ref": null,
        "doi": null,
        "primary_category": "stat.ML",
        "categories": [
            "stat.ML"
        ],
        "bib": "@article{1703.04691v5,\\nAuthor        = {Anastasia Borovykh and Sander Bohte and Cornelis W. Oosterlee},\\nTitle         = {Conditional Time Series Forecasting with Convolutional Neural Networks},\\nEprint        = {http://arxiv.org/abs/1703.04691v5},\\nArchivePrefix = {arXiv},\\nPrimaryClass  = {stat.ML},\\nAbstract      = {We present a method for conditional time series forecasting based on an\\nadaptation of the recent deep convolutional WaveNet architecture. The proposed\\nnetwork contains stacks of dilated convolutions that allow it to access a broad\\nrange of history when forecasting, a ReLU activation function and conditioning\\nis performed by applying multiple convolutional filters in parallel to separate\\ntime series which allows for the fast processing of data and the exploitation\\nof the correlation structure between the multivariate time series. We test and\\nanalyze the performance of the convolutional network both unconditionally as\\nwell as conditionally for financial time series forecasting using the S&P500,\\nthe volatility index, the CBOE interest rate and several exchange rates and\\nextensively compare it to the performance of the well-known autoregressive\\nmodel and a long-short term memory network. We show that a convolutional\\nnetwork is well-suited for regression-type problems and is able to effectively\\nlearn dependencies in and between the series without the need for long\\nhistorical time series, is a time-efficient and easy to implement alternative\\nto recurrent-type networks and tends to outperform linear and recurrent models.},\\nYear          = {2017},\\nMonth         = {3},\\nUrl           = {http://arxiv.org/pdf/1703.04691v5},\\nFile          = {1703.04691v5.pdf}\\n}",
        "reference": [
            "[8] X. Glorot and Y. Bengio , Understanding the Di\ufb03culty of Training Deep Feedforward Neural Net- works , Proceedings of the 13th International Conference on Arti\ufb01cial Intelligence and Statistics, (2010).",
            "[30] Y. Zheng, Q. Liu, E. Chen, Y. Ge, and J. Zhao , Exploiting Multi-Channels Deep Convolutional Neural Networks for Multivariate Time Series Classi\ufb01cation , Front. Comput. Sci., 10 (2016), pp. 96\u2013112.",
            "[6] R. Cont , Empirical properties of asset returns: Stylized facts and statistical issues , (2001).",
            "[12] S. Hochreiter and J. Schmidhuber , Long Short-Term Memory , Neural computation, 9 (1997), pp. 1735\u20131780.",
            "[20] R. Mittelman , Time-series modeling with undecimated fully convolutional neural networks , arXiv preprint arXiv:1508.00317, (2015).",
            "[19] M. Mathieu, M. Henaff, and Y. LeCun , Fast training of convolutional networks through FFTs , ArXiv e-prints, (2013).",
            "[7] T. Fisher and C. Krauss , Deep learning with Long Short-Term Memory networks for \ufb01nancial market predictions , FAU Discussion papers in Economics, (2017).",
            "[1] A. Aussem and F. Murtagh , Combining neural network forecasts on wavelet-transformed time series , Connection Science, 9 (1997), pp. 113\u2013122.",
            "[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton , ImageNet Classi\ufb01cation with Deep Convolutional Neural Networks , Advances in Neural Information Processing Systems 25, (2012), pp. 1097\u20131105.",
            "[21] P. Ramachandran, T. L. Paine, P. Khorrami, M. Babaeizadeh, S. Chang, Y. Zhang, M. A. Hasegawa-Johnson, R. H. Campbell, and T. S. Huang , Fast generation for convolutional autore- gressive models , arXiv preprint arXiv:1704.06001, (2017).",
            "[27] F. Yu and V. Koltun , Multi-Scale Context Aggregation by Dilated Convolutions , ArXiv e-prints, (2015).",
            "[23] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalch- brenner, A. Senior, and K. Kavukcuoglu , WaveNet: A Generative Model for Raw Audio , ArXiv e-prints, (2016).",
            "[15] D. Kingma and J. Ba , Adam: A method for stochastic optimization , arXiv preprint arXiv:1412.6980, (2014).",
            "[10] K. He, X. Zhang, S. Ren, and J. Sun , Delving deep into recti\ufb01ers: Surpassing human-level per- formance on imagenet classi\ufb01cation , in Proceedings of the IEEE international conference on computer vision, 2015, pp. 1026\u20131034.",
            "[9] J. D. Hamilton , Time series analysis , vol. 2, Princeton university press Princeton, 1994.",
            "[11] , Deep residual learning for image recognition , in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770\u2013778.",
            "[22] O. Rippel, J. Snoek, and R. P. Adams , Spectral representations for convolutional neural networks , NIPS15 Proceedings of the 28th International Conference on Neural Information Processing Systems, (2015), pp. 2449\u20132457.",
            "[24] A. van den Oord, N. Kalchbrenner, and K. Kavukcuoglu , Pixel Recurrent Neural Networks , CoRR, abs/1601.06759 (2016).",
            "[3] M. Binkowski, G. Marti, and P. Donnat , Autoregressive convolutional neural networks for asyn- chronous time series , ICML 2017 Time Series Workshop, (2017).",
            "[13] K. Hornik , Approximation capabilities of multilayer feedforward networks , Neural networks, 4 (1991), pp. 251\u2013257.",
            "[5] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio , Empirical evaluation of gated recurrent neural networks on sequence modeling , arXiv preprint arXiv:1412.3555, (2014).",
            "[29] G. P. Zhang , Time series forecasting using a hybrid ARIMA and neural network model , Neurocom- puting, 50 (2003), pp. 159\u2013175.",
            "[18] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner , Gradient-based learning applied to document recognition , Proceedings of the IEEE, 86 (1998), pp. 2278\u20132324.",
            "[26] Z. Wang, W. Yan, and T. Oates , Time Series Classi\ufb01cation from Scratch with Deep Neural Net- works: A Strong Baseline , CoRR, abs/1611.06455 (2016).",
            "[2] Y. Bengio, P. Simard, and P. Frasconi , Learning Long-Term Dependencies with Gradient Descent is Di\ufb03cult , IEEE Transactions on Neural Networks, 5 (1994).",
            "[14] D. Hsu , Time series forecasting based on augmented Long Short-Term Memory , arXiv preprint arXiv:1707.00666, (2017).",
            "[17] S. Lahmiri , Wavelet low- and high- frequency components as features for predicting stock prices with backpropagation neural networks , Journal of King Saud University - Computer and Information Sciences, 26 (2014), pp. 218\u2013227.",
            "[4] K. Chakraborty, K. Mehrotra, C. K. Mohan, and S. Ranka , Forecasting the Behavior of Multivariate Time Series using Neural Networks , Neural networks, 5 (1992), pp. 961\u2013970.",
            "[28] G. Zhang, B. E. Patuwo, and M. Y. Hu , Forecasting with arti\ufb01cial neural networks: The state of the art , International journal of forecasting, 14 (1998), pp. 35\u201362.",
            "3.1 An arti\ufb01cial example In order to show the ability of the model to learn both linear and non-linear dependencies in and between time series, we train and test the model on the chaotic Lorenz system. The Lorenz map is de\ufb01ned as the solution ( X, Y, Z ) to a system of ordinary di\ufb00erential equations (ODEs) given by"
        ]
    }
}