@article{1910.02054v3, Author        = {Samyam Rajbhandari and Jeff Rasley and Olatunji Ruwase and Yuxiong He}, Title         = {ZeRO: Memory Optimizations Toward Training Trillion Parameter Models}, Eprint        = {http://arxiv.org/abs/1910.02054v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware.   We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create the world's largest language model (Turing-NLG, 17B parameters) with record breaking accuracy.}, Year          = {2019}, Month         = {10}, Url           = {http://arxiv.org/pdf/1910.02054v3}, File          = {1910.02054v3.pdf} }
 
@article{1907.11692v1, Author        = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov}, Title         = {RoBERTa: A Robustly Optimized BERT Pretraining Approach}, Eprint        = {http://arxiv.org/abs/1907.11692v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.}, Year          = {2019}, Month         = {7}, Url           = {http://arxiv.org/pdf/1907.11692v1}, File          = {1907.11692v1.pdf} }
 
@article{2303.09540v3, Author        = {Amro Abbas and Kushal Tirumala and DÃ¡niel Simig and Surya Ganguli and Ari S. Morcos}, Title         = {SemDeDup: Data-efficient learning at web-scale through semantic deduplication}, Eprint        = {http://arxiv.org/abs/2303.09540v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {Progress in machine learning has been driven in large part by massive increases in data. However, large web-scale datasets such as LAION are largely uncurated beyond searches for exact duplicates, potentially leaving much redundancy. Here, we introduce SemDeDup, a method which leverages embeddings from pre-trained models to identify and remove semantic duplicates: data pairs which are semantically similar, but not exactly identical. Removing semantic duplicates preserves performance and speeds up learning. Analyzing a subset of LAION, we show that SemDeDup can remove 50% of the data with minimal performance loss, effectively halving training time. Moreover, performance increases out of distribution. Also, analyzing language models trained on C4, a partially curated dataset, we show that SemDeDup improves over prior approaches while providing efficiency gains. SemDeDup provides an example of how simple ways of leveraging quality embeddings can be used to make models learn faster with less data.}, Year          = {2023}, Month         = {3}, Url           = {http://arxiv.org/pdf/2303.09540v3}, File          = {2303.09540v3.pdf} }
 
@article{2402.13718v3, Author        = {Xinrong Zhang and Yingfa Chen and Shengding Hu and Zihang Xu and Junhao Chen and Moo Khai Hao and Xu Han and Zhen Leng Thai and Shuo Wang and Zhiyuan Liu and Maosong Sun}, Title         = {$\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens}, Eprint        = {http://arxiv.org/abs/2402.13718v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose $\infty$Bench, the first LLM benchmark featuring an average data length surpassing 100K tokens. $\infty$Bench comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in $\infty$Bench are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. In our experiments, based on $\infty$Bench, we evaluate the state-of-the-art proprietary and open-source LLMs tailored for processing long contexts. The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context. We further present three intriguing analyses regarding the behavior of LLMs processing long context.}, Year          = {2024}, Month         = {2}, Url           = {http://arxiv.org/pdf/2402.13718v3}, File          = {2402.13718v3.pdf} }
 
@article{2304.11277v2, Author        = {Yanli Zhao and Andrew Gu and Rohan Varma and Liang Luo and Chien-Chin Huang and Min Xu and Less Wright and Hamid Shojanazeri and Myle Ott and Sam Shleifer and Alban Desmaison and Can Balioglu and Pritam Damania and Bernard Nguyen and Geeta Chauhan and Yuchen Hao and Ajit Mathews and Shen Li}, Title         = {PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel}, Eprint        = {http://arxiv.org/abs/2304.11277v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.DC}, Abstract      = {It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The experimental results demonstrate that FSDP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of TFLOPS.}, Year          = {2023}, Month         = {4}, Url           = {http://arxiv.org/pdf/2304.11277v2}, File          = {2304.11277v2.pdf} }
 
@article{1811.06965v5, Author        = {Yanping Huang and Youlong Cheng and Ankur Bapna and Orhan Firat and Mia Xu Chen and Dehao Chen and HyoukJoong Lee and Jiquan Ngiam and Quoc V. Le and Yonghui Wu and Zhifeng Chen}, Title         = {GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism}, Eprint        = {http://arxiv.org/abs/1811.06965v5}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.}, Year          = {2018}, Month         = {11}, Url           = {http://arxiv.org/pdf/1811.06965v5}, File          = {1811.06965v5.pdf} }
 
@article{2208.03306v1, Author        = {Margaret Li and Suchin Gururangan and Tim Dettmers and Mike Lewis and Tim Althoff and Noah A. Smith and Luke Zettlemoyer}, Title         = {Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models}, Eprint        = {http://arxiv.org/abs/2208.03306v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We present Branch-Train-Merge (BTM), a communication-efficient algorithm for embarrassingly parallel training of large language models (LLMs). We show it is possible to independently train subparts of a new class of LLMs on different subsets of the data, eliminating the massive multi-node synchronization currently required to train LLMs. BTM learns a set of independent expert LMs (ELMs), each specialized to a different textual domain, such as scientific or legal text. These ELMs can be added and removed to update data coverage, ensembled to generalize to new domains, or averaged to collapse back to a single LM for efficient inference. New ELMs are learned by branching from (mixtures of) ELMs in the current set, further training the parameters on data for the new domain, and then merging the resulting model back into the set for future use. Experiments show that BTM improves in- and out-of-domain perplexities as compared to GPT-style Transformer LMs, when controlling for training cost. Through extensive analysis, we show that these results are robust to different ELM initialization schemes, but require expert domain specialization; LM ensembles with random data splits do not perform well. We also present a study of scaling BTM into a new corpus of 64 domains (192B whitespace-separated tokens in total); the resulting LM (22.4B total parameters) performs as well as a Transformer LM trained with 2.5 times more compute. These gains grow with the number of domains, suggesting more aggressive parallelism could be used to efficiently train larger models in future work.}, Year          = {2022}, Month         = {8}, Url           = {http://arxiv.org/pdf/2208.03306v1}, File          = {2208.03306v1.pdf} }
 
@article{2203.05482v3, Author        = {Mitchell Wortsman and Gabriel Ilharco and Samir Yitzhak Gadre and Rebecca Roelofs and Raphael Gontijo-Lopes and Ari S. Morcos and Hongseok Namkoong and Ali Farhadi and Yair Carmon and Simon Kornblith and Ludwig Schmidt}, Title         = {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time}, Eprint        = {http://arxiv.org/abs/2203.05482v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results "model soups."" When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.}, Year          = {2022}, Month         = {3}, Url           = {http://arxiv.org/pdf/2203.05482v3}, File          = {2203.05482v3.pdf} }"
 
@article{2311.17035v1, Author        = {Milad Nasr and Nicholas Carlini and Jonathan Hayase and Matthew Jagielski and A. Feder Cooper and Daphne Ippolito and Christopher A. Choquette-Choo and Eric Wallace and Florian TramÃ¨r and Katherine Lee}, Title         = {Scalable Extraction of Training Data from (Production) Language Models}, Eprint        = {http://arxiv.org/abs/2311.17035v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.}, Year          = {2023}, Month         = {11}, Url           = {http://arxiv.org/pdf/2311.17035v1}, File          = {2311.17035v1.pdf} }
 
@article{2211.15533v1, Author        = {Denis Kocetkov and Raymond Li and Loubna Ben Allal and Jia Li and Chenghao Mou and Carlos MuÃ±oz Ferrandis and Yacine Jernite and Margaret Mitchell and Sean Hughes and Thomas Wolf and Dzmitry Bahdanau and Leandro von Werra and Harm de Vries}, Title         = {The Stack: 3 TB of permissively licensed source code}, Eprint        = {http://arxiv.org/abs/2211.15533v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called "Am I in The Stack"" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.}, Year          = {2022}, Month         = {11}, Url           = {http://arxiv.org/pdf/2211.15533v1}, File          = {2211.15533v1.pdf} }"
 
@article{2212.03860v3, Author        = {Gowthami Somepalli and Vasu Singla and Micah Goldblum and Jonas Geiping and Tom Goldstein}, Title         = {Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models}, Eprint        = {http://arxiv.org/abs/2212.03860v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data.}, Year          = {2022}, Month         = {12}, Url           = {http://arxiv.org/pdf/2212.03860v3}, File          = {2212.03860v3.pdf} }
 
@article{2404.14219v4, Author        = {Marah Abdin and Jyoti Aneja and Hany Awadalla and Ahmed Awadallah and Ammar Ahmad Awan and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Jianmin Bao and Harkirat Behl and Alon Benhaim and Misha Bilenko and Johan Bjorck and SÃ©bastien Bubeck and Martin Cai and Qin Cai and Vishrav Chaudhary and Dong Chen and Dongdong Chen and Weizhu Chen and Yen-Chun Chen and Yi-Ling Chen and Hao Cheng and Parul Chopra and Xiyang Dai and Matthew Dixon and Ronen Eldan and Victor Fragoso and Jianfeng Gao and Mei Gao and Min Gao and Amit Garg and Allie Del Giorno and Abhishek Goswami and Suriya Gunasekar and Emman Haider and Junheng Hao and Russell J. Hewett and Wenxiang Hu and Jamie Huynh and Dan Iter and Sam Ade Jacobs and Mojan Javaheripi and Xin Jin and Nikos Karampatziakis and Piero Kauffmann and Mahoud Khademi and Dongwoo Kim and Young Jin Kim and Lev Kurilenko and James R. Lee and Yin Tat Lee and Yuanzhi Li and Yunsheng Li and Chen Liang and Lars Liden and Xihui Lin and Zeqi Lin and Ce Liu and Liyuan Liu and Mengchen Liu and Weishung Liu and Xiaodong Liu and Chong Luo and Piyush Madan and Ali Mahmoudzadeh and David Majercak and Matt Mazzola and Caio CÃ©sar Teodoro Mendes and Arindam Mitra and Hardik Modi and Anh Nguyen and Brandon Norick and Barun Patra and Daniel Perez-Becker and Thomas Portet and Reid Pryzant and Heyang Qin and Marko Radmilac and Liliang Ren and Gustavo de Rosa and Corby Rosset and Sambudha Roy and Olatunji Ruwase and Olli Saarikivi and Amin Saied and Adil Salim and Michael Santacroce and Shital Shah and Ning Shang and Hiteshi Sharma and Yelong Shen and Swadheen Shukla and Xia Song and Masahiro Tanaka and Andrea Tupini and Praneetha Vaddamanu and Chunyu Wang and Guanhua Wang and Lijuan Wang and Shuohang Wang and Xin Wang and Yu Wang and Rachel Ward and Wen Wen and Philipp Witte and Haiping Wu and Xiaoxia Wu and Michael Wyatt and Bin Xiao and Can Xu and Jiahang Xu and Weijian Xu and Jilong Xue and Sonali Yadav and Fan Yang and Jianwei Yang and Yifan Yang and Ziyi Yang and Donghan Yu and Lu Yuan and Chenruidong Zhang and Cyril Zhang and Jianwen Zhang and Li Lyna Zhang and Yi Zhang and Yue Zhang and Yunan Zhang and Xiren Zhou}, Title         = {Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone}, Eprint        = {http://arxiv.org/abs/2404.14219v4}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.}, Year          = {2024}, Month         = {4}, Url           = {http://arxiv.org/pdf/2404.14219v4}, File          = {2404.14219v4.pdf} }
 
@article{2112.09332v3, Author        = {Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman}, Title         = {WebGPT: Browser-assisted question-answering with human feedback}, Eprint        = {http://arxiv.org/abs/2112.09332v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.}, Year          = {2021}, Month         = {12}, Url           = {http://arxiv.org/pdf/2112.09332v3}, File          = {2112.09332v3.pdf} }
 
@article{2205.12255v1, Author        = {Aaron Parisi and Yao Zhao and Noah Fiedel}, Title         = {TALM: Tool Augmented Language Models}, Eprint        = {http://arxiv.org/abs/2205.12255v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Transformer based language models (LMs) demonstrate increasing performance with scale across a wide variety of tasks. Scale alone however cannot enable models to solve tasks that require access to ephemeral, changing, or private data that was unavailable at training time. Many useful tasks may also benefit from LMs being able to access APIs that read or modify state. In this work, we present Tool Augmented Language Models (TALM), combining a text-only approach to augment language models with non-differentiable tools, and an iterative "self-play"" technique to bootstrap performance starting from few tool demonstrations. TALM exhibits strong performance on both a knowledge-heavy QA task and a reasoning oriented math task with simple tools. At a given model scale, TALM significantly outperforms non-augmented LMs. We further demonstrate that TALM successfully performs out-of-distribution inferences on both QA and math tasks, where non-augmented LMs fail. Our results suggest that Tool Augmented Language Models are a promising direction to enrich LMs' capabilities, with less dependence on scale.}, Year          = {2022}, Month         = {5}, Url           = {http://arxiv.org/pdf/2205.12255v1}, File          = {2205.12255v1.pdf} }"
 
@article{1904.09728v3, Author        = {Maarten Sap and Hannah Rashkin and Derek Chen and Ronan LeBras and Yejin Choi}, Title         = {SocialIQA: Commonsense Reasoning about Social Interactions}, Eprint        = {http://arxiv.org/abs/1904.09728v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We introduce Social IQa, the first largescale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: "Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?"" A: ""Make sure no one else could hear""). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance (>20% gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).}, Year          = {2019}, Month         = {4}, Url           = {http://arxiv.org/pdf/1904.09728v3}, File          = {1904.09728v3.pdf} }"
 
@article{1811.00937v2, Author        = {Alon Talmor and Jonathan Herzig and Nicholas Lourie and Jonathan Berant}, Title         = {CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge}, Eprint        = {http://arxiv.org/abs/1811.00937v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy, well below human performance, which is 89%.}, Year          = {2018}, Month         = {11}, Url           = {http://arxiv.org/pdf/1811.00937v2}, File          = {1811.00937v2.pdf} }
 
@article{2210.09261v1, Author        = {Mirac Suzgun and Nathan Scales and Nathanael SchÃ¤rli and Sebastian Gehrmann and Yi Tay and Hyung Won Chung and Aakanksha Chowdhery and Quoc V. Le and Ed H. Chi and Denny Zhou and Jason Wei}, Title         = {Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them}, Eprint        = {http://arxiv.org/abs/2210.09261v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?   In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.}, Year          = {2022}, Month         = {10}, Url           = {http://arxiv.org/pdf/2210.09261v1}, File          = {2210.09261v1.pdf} }
 
@article{2403.04706v1, Author        = {Chen Li and Weiqi Wang and Jingcheng Hu and Yixuan Wei and Nanning Zheng and Han Hu and Zheng Zhang and Houwen Peng}, Title         = {Common 7B Language Models Already Possess Strong Math Capabilities}, Eprint        = {http://arxiv.org/abs/2403.04706v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training. This paper shows that the LLaMA-2 7B model with common pre-training already exhibits strong mathematical abilities, as evidenced by its impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks, respectively, when selecting the best response from 256 random generations. The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities. Notably, the accuracy for the first answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks, respectively. We find that simply scaling up the SFT data can significantly enhance the reliability of generating correct answers. However, the potential for extensive scaling is constrained by the scarcity of publicly available math questions. To overcome this limitation, we employ synthetic data, which proves to be nearly as effective as real data and shows no clear saturation when scaled up to approximately one million samples. This straightforward approach achieves an accuracy of 82.6% on GSM8K and 40.6% on MATH using LLaMA-2 7B models, surpassing previous models by 14.2% and 20.8%, respectively. We also provide insights into scaling behaviors across different reasoning complexities and error types.}, Year          = {2024}, Month         = {3}, Url           = {http://arxiv.org/pdf/2403.04706v1}, File          = {2403.04706v1.pdf} }
 
@article{2402.19255v2, Author        = {Qintong Li and Leyang Cui and Xueliang Zhao and Lingpeng Kong and Wei Bi}, Title         = {GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers}, Eprint        = {http://arxiv.org/abs/2402.19255v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs' math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (GSM-Plus) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes when new statements are added or the question targets are altered. We also explore whether more robust performance can be achieved by composing existing prompting methods, in which we try an iterative method that generates and verifies each intermediate thought based on its reasoning goal and calculation result.}, Year          = {2024}, Month         = {2}, Url           = {http://arxiv.org/pdf/2402.19255v2}, File          = {2402.19255v2.pdf} }
 
@article{2405.12205v1, Author        = {Aniket Didolkar and Anirudh Goyal and Nan Rosemary Ke and Siyuan Guo and Michal Valko and Timothy Lillicrap and Danilo Rezende and Yoshua Bengio and Michael Mozer and Sanjeev Arora}, Title         = {Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving}, Eprint        = {http://arxiv.org/abs/2405.12205v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.AI}, Abstract      = {Metacognitive knowledge refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans.   To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH. (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label. This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic, even though this article applies it to math problems.}, Year          = {2024}, Month         = {5}, Url           = {http://arxiv.org/pdf/2405.12205v1}, File          = {2405.12205v1.pdf} }
 
@article{2303.17651v2, Author        = {Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Shashank Gupta and Bodhisattwa Prasad Majumder and Katherine Hermann and Sean Welleck and Amir Yazdanbakhsh and Peter Clark}, Title         = {Self-Refine: Iterative Refinement with Self-Feedback}, Eprint        = {http://arxiv.org/abs/2303.17651v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.}, Year          = {2023}, Month         = {3}, Url           = {http://arxiv.org/pdf/2303.17651v2}, File          = {2303.17651v2.pdf} }
 
@article{2310.13486v1, Author        = {Lucas Weber and Elia Bruni and Dieuwke Hupkes}, Title         = {Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning}, Eprint        = {http://arxiv.org/abs/2310.13486v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Finding the best way of adapting pre-trained language models to a task is a big challenge in current NLP. Just like the previous generation of task-tuned models (TT), models that are adapted to tasks via in-context-learning (ICL) are robust in some setups but not in others. Here, we present a detailed analysis of which design choices cause instabilities and inconsistencies in LLM predictions. First, we show how spurious correlations between input distributions and labels -- a known issue in TT models -- form only a minor problem for prompted models. Then, we engage in a systematic, holistic evaluation of different factors that have been found to influence predictions in a prompting setup. We test all possible combinations of a range of factors on both vanilla and instruction-tuned (IT) LLMs of different scale and statistically analyse the results to show which factors are the most influential, interactive or stable. Our results show which factors can be used without precautions and which should be avoided or handled with care in most settings.}, Year          = {2023}, Month         = {10}, Url           = {http://arxiv.org/pdf/2310.13486v1}, File          = {2310.13486v1.pdf} }
 
@article{2308.03188v2, Author        = {Liangming Pan and Michael Saxon and Wenda Xu and Deepak Nathani and Xinyi Wang and William Yang Wang}, Title         = {Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies}, Eprint        = {http://arxiv.org/abs/2308.03188v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.}, Year          = {2023}, Month         = {8}, Url           = {http://arxiv.org/pdf/2308.03188v2}, File          = {2308.03188v2.pdf} }
 
@article{2405.00451v2, Author        = {Yuxi Xie and Anirudh Goyal and Wenyue Zheng and Min-Yen Kan and Timothy P. Lillicrap and Kenji Kawaguchi and Michael Shieh}, Title         = {Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning}, Eprint        = {http://arxiv.org/abs/2405.00451v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.AI}, Abstract      = {We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to $81.8\%$ (+$5.9\%$), $34.7\%$ (+$5.8\%$), and $76.4\%$ (+$15.8\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains. Our code is publicly available at https://github.com/YuxiXie/MCTS-DPO.}, Year          = {2024}, Month         = {5}, Url           = {http://arxiv.org/pdf/2405.00451v2}, File          = {2405.00451v2.pdf} }
 
@article{2103.00020v1, Author        = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever}, Title         = {Learning Transferable Visual Models From Natural Language Supervision}, Eprint        = {http://arxiv.org/abs/2103.00020v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.}, Year          = {2021}, Month         = {2}, Url           = {http://arxiv.org/pdf/2103.00020v1}, File          = {2103.00020v1.pdf} }
 
@article{2311.10122v3, Author        = {Bin Lin and Yang Ye and Bin Zhu and Jiaxi Cui and Munan Ning and Peng Jin and Li Yuan}, Title         = {Video-LLaVA: Learning United Visual Representation by Alignment Before Projection}, Eprint        = {http://arxiv.org/abs/2311.10122v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {The Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers. In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other. Video-LLaVA achieves superior performances on a broad range of 9 image benchmarks across 5 image question-answering datasets and 4 image benchmark toolkits. Additionally, our Video-LLaVA also outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos. We aim for this work to provide modest insights into the multi-modal inputs for the LLM. Code address: \href{https://github.com/PKU-YuanGroup/Video-LLaVA}}, Year          = {2023}, Month         = {11}, Url           = {http://arxiv.org/pdf/2311.10122v3}, File          = {2311.10122v3.pdf} }
 
@article{2311.06242v1, Author        = {Bin Xiao and Haiping Wu and Weijian Xu and Xiyang Dai and Houdong Hu and Yumao Lu and Michael Zeng and Ce Liu and Lu Yuan}, Title         = {Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks}, Eprint        = {http://arxiv.org/abs/2311.06242v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {We introduce Florence-2, a novel vision foundation model with a unified, prompt-based representation for a variety of computer vision and vision-language tasks. While existing large vision models excel in transfer learning, they struggle to perform a diversity of tasks with simple instructions, a capability that implies handling the complexity of various spatial hierarchy and semantic granularity. Florence-2 was designed to take text-prompt as task instructions and generate desirable results in text forms, whether it be captioning, object detection, grounding or segmentation. This multi-task learning setup demands large-scale, high-quality annotated data. To this end, we co-developed FLD-5B that consists of 5.4 billion comprehensive visual annotations on 126 million images, using an iterative strategy of automated image annotation and model refinement. We adopted a sequence-to-sequence structure to train Florence-2 to perform versatile and comprehensive vision tasks. Extensive evaluations on numerous tasks demonstrated Florence-2 to be a strong vision foundation model contender with unprecedented zero-shot and fine-tuning capabilities.}, Year          = {2023}, Month         = {11}, Url           = {http://arxiv.org/pdf/2311.06242v1}, File          = {2311.06242v1.pdf} }
 
@article{1603.07396v1, Author        = {Aniruddha Kembhavi and Mike Salvato and Eric Kolve and Minjoon Seo and Hannaneh Hajishirzi and Ali Farhadi}, Title         = {A Diagram Is Worth A Dozen Images}, Eprint        = {http://arxiv.org/abs/1603.07396v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {Diagrams are common tools for representing complex concepts, relationships and events, often when it would be difficult to portray the same information with natural images. Understanding natural images has been extensively studied in computer vision, while diagram understanding has received little attention. In this paper, we study the problem of diagram interpretation and reasoning, the challenging task of identifying the structure of a diagram and the semantics of its constituents and their relationships. We introduce Diagram Parse Graphs (DPG) as our representation to model the structure of diagrams. We define syntactic parsing of diagrams as learning to infer DPGs for diagrams and study semantic interpretation and reasoning of diagrams in the context of diagram question answering. We devise an LSTM-based method for syntactic parsing of diagrams and introduce a DPG-based attention model for diagram question answering. We compile a new dataset of diagrams with exhaustive annotations of constituents and relationships for over 5,000 diagrams and 15,000 questions and answers. Our results show the significance of our models for syntactic parsing and question answering in diagrams using DPGs.}, Year          = {2016}, Month         = {3}, Url           = {http://arxiv.org/pdf/1603.07396v1}, File          = {1603.07396v1.pdf} }
 
@article{2301.13188v1, Author        = {Nicholas Carlini and Jamie Hayes and Milad Nasr and Matthew Jagielski and Vikash Sehwag and Florian TramÃ¨r and Borja Balle and Daphne Ippolito and Eric Wallace}, Title         = {Extracting Training Data from Diffusion Models}, Eprint        = {http://arxiv.org/abs/2301.13188v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CR}, Abstract      = {Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.}, Year          = {2023}, Month         = {1}, Url           = {http://arxiv.org/pdf/2301.13188v1}, File          = {2301.13188v1.pdf} }
 
@article{2107.06499v2, Author        = {Katherine Lee and Daphne Ippolito and Andrew Nystrom and Chiyuan Zhang and Douglas Eck and Chris Callison-Burch and Nicholas Carlini}, Title         = {Deduplicating Training Data Makes Language Models Better}, Eprint        = {http://arxiv.org/abs/2107.06499v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings. As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data. We develop two tools that allow us to deduplicate training datasets -- for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times. Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer train steps to achieve the same or better accuracy. We can also reduce train-test overlap, which affects over 4% of the validation set of standard datasets, thus allowing for more accurate evaluation. We release code for reproducing our work and performing dataset deduplication at https://github.com/google-research/deduplicate-text-datasets.}, Year          = {2021}, Month         = {7}, Url           = {http://arxiv.org/pdf/2107.06499v2}, File          = {2107.06499v2.pdf} }
 
@article{2309.06180v1, Author        = {Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica}, Title         = {Efficient Memory Management for Large Language Model Serving with PagedAttention}, Eprint        = {http://arxiv.org/abs/2309.06180v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm}, Year          = {2023}, Month         = {9}, Url           = {http://arxiv.org/pdf/2309.06180v1}, File          = {2309.06180v1.pdf} }
 
@article{2308.07074v2, Author        = {Keming Lu and Hongyi Yuan and Zheng Yuan and Runji Lin and Junyang Lin and Chuanqi Tan and Chang Zhou and Jingren Zhou}, Title         = {#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models}, Eprint        = {http://arxiv.org/abs/2308.07074v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Foundation language models obtain the instruction-following ability through supervised fine-tuning (SFT). Diversity and complexity are considered critical factors of a successful SFT dataset, while their definitions remain obscure and lack quantitative analyses. In this work, we propose InsTag, an open-set fine-grained tagger, to tag samples within SFT datasets based on semantics and intentions and define instruction diversity and complexity regarding tags. We obtain 6.6K tags to describe comprehensive user queries. Then we analyze popular open-sourced SFT datasets and find that the model ability grows with more diverse and complex data. Based on this observation, we propose a data selector based on InsTag to select 6K diverse and complex samples from open-source datasets and fine-tune models on InsTag-selected data. The resulting models, TagLM, outperform open-source models based on considerably larger SFT data evaluated by MT-Bench, echoing the importance of query diversity and complexity. We open-source InsTag in https://github.com/OFA-Sys/InsTag.}, Year          = {2023}, Month         = {8}, Url           = {http://arxiv.org/pdf/2308.07074v2}, File          = {2308.07074v2.pdf} }
 
@article{2110.08271v2, Author        = {Xinyu Zhang and Ian Colbert and Ken Kreutz-Delgado and Srinjoy Das}, Title         = {Training Deep Neural Networks with Joint Quantization and Pruning of Weights and Activations}, Eprint        = {http://arxiv.org/abs/2110.08271v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {Quantization and pruning are core techniques used to reduce the inference costs of deep neural networks. State-of-the-art quantization techniques are currently applied to both the weights and activations; however, pruning is most often applied to only the weights of the network. In this work, we jointly apply novel uniform quantization and unstructured pruning methods to both the weights and activations of deep neural networks during training. Using our methods, we empirically evaluate the currently accepted prune-then-quantize paradigm across a wide range of computer vision tasks and observe a non-commutative nature when applied to both the weights and activations of deep neural networks. Informed by these observations, we articulate the non-commutativity hypothesis: for a given deep neural network being trained for a specific task, there exists an exact training schedule in which quantization and pruning can be introduced to optimize network performance. We identify that this optimal ordering not only exists, but also varies across discriminative and generative tasks. Using the optimal training schedule within our training framework, we demonstrate increased performance per memory footprint over existing solutions.}, Year          = {2021}, Month         = {10}, Url           = {http://arxiv.org/pdf/2110.08271v2}, File          = {2110.08271v2.pdf} }
 
@article{2406.10229v1, Author        = {Lovish Madaan and Aaditya K. Singh and Rylan Schaeffer and Andrew Poulton and Sanmi Koyejo and Pontus Stenetorp and Sharan Narang and Dieuwke Hupkes}, Title         = {Quantifying Variance in Evaluation Benchmarks}, Eprint        = {http://arxiv.org/abs/2406.10229v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {Evaluation benchmarks are the cornerstone of measuring capabilities of large language models (LLMs), as well as driving progress in said capabilities. Originally designed to make claims about capabilities (or lack thereof) in fully pretrained models, evaluation benchmarks are now also extensively used to decide between various training choices. Despite this widespread usage, we rarely quantify the variance in our evaluation benchmarks, which dictates whether differences in performance are meaningful. Here, we define and measure a range of metrics geared towards measuring variance in evaluation benchmarks, including seed variance across initialisations, and monotonicity during training. By studying a large number of models -- both openly available and pretrained from scratch -- we provide empirical estimates for a variety of variance metrics, with considerations and recommendations for practitioners. We also evaluate the utility and tradeoffs of continuous versus discrete performance measures and explore options for better understanding and reducing this variance. We find that simple changes, such as framing choice tasks (like MMLU) as completion tasks, can often reduce variance for smaller scale ($\sim$7B) models, while more involved methods inspired from human testing literature (such as item analysis and item response theory) struggle to meaningfully reduce variance. Overall, our work provides insights into variance in evaluation benchmarks, suggests LM-specific techniques to reduce variance, and more generally encourages practitioners to carefully factor in variance when comparing models.}, Year          = {2024}, Month         = {6}, Url           = {http://arxiv.org/pdf/2406.10229v1}, File          = {2406.10229v1.pdf} }
 
@article{2402.01781v2, Author        = {Norah Alzahrani and Hisham Abdullah Alyahya and Yazeed Alnumay and Sultan Alrashed and Shaykhah Alsubaie and Yusef Almushaykeh and Faisal Mirza and Nouf Alotaibi and Nora Altwairesh and Areeb Alowisheq and M Saiful Bari and Haidar Khan}, Title         = {When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards}, Eprint        = {http://arxiv.org/abs/2402.01781v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value - we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple-choice question benchmarks (e.g., MMLU), minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a hybrid scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust evaluation schemes on the existing benchmarks. The code for this paper is available at https://github.com/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness.}, Year          = {2024}, Month         = {2}, Url           = {http://arxiv.org/pdf/2402.01781v2}, File          = {2402.01781v2.pdf} }
 
@article{2104.14337v1, Author        = {Douwe Kiela and Max Bartolo and Yixin Nie and Divyansh Kaushik and Atticus Geiger and Zhengxuan Wu and Bertie Vidgen and Grusha Prasad and Amanpreet Singh and Pratik Ringshia and Zhiyi Ma and Tristan Thrush and Sebastian Riedel and Zeerak Waseem and Pontus Stenetorp and Robin Jia and Mohit Bansal and Christopher Potts and Adina Williams}, Title         = {Dynabench: Rethinking Benchmarking in NLP}, Eprint        = {http://arxiv.org/abs/2104.14337v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.}, Year          = {2021}, Month         = {4}, Url           = {http://arxiv.org/pdf/2104.14337v1}, File          = {2104.14337v1.pdf} }
 
@article{2109.03264v2, Author        = {Eugene Kharitonov and Ann Lee and Adam Polyak and Yossi Adi and Jade Copet and Kushal Lakhotia and Tu-Anh Nguyen and Morgane RiviÃ¨re and Abdelrahman Mohamed and Emmanuel Dupoux and Wei-Ning Hsu}, Title         = {Text-Free Prosody-Aware Generative Spoken Language Modeling}, Eprint        = {http://arxiv.org/abs/2109.03264v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) \cite{Lakhotia2021} is the only prior work addressing the generative aspects of speech pre-training, which replaces text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences. Unfortunately, despite eliminating the need of text, the units used in GSLM discard most of the prosodic information. Hence, GSLM fails to leverage prosody for better comprehension, and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. We devise a series of metrics for prosody modeling and generation, and re-use metrics from GSLM for content modeling. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm. Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.}, Year          = {2021}, Month         = {9}, Url           = {http://arxiv.org/pdf/2109.03264v2}, File          = {2109.03264v2.pdf} }
 
@article{2401.06321v1, Author        = {Wonjune Kang and Yun Wang and Shun Zhang and Arthur Hinsvark and Qing He}, Title         = {Multi-Task Learning for Front-End Text Processing in TTS}, Eprint        = {http://arxiv.org/abs/2401.06321v1}, DOI           = {10.1109/ICASSP48485.2024.10446241}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We propose a multi-task learning (MTL) model for jointly performing three tasks that are commonly solved in a text-to-speech (TTS) front-end: text normalization (TN), part-of-speech (POS) tagging, and homograph disambiguation (HD). Our framework utilizes a tree-like structure with a trunk that learns shared representations, followed by separate task-specific heads. We further incorporate a pre-trained language model to utilize its built-in lexical and contextual knowledge, and study how to best use its embeddings so as to most effectively benefit our multi-task model. Through task-wise ablations, we show that our full model trained on all three tasks achieves the strongest overall performance compared to models trained on individual or sub-combinations of tasks, confirming the advantages of our MTL framework. Finally, we introduce a new HD dataset containing a balanced number of sentences in diverse contexts for a variety of homographs and their pronunciations. We demonstrate that incorporating this dataset into training significantly improves HD performance over only using a commonly used, but imbalanced, pre-existing dataset.}, Year          = {2024}, Month         = {1}, Url           = {http://arxiv.org/pdf/2401.06321v1}, File          = {2401.06321v1.pdf} }
 
@article{2205.12446v1, Author        = {Alexis Conneau and Min Ma and Simran Khanuja and Yu Zhang and Vera Axelrod and Siddharth Dalmia and Jason Riesa and Clara Rivera and Ankur Bapna}, Title         = {FLEURS: Few-shot Learning Evaluation of Universal Representations of Speech}, Eprint        = {http://arxiv.org/abs/2205.12446v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We introduce FLEURS, the Few-shot Learning Evaluation of Universal Representations of Speech benchmark. FLEURS is an n-way parallel speech dataset in 102 languages built on top of the machine translation FLoRes-101 benchmark, with approximately 12 hours of speech supervision per language. FLEURS can be used for a variety of speech tasks, including Automatic Speech Recognition (ASR), Speech Language Identification (Speech LangID), Translation and Retrieval. In this paper, we provide baselines for the tasks based on multilingual pre-trained models like mSLAM. The goal of FLEURS is to enable speech technology in more languages and catalyze research in low-resource speech understanding.}, Year          = {2022}, Month         = {5}, Url           = {http://arxiv.org/pdf/2205.12446v1}, File          = {2205.12446v1.pdf} }
 
@article{2310.11441v2, Author        = {Jianwei Yang and Hao Zhang and Feng Li and Xueyan Zou and Chunyuan Li and Jianfeng Gao}, Title         = {Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V}, Eprint        = {http://arxiv.org/abs/2310.11441v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {We present Set-of-Mark (SoM), a new visual prompting method, to unleash the visual grounding abilities of large multimodal models (LMMs), such as GPT-4V. As illustrated in Fig. 1 (right), we employ off-the-shelf interactive segmentation models, such as SEEM/SAM, to partition an image into regions at different levels of granularity, and overlay these regions with a set of marks e.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V can answer the questions that require visual grounding. We perform a comprehensive empirical study to validate the effectiveness of SoM on a wide range of fine-grained vision and multimodal tasks. For example, our experiments show that GPT-4V with SoM in zero-shot setting outperforms the state-of-the-art fully-finetuned referring expression comprehension and segmentation model on RefCOCOg. Code for SoM prompting is made public at: https://github.com/microsoft/SoM.}, Year          = {2023}, Month         = {10}, Url           = {http://arxiv.org/pdf/2310.11441v2}, File          = {2310.11441v2.pdf} }
 
@article{2306.02858v4, Author        = {Hang Zhang and Xin Li and Lidong Bing}, Title         = {Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding}, Eprint        = {http://arxiv.org/abs/2306.02858v4}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We present Video-LLaMA a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual and audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual and audio encoders with LLM's embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.}, Year          = {2023}, Month         = {6}, Url           = {http://arxiv.org/pdf/2306.02858v4}, File          = {2306.02858v4.pdf} }
 
@article{2405.03770v1, Author        = {Neelu Madan and Andreas Moegelmose and Rajat Modi and Yogesh S. Rawat and Thomas B. Moeslund}, Title         = {Foundation Models for Video Understanding: A Survey}, Eprint        = {http://arxiv.org/abs/2405.03770v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {Video Foundation Models (ViFMs) aim to learn a general-purpose representation for various video understanding tasks. Leveraging large-scale datasets and powerful models, ViFMs achieve this by capturing robust and generic features from video data. This survey analyzes over 200 video foundational models, offering a comprehensive overview of benchmarks and evaluation metrics across 14 distinct video tasks categorized into 3 main categories. Additionally, we offer an in-depth performance analysis of these models for the 6 most common video tasks. We categorize ViFMs into three categories: 1) Image-based ViFMs, which adapt existing image models for video tasks, 2) Video-Based ViFMs, which utilize video-specific encoding methods, and 3) Universal Foundational Models (UFMs), which combine multiple modalities (image, video, audio, and text etc.) within a single framework. By comparing the performance of various ViFMs on different tasks, this survey offers valuable insights into their strengths and weaknesses, guiding future advancements in video understanding. Our analysis surprisingly reveals that image-based foundation models consistently outperform video-based models on most video understanding tasks. Additionally, UFMs, which leverage diverse modalities, demonstrate superior performance on video tasks. We share the comprehensive list of ViFMs studied in this work at: \url{https://github.com/NeeluMadan/ViFM_Survey.git}}, Year          = {2024}, Month         = {5}, Url           = {http://arxiv.org/pdf/2405.03770v1}, File          = {2405.03770v1.pdf} }
 
@article{2403.03206v1, Author        = {Patrick Esser and Sumith Kulal and Andreas Blattmann and Rahim Entezari and Jonas MÃ¼ller and Harry Saini and Yam Levi and Dominik Lorenz and Axel Sauer and Frederic Boesel and Dustin Podell and Tim Dockhorn and Zion English and Kyle Lacey and Alex Goodwin and Yannik Marek and Robin Rombach}, Title         = {Scaling Rectified Flow Transformers for High-Resolution Image Synthesis}, Eprint        = {http://arxiv.org/abs/2403.03206v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available.}, Year          = {2024}, Month         = {3}, Url           = {http://arxiv.org/pdf/2403.03206v1}, File          = {2403.03206v1.pdf} }
 
@article{2103.03206v2, Author        = {Andrew Jaegle and Felix Gimeno and Andrew Brock and Andrew Zisserman and Oriol Vinyals and Joao Carreira}, Title         = {Perceiver: General Perception with Iterative Attention}, Eprint        = {http://arxiv.org/abs/2103.03206v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.}, Year          = {2021}, Month         = {3}, Url           = {http://arxiv.org/pdf/2103.03206v2}, File          = {2103.03206v2.pdf} }
 
@article{2211.12171v1, Author        = {Zhifang Guo and Yichong Leng and Yihan Wu and Sheng Zhao and Xu Tan}, Title         = {PromptTTS: Controllable Text-to-Speech with Text Descriptions}, Eprint        = {http://arxiv.org/abs/2211.12171v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {eess.AS}, Abstract      = {Using a text description as prompt to guide the generation of text or images (e.g., GPT-3 or DALLE-2) has drawn wide attention recently. Beyond text and image generation, in this work, we explore the possibility of utilizing text descriptions to guide speech synthesis. Thus, we develop a text-to-speech (TTS) system (dubbed as PromptTTS) that takes a prompt with both style and content descriptions as input to synthesize the corresponding speech. Specifically, PromptTTS consists of a style encoder and a content encoder to extract the corresponding representations from the prompt, and a speech decoder to synthesize speech according to the extracted style and content representations. Compared with previous works in controllable TTS that require users to have acoustic knowledge to understand style factors such as prosody and pitch, PromptTTS is more user-friendly since text descriptions are a more natural way to express speech style (e.g., ''A lady whispers to her friend slowly''). Given that there is no TTS dataset with prompts, to benchmark the task of PromptTTS, we construct and release a dataset containing prompts with style and content information and the corresponding speech. Experiments show that PromptTTS can generate speech with precise style control and high speech quality. Audio samples and our dataset are publicly available.}, Year          = {2022}, Month         = {11}, Url           = {http://arxiv.org/pdf/2211.12171v1}, File          = {2211.12171v1.pdf} }
 
@article{2310.08715v1, Author        = {Ju-Chieh Chou and Chung-Ming Chien and Wei-Ning Hsu and Karen Livescu and Arun Babu and Alexis Conneau and Alexei Baevski and Michael Auli}, Title         = {Toward Joint Language Modeling for Speech Units and Text}, Eprint        = {http://arxiv.org/abs/2310.08715v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Speech and text are two major forms of human language. The research community has been focusing on mapping speech to text or vice versa for many years. However, in the field of language modeling, very little effort has been made to model them jointly. In light of this, we explore joint language modeling for speech units and text. Specifically, we compare different speech tokenizers to transform continuous speech signals into discrete units and use different methods to construct mixed speech-text data. We introduce automatic metrics to evaluate how well the joint LM mixes speech and text. We also fine-tune the LM on downstream spoken language understanding (SLU) tasks with different modalities (speech or text) and test its performance to assess the model's learning of shared representations. Our results show that by mixing speech units and text with our proposed mixing techniques, the joint LM improves over a speech-only baseline on SLU tasks and shows zero-shot cross-modal transferability.}, Year          = {2023}, Month         = {10}, Url           = {http://arxiv.org/pdf/2310.08715v1}, File          = {2310.08715v1.pdf} }
 
@article{2402.05755v2, Author        = {Tu Anh Nguyen and Benjamin Muller and Bokai Yu and Marta R. Costa-jussa and Maha Elbayad and Sravya Popuri and Christophe Ropers and Paul-Ambroise Duquenne and Robin Algayres and Ruslan Mavlyutov and Itai Gat and Mary Williamson and Gabriel Synnaeve and Juan Pino and Benoit Sagot and Emmanuel Dupoux}, Title         = {Spirit LM: Interleaved Spoken and Written Language Model}, Eprint        = {http://arxiv.org/abs/2402.05755v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We introduce Spirit LM, a foundation multimodal language model that freely mixes text and speech. Our model is based on a 7B pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single stream of tokens, and trained with a word-level interleaving method using a small automatically-curated speech-text parallel corpus. Spirit LM comes in two versions: a Base version that uses speech phonetic units (HuBERT) and an Expressive version that models expressivity using pitch and style units in addition to the phonetic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that Spirit LM can learn new tasks in a few-shot fashion across modalities (i.e. ASR, TTS, Speech Classification). We make available model weights and inference code.}, Year          = {2024}, Month         = {2}, Url           = {http://arxiv.org/pdf/2402.05755v2}, File          = {2402.05755v2.pdf} }
 
@article{2312.00784v2, Author        = {Mu Cai and Haotian Liu and Dennis Park and Siva Karthik Mustikovela and Gregory P. Meyer and Yuning Chai and Yong Jae Lee}, Title         = {ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts}, Eprint        = {http://arxiv.org/abs/2312.00784v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {While existing large vision-language multimodal models focus on whole image understanding, there is a prominent gap in achieving region-specific comprehension. Current approaches that use textual coordinates or spatial encodings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we introduce a novel multimodal model capable of decoding arbitrary visual prompts. This allows users to intuitively mark images and interact with the model using natural cues like a "red bounding box"" or ""pointed arrow"". Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encodings, yet achieves state-of-the-art performance on region-understanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present ViP-Bench, a comprehensive benchmark to assess the capability of models in understanding visual prompts across multiple dimensions, enabling future research in this domain. Code, data, and model are publicly available.}, Year          = {2023}, Month         = {12}, Url           = {http://arxiv.org/pdf/2312.00784v2}, File          = {2312.00784v2.pdf} }"
 
@article{2311.16502v4, Author        = {Xiang Yue and Yuansheng Ni and Kai Zhang and Tianyu Zheng and Ruoqi Liu and Ge Zhang and Samuel Stevens and Dongfu Jiang and Weiming Ren and Yuxuan Sun and Cong Wei and Botao Yu and Ruibin Yuan and Renliang Sun and Ming Yin and Boyuan Zheng and Zhenzhu Yang and Yibo Liu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen}, Title         = {MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI}, Eprint        = {http://arxiv.org/abs/2311.16502v4}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We introduce MMMU: a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering. These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. Unlike existing benchmarks, MMMU focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. The evaluation of 14 open-source LMMs as well as the proprietary GPT-4V(ision) and Gemini highlights the substantial challenges posed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve accuracies of 56% and 59% respectively, indicating significant room for improvement. We believe MMMU will stimulate the community to build next-generation multimodal foundation models towards expert artificial general intelligence.}, Year          = {2023}, Month         = {11}, Url           = {http://arxiv.org/pdf/2311.16502v4}, File          = {2311.16502v4.pdf} }
 
@article{2308.09583v2, Author        = {Haipeng Luo and Qingfeng Sun and Can Xu and Pu Zhao and Jianguang Lou and Chongyang Tao and Xiubo Geng and Qingwei Lin and Shifeng Chen and Yansong Tang and Dongmei Zhang}, Title         = {WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct}, Eprint        = {http://arxiv.org/abs/2308.09583v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical CoT reasoning abilities of LLMs without using external python tools, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency. Furthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini Pro and GPT-4-early-version. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance. For more details refer to https://github.com/nlpxucan/WizardLM}, Year          = {2023}, Month         = {8}, Url           = {http://arxiv.org/pdf/2308.09583v2}, File          = {2308.09583v2.pdf} }
 
@article{2312.04724v1, Author        = {Manish Bhatt and Sahana Chennabasappa and Cyrus Nikolaidis and Shengye Wan and Ivan Evtimov and Dominik Gabi and Daniel Song and Faizan Ahmad and Cornelius Aschermann and Lorenzo Fontana and Sasha Frolov and Ravi Prakash Giri and Dhaval Kapil and Yiannis Kozyrakis and David LeBlanc and James Milazzo and Aleksandar Straumann and Gabriel Synnaeve and Varun Vontimitta and Spencer Whitman and Joshua Saxe}, Title         = {Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models}, Eprint        = {http://arxiv.org/abs/2312.04724v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CR}, Abstract      = {This paper presents CyberSecEval, a comprehensive benchmark developed to help bolster the cybersecurity of Large Language Models (LLMs) employed as coding assistants. As what we believe to be the most extensive unified cybersecurity safety benchmark to date, CyberSecEval provides a thorough evaluation of LLMs in two crucial security domains: their propensity to generate insecure code and their level of compliance when asked to assist in cyberattacks. Through a case study involving seven models from the Llama 2, Code Llama, and OpenAI GPT large language model families, CyberSecEval effectively pinpointed key cybersecurity risks. More importantly, it offered practical insights for refining these models. A significant observation from the study was the tendency of more advanced models to suggest insecure code, highlighting the critical need for integrating security considerations in the development of sophisticated LLMs. CyberSecEval, with its automated test case generation and evaluation pipeline covers a broad scope and equips LLM designers and researchers with a tool to broadly measure and enhance the cybersecurity safety properties of LLMs, contributing to the development of more secure AI systems.}, Year          = {2023}, Month         = {12}, Url           = {http://arxiv.org/pdf/2312.04724v1}, File          = {2312.04724v1.pdf} }
 
@article{2212.04501v1, Author        = {Yue Zhao and Ishan Misra and Philipp KrÃ¤henbÃ¼hl and Rohit Girdhar}, Title         = {Learning Video Representations from Large Language Models}, Eprint        = {http://arxiv.org/abs/2212.04501v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {We introduce LaViLa, a new approach to learning video-language representations by leveraging Large Language Models (LLMs). We repurpose pre-trained LLMs to be conditioned on visual input, and finetune them to create automatic video narrators. Our auto-generated narrations offer a number of advantages, including dense coverage of long videos, better temporal synchronization of the visual information and text, and much higher diversity of text. The video-text embedding learned contrastively with these additional auto-generated narrations outperforms the previous state-of-the-art on multiple first-person and third-person video tasks, both in zero-shot and finetuned setups. Most notably, LaViLa obtains an absolute gain of 10.1% on EGTEA classification and 5.9% Epic-Kitchens-100 multi-instance retrieval benchmarks. Furthermore, LaViLa trained with only half the narrations from the Ego4D dataset outperforms baseline models trained on the full set, and shows positive scaling behavior on increasing pre-training data and model size.}, Year          = {2022}, Month         = {12}, Url           = {http://arxiv.org/pdf/2212.04501v1}, File          = {2212.04501v1.pdf} }
 
@article{2304.08244v2, Author        = {Minghao Li and Yingxiu Zhao and Bowen Yu and Feifan Song and Hangyu Li and Haiyang Yu and Zhoujun Li and Fei Huang and Yongbin Li}, Title         = {API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs}, Eprint        = {http://arxiv.org/abs/2304.08244v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Recent research has demonstrated that Large Language Models (LLMs) can enhance their capabilities by utilizing external tools. However, three pivotal questions remain unanswered: (1) How effective are current LLMs in utilizing tools? (2) How can we enhance LLMs' ability to utilize tools? (3) What obstacles need to be overcome to leverage tools? To address these questions, we introduce API-Bank, a groundbreaking benchmark, specifically designed for tool-augmented LLMs. For the first question, we develop a runnable evaluation system consisting of 73 API tools. We annotate 314 tool-use dialogues with 753 API calls to assess the existing LLMs' capabilities in planning, retrieving, and calling APIs. For the second question, we construct a comprehensive training set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000 distinct domains. Using this dataset, we train Lynx, a tool-augmented LLM initialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits improved tool utilization compared to GPT-3, while GPT-4 excels in planning. However, there is still significant potential for further improvement. Moreover, Lynx surpasses Alpaca's tool utilization performance by more than 26 pts and approaches the effectiveness of GPT-3.5. Through error analysis, we highlight the key challenges for future research in this field to answer the third question.}, Year          = {2023}, Month         = {4}, Url           = {http://arxiv.org/pdf/2304.08244v2}, File          = {2304.08244v2.pdf} }
 
@article{2303.11381v1, Author        = {Zhengyuan Yang and Linjie Li and Jianfeng Wang and Kevin Lin and Ehsan Azarnasab and Faisal Ahmed and Zicheng Liu and Ce Liu and Michael Zeng and Lijuan Wang}, Title         = {MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action}, Eprint        = {http://arxiv.org/abs/2303.11381v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action. In this paper, we define and explore a comprehensive list of advanced vision tasks that are intriguing to solve, but may exceed the capabilities of existing vision and vision-language models. To achieve such advanced visual intelligence, MM-REACT introduces a textual prompt design that can represent text descriptions, textualized spatial coordinates, and aligned file names for dense visual signals such as images and videos. MM-REACT's prompt design allows language models to accept, associate, and process multimodal information, thereby facilitating the synergetic combination of ChatGPT and various vision experts. Zero-shot experiments demonstrate MM-REACT's effectiveness in addressing the specified capabilities of interests and its wide application in different scenarios that require advanced visual understanding. Furthermore, we discuss and compare MM-REACT's system paradigm with an alternative approach that extends language models for multimodal scenarios through joint finetuning. Code, demo, video, and visualization are available at https://multimodal-react.github.io/}, Year          = {2023}, Month         = {3}, Url           = {http://arxiv.org/pdf/2303.11381v1}, File          = {2303.11381v1.pdf} }
 
@article{2305.16107v1, Author        = {Tianrui Wang and Long Zhou and Ziqiang Zhang and Yu Wu and Shujie Liu and Yashesh Gaur and Zhuo Chen and Jinyu Li and Furu Wei}, Title         = {VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation}, Eprint        = {http://arxiv.org/abs/2305.16107v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Recent research shows a big convergence in model architecture, training objectives, and inference methods across various tasks for different modalities. In this paper, we propose VioLA, a single auto-regressive Transformer decoder-only network that unifies various cross-modal tasks involving speech and text, such as speech-to-text, text-to-text, text-to-speech, and speech-to-speech tasks, as a conditional codec language model task via multi-task learning framework. To accomplish this, we first convert all the speech utterances to discrete tokens (similar to the textual data) using an offline neural codec encoder. In such a way, all these tasks are converted to token-based sequence conversion problems, which can be naturally handled with one conditional language model. We further integrate task IDs (TID) and language IDs (LID) into the proposed model to enhance the modeling capability of handling different languages and tasks. Experimental results demonstrate that the proposed VioLA model can support both single-modal and cross-modal tasks well, and the decoder-only model achieves a comparable and even better performance than the strong baselines.}, Year          = {2023}, Month         = {5}, Url           = {http://arxiv.org/pdf/2305.16107v1}, File          = {2305.16107v1.pdf} }
 
@article{2005.08100v1, Author        = {Anmol Gulati and James Qin and Chung-Cheng Chiu and Niki Parmar and Yu Zhang and Jiahui Yu and Wei Han and Shibo Wang and Zhengdong Zhang and Yonghui Wu and Ruoming Pang}, Title         = {Conformer: Convolution-augmented Transformer for Speech Recognition}, Eprint        = {http://arxiv.org/abs/2005.08100v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {eess.AS}, Abstract      = {Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.}, Year          = {2020}, Month         = {5}, Url           = {http://arxiv.org/pdf/2005.08100v1}, File          = {2005.08100v1.pdf} }
 
@article{2306.05424v2, Author        = {Muhammad Maaz and Hanoona Rasheed and Salman Khan and Fahad Shahbaz Khan}, Title         = {Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models}, Eprint        = {http://arxiv.org/abs/2306.05424v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {Conversation agents fueled by Large Language Models (LLMs) are providing a new way to interact with visual data. While there have been initial attempts for image-based conversation models, this work addresses the under-explored field of \emph{video-based conversation} by introducing Video-ChatGPT. It is a multimodal model that merges a video-adapted visual encoder with an LLM. The resulting model is capable of understanding and generating detailed conversations about videos. We introduce a new dataset of 100,000 video-instruction pairs used to train Video-ChatGPT acquired via manual and semi-automated pipeline that is easily scalable and robust to label noise. We also develop a quantitative evaluation framework for video-based dialogue models to objectively analyze the strengths and weaknesses of video-based dialogue models. Code: https://github.com/mbzuai-oryx/Video-ChatGPT.}, Year          = {2023}, Month         = {6}, Url           = {http://arxiv.org/pdf/2306.05424v2}, File          = {2306.05424v2.pdf} }
 
@article{2211.01786v2, Author        = {Niklas Muennighoff and Thomas Wang and Lintang Sutawika and Adam Roberts and Stella Biderman and Teven Le Scao and M Saiful Bari and Sheng Shen and Zheng-Xin Yong and Hailey Schoelkopf and Xiangru Tang and Dragomir Radev and Alham Fikri Aji and Khalid Almubarak and Samuel Albanie and Zaid Alyafeai and Albert Webson and Edward Raff and Colin Raffel}, Title         = {Crosslingual Generalization through Multitask Finetuning}, Eprint        = {http://arxiv.org/abs/2211.01786v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.}, Year          = {2022}, Month         = {11}, Url           = {http://arxiv.org/pdf/2211.01786v2}, File          = {2211.01786v2.pdf} }
 
@article{2310.08419v4, Author        = {Patrick Chao and Alexander Robey and Edgar Dobriban and Hamed Hassani and George J. Pappas and Eric Wong}, Title         = {Jailbreaking Black Box Large Language Models in Twenty Queries}, Eprint        = {http://arxiv.org/abs/2310.08419v4}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and Gemini.}, Year          = {2023}, Month         = {10}, Url           = {http://arxiv.org/pdf/2310.08419v4}, File          = {2310.08419v4.pdf} }
 
@article{360287970189639893} is missing info ,Title={Adversarial Examples for Evaluating Reading Comprehension Systems},
 
@article{2402.16822v3, Author        = {Mikayel Samvelyan and Sharath Chandra Raparthy and Andrei Lupu and Eric Hambro and Aram H. Markosyan and Manish Bhatt and Yuning Mao and Minqi Jiang and Jack Parker-Holder and Jakob Foerster and Tim RocktÃ¤schel and Roberta Raileanu}, Title         = {Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts}, Eprint        = {http://arxiv.org/abs/2402.16822v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to adversarial attacks is of paramount importance. Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel black-box approach for producing a diverse collection of adversarial prompts. Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem and uses open-ended search to generate prompts that are both effective and diverse. Focusing on the safety domain, we use Rainbow Teaming to target various state-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach reveals hundreds of effective adversarial prompts, with an attack success rate exceeding 90% across all tested models. Furthermore, we demonstrate that prompts generated by Rainbow Teaming are highly transferable and that fine-tuning models with synthetic data generated by our method significantly enhances their safety without sacrificing general performance or helpfulness. We additionally explore the versatility of Rainbow Teaming by applying it to question answering and cybersecurity, showcasing its potential to drive robust open-ended self-improvement in a wide range of applications.}, Year          = {2024}, Month         = {2}, Url           = {http://arxiv.org/pdf/2402.16822v3}, File          = {2402.16822v3.pdf} }
 
@article{2308.01263v3, Author        = {Paul RÃ¶ttger and Hannah Rose Kirk and Bertie Vidgen and Giuseppe Attanasio and Federico Bianchi and Dirk Hovy}, Title         = {XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models}, Eprint        = {http://arxiv.org/abs/2308.01263v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This risk motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way. XSTest comprises 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. We describe XSTest's creation and composition, and then use the test suite to highlight systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models.}, Year          = {2023}, Month         = {8}, Url           = {http://arxiv.org/pdf/2308.01263v3}, File          = {2308.01263v3.pdf} }
 
@article{2404.13161v1, Author        = {Manish Bhatt and Sahana Chennabasappa and Yue Li and Cyrus Nikolaidis and Daniel Song and Shengye Wan and Faizan Ahmad and Cornelius Aschermann and Yaohui Chen and Dhaval Kapil and David Molnar and Spencer Whitman and Joshua Saxe}, Title         = {CyberSecEval 2: A Wide-Ranging Cybersecurity Evaluation Suite for Large Language Models}, Eprint        = {http://arxiv.org/abs/2404.13161v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CR}, Abstract      = {Large language models (LLMs) introduce new security risks, but there are few comprehensive evaluation suites to measure and reduce these risks. We present BenchmarkName, a novel benchmark to quantify LLM security risks and capabilities. We introduce two new areas for testing: prompt injection and code interpreter abuse. We evaluated multiple state-of-the-art (SOTA) LLMs, including GPT-4, Mistral, Meta Llama 3 70B-Instruct, and Code Llama. Our results show that conditioning away risk of attack remains an unsolved problem; for example, all tested models showed between 26% and 41% successful prompt injection tests. We further introduce the safety-utility tradeoff: conditioning an LLM to reject unsafe prompts can cause the LLM to falsely reject answering benign prompts, which lowers utility. We propose quantifying this tradeoff using False Refusal Rate (FRR). As an illustration, we introduce a novel test set to quantify FRR for cyberattack helpfulness risk. We find many LLMs able to successfully comply with "borderline"" benign requests while still rejecting most unsafe requests. Finally, we quantify the utility of LLMs for automating a core cybersecurity task, that of exploiting software vulnerabilities. This is important because the offensive capabilities of LLMs are of intense interest; we quantify this by creating novel test sets for four representative problems. We find that models with coding capabilities perform better than those without, but that further work is needed for LLMs to become proficient at exploit generation. Our code is open source and can be used to evaluate other LLMs.}, Year          = {2024}, Month         = {4}, Url           = {http://arxiv.org/pdf/2404.13161v1}, File          = {2404.13161v1.pdf} }"
 
@article{2401.05060v2, Author        = {Marta R. Costa-jussÃ  and Mariano Coria Meglioli and Pierre Andrews and David Dale and Prangthip Hansanti and Elahe Kalbassi and Alex Mourachko and Christophe Ropers and Carleigh Wood}, Title         = {MuTox: Universal MUltilingual Audio-based TOXicity Dataset and Zero-shot Detector}, Eprint        = {http://arxiv.org/abs/2401.05060v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.SD}, Abstract      = {Research in toxicity detection in natural language processing for the speech modality (audio-based) is quite limited, particularly for languages other than English. To address these limitations and lay the groundwork for truly multilingual audio-based toxicity detection, we introduce MuTox, the first highly multilingual audio-based dataset with toxicity labels. The dataset comprises 20,000 audio utterances for English and Spanish, and 4,000 for the other 19 languages. To demonstrate the quality of this dataset, we trained the MuTox audio-based toxicity classifier, which enables zero-shot toxicity detection across a wide range of languages. This classifier outperforms existing text-based trainable classifiers by more than 1% AUC, while expanding the language coverage more than tenfold. When compared to a wordlist-based classifier that covers a similar number of languages, MuTox improves precision and recall by approximately 2.5 times. This significant improvement underscores the potential of MuTox in advancing the field of audio-based toxicity detection.}, Year          = {2024}, Month         = {1}, Url           = {http://arxiv.org/pdf/2401.05060v2}, File          = {2401.05060v2.pdf} }
 
@article{2312.06674v1, Author        = {Hakan Inan and Kartikeya Upasani and Jianfeng Chi and Rashi Rungta and Krithika Iyer and Yuning Mao and Michael Tontchev and Qing Hu and Brian Fuller and Davide Testuggine and Madian Khabsa}, Title         = {Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations}, Eprint        = {http://arxiv.org/abs/2312.06674v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.}, Year          = {2023}, Month         = {12}, Url           = {http://arxiv.org/pdf/2312.06674v1}, File          = {2312.06674v1.pdf} }
 
@article{2411.03923v1, Author        = {Aaditya K. Singh and Muhammed Yusuf Kocyigit and Andrew Poulton and David Esiobu and Maria Lomeli and Gergely Szilvasy and Dieuwke Hupkes}, Title         = {Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?}, Eprint        = {http://arxiv.org/abs/2411.03923v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Hampering the interpretation of benchmark scores, evaluation data contamination has become a growing concern in the evaluation of LLMs, and an active area of research studies its effects. While evaluation data contamination is easily understood intuitively, it is surprisingly difficult to define precisely which samples should be considered contaminated and, consequently, how it impacts benchmark scores. We propose that these questions should be addressed together and that contamination metrics can be assessed based on whether models benefit from the examples they mark contaminated. We propose a novel analysis method called ConTAM, and show with a large scale survey of existing and novel n-gram based contamination metrics across 13 benchmarks and 7 models from 2 different families that ConTAM can be used to better understand evaluation data contamination and its effects. We find that contamination may have a much larger effect than reported in recent LLM releases and benefits models differently at different scales. We also find that considering only the longest contaminated substring provides a better signal than considering a union of all contaminated substrings, and that doing model and benchmark specific threshold analysis greatly increases the specificity of the results. Lastly, we investigate the impact of hyperparameter choices, finding that, among other things, both using larger values of n and disregarding matches that are infrequent in the pre-training data lead to many false negatives. With ConTAM, we provide a method to empirically ground evaluation data contamination metrics in downstream effects. With our exploration, we shed light on how evaluation data contamination can impact LLMs and provide insight into the considerations important when doing contamination analysis. We end our paper by discussing these in more detail and providing concrete suggestions for future work.}, Year          = {2024}, Month         = {11}, Url           = {http://arxiv.org/pdf/2411.03923v1}, File          = {2411.03923v1.pdf} }