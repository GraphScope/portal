@article{1803.05407v3, Author        = {Pavel Izmailov and Dmitrii Podoprikhin and Timur Garipov and Dmitry Vetrov and Andrew Gordon Wilson}, Title         = {Averaging Weights Leads to Wider Optima and Better Generalization}, Eprint        = {http://arxiv.org/abs/1803.05407v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.}, Year          = {2018}, Month         = {3}, Url           = {http://arxiv.org/pdf/1803.05407v3}, File          = {1803.05407v3.pdf} }
 
@article{2004.10964v3, Author        = {Suchin Gururangan and Ana Marasović and Swabha Swayamdipta and Kyle Lo and Iz Beltagy and Doug Downey and Noah A. Smith}, Title         = {Don't Stop Pretraining: Adapt Language Models to Domains and Tasks}, Eprint        = {http://arxiv.org/abs/2004.10964v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.}, Year          = {2020}, Month         = {4}, Url           = {http://arxiv.org/pdf/2004.10964v3}, File          = {2004.10964v3.pdf} }
 
@article{1707.06347v2, Author        = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov}, Title         = {Proximal Policy Optimization Algorithms}, Eprint        = {http://arxiv.org/abs/1707.06347v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate"" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.}, Year          = {2017}, Month         = {7}, Url           = {http://arxiv.org/pdf/1707.06347v2}, File          = {1707.06347v2.pdf} }"
 
@article{1907.11692v1, Author        = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov}, Title         = {RoBERTa: A Robustly Optimized BERT Pretraining Approach}, Eprint        = {http://arxiv.org/abs/1907.11692v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.}, Year          = {2019}, Month         = {7}, Url           = {http://arxiv.org/pdf/1907.11692v1}, File          = {1907.11692v1.pdf} }
 
@article{2403.04706v1, Author        = {Chen Li and Weiqi Wang and Jingcheng Hu and Yixuan Wei and Nanning Zheng and Han Hu and Zheng Zhang and Houwen Peng}, Title         = {Common 7B Language Models Already Possess Strong Math Capabilities}, Eprint        = {http://arxiv.org/abs/2403.04706v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training. This paper shows that the LLaMA-2 7B model with common pre-training already exhibits strong mathematical abilities, as evidenced by its impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks, respectively, when selecting the best response from 256 random generations. The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities. Notably, the accuracy for the first answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks, respectively. We find that simply scaling up the SFT data can significantly enhance the reliability of generating correct answers. However, the potential for extensive scaling is constrained by the scarcity of publicly available math questions. To overcome this limitation, we employ synthetic data, which proves to be nearly as effective as real data and shows no clear saturation when scaled up to approximately one million samples. This straightforward approach achieves an accuracy of 82.6% on GSM8K and 40.6% on MATH using LLaMA-2 7B models, surpassing previous models by 14.2% and 20.8%, respectively. We also provide insights into scaling behaviors across different reasoning complexities and error types.}, Year          = {2024}, Month         = {3}, Url           = {http://arxiv.org/pdf/2403.04706v1}, File          = {2403.04706v1.pdf} }
 
@article{1905.13319v1, Author        = {Aida Amini and Saadia Gabriel and Peter Lin and Rik Koncel-Kedziorski and Yejin Choi and Hannaneh Hajishirzi}, Title         = {MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms}, Eprint        = {http://arxiv.org/abs/1905.13319v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver that learns to map problems to operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model precise operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, our new dataset, MathQA, significantly enhances the AQuA dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model enhanced with automatic problem categorization. Our experiments show improvements over competitive baselines in our MathQA as well as the AQuA dataset. The results are still significantly lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at: https://math-qa.github.io/math-QA/}, Year          = {2019}, Month         = {5}, Url           = {http://arxiv.org/pdf/1905.13319v1}, File          = {1905.13319v1.pdf} }
 
@article{2110.14168v2, Author        = {Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman}, Title         = {Training Verifiers to Solve Math Word Problems}, Eprint        = {http://arxiv.org/abs/2110.14168v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.}, Year          = {2021}, Month         = {10}, Url           = {http://arxiv.org/pdf/2110.14168v2}, File          = {2110.14168v2.pdf} }
 
@article{1905.03197v3, Author        = {Li Dong and Nan Yang and Wenhui Wang and Furu Wei and Xiaodong Liu and Yu Wang and Jianfeng Gao and Ming Zhou and Hsiao-Wuen Hon}, Title         = {Unified Language Model Pre-training for Natural Language Understanding and Generation}, Eprint        = {http://arxiv.org/abs/1905.03197v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https://github.com/microsoft/unilm.}, Year          = {2019}, Month         = {5}, Url           = {http://arxiv.org/pdf/1905.03197v3}, File          = {1905.03197v3.pdf} }
 
@article{2109.07830v3, Author        = {Swaroop Mishra and Daniel Khashabi and Chitta Baral and Yejin Choi and Hannaneh Hajishirzi}, Title         = {Reframing Instructional Prompts to GPTk's Language}, Eprint        = {http://arxiv.org/abs/2109.07830v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {What kinds of instructional prompts are easier to follow for Language Models (LMs)? We study this question by conducting extensive empirical analysis that shed light on important features of successful instructional prompts. Specifically, we study several classes of reframing techniques for manual reformulation of prompts into more effective ones. Some examples include decomposing a complex task instruction into multiple simpler tasks or itemizing instructions into sequential steps. Our experiments compare the zero-shot and few-shot performance of LMs prompted with reframed instructions on 12 NLP tasks across 6 categories. Compared with original instructions, our reframed instructions lead to significant improvements across LMs with different sizes. For example, the same reframed prompts boost few-shot performance of GPT3-series and GPT2-series by 12.5% and 6.7% respectively averaged over all tasks. Furthermore, reframed instructions reduce the number of examples required to prompt LMs in the few-shot setting. We hope these empirically-driven techniques will pave the way towards more effective future prompting algorithms.}, Year          = {2021}, Month         = {9}, Url           = {http://arxiv.org/pdf/2109.07830v3}, File          = {2109.07830v3.pdf} }
 
@article{2211.00053v1, Author        = {Sean Welleck and Ximing Lu and Peter West and Faeze Brahman and Tianxiao Shen and Daniel Khashabi and Yejin Choi}, Title         = {Generating Sequences by Learning to Self-Correct}, Eprint        = {http://arxiv.org/abs/2211.00053v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Sequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. We present Self-Correction, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. To train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. We show that Self-Correction improves upon the base generator in three diverse generation tasks - mathematical program synthesis, lexically-constrained generation, and toxicity control - even when the corrector is much smaller than the base generator.}, Year          = {2022}, Month         = {10}, Url           = {http://arxiv.org/pdf/2211.00053v1}, File          = {2211.00053v1.pdf} }
 
@article{2404.14619v2, Author        = {Sachin Mehta and Mohammad Hossein Sekhavat and Qingqing Cao and Maxwell Horton and Yanzi Jin and Chenfan Sun and Iman Mirzadeh and Mahyar Najibi and Dmitry Belenko and Peter Zatloukal and Mohammad Rastegari}, Title         = {OpenELM: An Efficient Language Model Family with Open Training and Inference Framework}, Eprint        = {http://arxiv.org/abs/2404.14619v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring $2\times$ fewer pre-training tokens.   Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors.   Our source code along with pre-trained model weights and training recipes is available at \url{https://github.com/apple/corenet}. Additionally, \model models can be found on HuggingFace at: \url{https://huggingface.co/apple/OpenELM}.}, Year          = {2024}, Month         = {4}, Url           = {http://arxiv.org/pdf/2404.14619v2}, File          = {2404.14619v2.pdf} }
 
@article{2211.15533v1, Author        = {Denis Kocetkov and Raymond Li and Loubna Ben Allal and Jia Li and Chenghao Mou and Carlos Muñoz Ferrandis and Yacine Jernite and Margaret Mitchell and Sean Hughes and Thomas Wolf and Dzmitry Bahdanau and Leandro von Werra and Harm de Vries}, Title         = {The Stack: 3 TB of permissively licensed source code}, Eprint        = {http://arxiv.org/abs/2211.15533v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called "Am I in The Stack"" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.}, Year          = {2022}, Month         = {11}, Url           = {http://arxiv.org/pdf/2211.15533v1}, File          = {2211.15533v1.pdf} }"
 
@article{1910.02054v3, Author        = {Samyam Rajbhandari and Jeff Rasley and Olatunji Ruwase and Yuxiong He}, Title         = {ZeRO: Memory Optimizations Toward Training Trillion Parameter Models}, Eprint        = {http://arxiv.org/abs/1910.02054v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware.   We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create the world's largest language model (Turing-NLG, 17B parameters) with record breaking accuracy.}, Year          = {2019}, Month         = {10}, Url           = {http://arxiv.org/pdf/1910.02054v3}, File          = {1910.02054v3.pdf} }
 
@article{2101.06840v1, Author        = {Jie Ren and Samyam Rajbhandari and Reza Yazdani Aminabadi and Olatunji Ruwase and Shuangyan Yang and Minjia Zhang and Dong Li and Yuxiong He}, Title         = {ZeRO-Offload: Democratizing Billion-Scale Model Training}, Eprint        = {http://arxiv.org/abs/2101.06840v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.DC}, Abstract      = {Large-scale model training has been a playing ground for a limited few requiring complex model refactoring and access to prohibitively expensive GPU clusters. ZeRO-Offload changes the large model training landscape by making large model training accessible to nearly everyone. It can train models with over 13 billion parameters on a single GPU, a 10x increase in size compared to popular framework such as PyTorch, and it does so without requiring any model change from the data scientists or sacrificing computational efficiency. ZeRO-Offload enables large model training by offloading data and compute to CPU. To preserve compute efficiency, it is designed to minimize the data movement to/from GPU, and reduce CPU compute time while maximizing memory savings on GPU. As a result, ZeRO-Offload can achieve 40 TFlops/GPU on a single NVIDIA V100 GPU for 10B parameter model compared to 30TF using PyTorch alone for a 1.4B parameter model, the largest that can be trained without running out of memory. ZeRO-Offload is also designed to scale on multiple-GPUs when available, offering near linear speedup on up to 128 GPUs. Additionally, it can work together with model parallelism to train models with over 70 billion parameters on a single DGX-2 box, a 4.5x increase in model size compared to using model parallelism alone. By combining compute and memory efficiency with ease-of-use, ZeRO-Offload democratizes large-scale model training making it accessible to even data scientists with access to just a single GPU.}, Year          = {2021}, Month         = {1}, Url           = {http://arxiv.org/pdf/2101.06840v1}, File          = {2101.06840v1.pdf} }
 
@article{2208.03306v1, Author        = {Margaret Li and Suchin Gururangan and Tim Dettmers and Mike Lewis and Tim Althoff and Noah A. Smith and Luke Zettlemoyer}, Title         = {Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models}, Eprint        = {http://arxiv.org/abs/2208.03306v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We present Branch-Train-Merge (BTM), a communication-efficient algorithm for embarrassingly parallel training of large language models (LLMs). We show it is possible to independently train subparts of a new class of LLMs on different subsets of the data, eliminating the massive multi-node synchronization currently required to train LLMs. BTM learns a set of independent expert LMs (ELMs), each specialized to a different textual domain, such as scientific or legal text. These ELMs can be added and removed to update data coverage, ensembled to generalize to new domains, or averaged to collapse back to a single LM for efficient inference. New ELMs are learned by branching from (mixtures of) ELMs in the current set, further training the parameters on data for the new domain, and then merging the resulting model back into the set for future use. Experiments show that BTM improves in- and out-of-domain perplexities as compared to GPT-style Transformer LMs, when controlling for training cost. Through extensive analysis, we show that these results are robust to different ELM initialization schemes, but require expert domain specialization; LM ensembles with random data splits do not perform well. We also present a study of scaling BTM into a new corpus of 64 domains (192B whitespace-separated tokens in total); the resulting LM (22.4B total parameters) performs as well as a Transformer LM trained with 2.5 times more compute. These gains grow with the number of domains, suggesting more aggressive parallelism could be used to efficiently train larger models in future work.}, Year          = {2022}, Month         = {8}, Url           = {http://arxiv.org/pdf/2208.03306v1}, File          = {2208.03306v1.pdf} }
 
@article{2311.10709v2, Author        = {Rohit Girdhar and Mannat Singh and Andrew Brown and Quentin Duval and Samaneh Azadi and Sai Saketh Rambhatla and Akbar Shah and Xi Yin and Devi Parikh and Ishan Misra}, Title         = {Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning}, Eprint        = {http://arxiv.org/abs/2311.10709v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {We present Emu Video, a text-to-video generation model that factorizes the generation into two steps: first generating an image conditioned on the text, and then generating a video conditioned on the text and the generated image. We identify critical design decisions--adjusted noise schedules for diffusion, and multi-stage training that enable us to directly generate high quality and high resolution videos, without requiring a deep cascade of models as in prior work. In human evaluations, our generated videos are strongly preferred in quality compared to all prior work--81% vs. Google's Imagen Video, 90% vs. Nvidia's PYOCO, and 96% vs. Meta's Make-A-Video. Our model outperforms commercial solutions such as RunwayML's Gen2 and Pika Labs. Finally, our factorizing approach naturally lends itself to animating images based on a user's text prompt, where our generations are preferred 96% over prior work.}, Year          = {2023}, Month         = {11}, Url           = {http://arxiv.org/pdf/2311.10709v2}, File          = {2311.10709v2.pdf} }
 
@article{360287970189639904} is missing info ,Title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
 
@article{2304.01373v2, Author        = {Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal}, Title         = {Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, Eprint        = {http://arxiv.org/abs/2304.01373v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \url{https://github.com/EleutherAI/pythia}.}, Year          = {2023}, Month         = {4}, Url           = {http://arxiv.org/pdf/2304.01373v2}, File          = {2304.01373v2.pdf} }
 
@article{2202.01855v2, Author        = {Chung-Cheng Chiu and James Qin and Yu Zhang and Jiahui Yu and Yonghui Wu}, Title         = {Self-supervised Learning with Random-projection Quantizer for Speech Recognition}, Eprint        = {http://arxiv.org/abs/2202.01855v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We present a simple and effective self-supervised learning approach for speech recognition. The approach learns a model to predict the masked speech signals, in the form of discrete labels generated with a random-projection quantizer. In particular the quantizer projects speech inputs with a randomly initialized matrix, and does a nearest-neighbor lookup in a randomly-initialized codebook. Neither the matrix nor the codebook is updated during self-supervised learning. Since the random-projection quantizer is not trained and is separated from the speech recognition model, the design makes the approach flexible and is compatible with universal speech recognition architecture. On LibriSpeech our approach achieves similar word-error-rates as previous work using self-supervised learning with non-streaming models, and provides lower word-error-rates and latency than wav2vec 2.0 and w2v-BERT with streaming models. On multilingual tasks the approach also provides significant improvement over wav2vec 2.0 and w2v-BERT.}, Year          = {2022}, Month         = {2}, Url           = {http://arxiv.org/pdf/2202.01855v2}, File          = {2202.01855v2.pdf} }
 
@article{2303.09540v3, Author        = {Amro Abbas and Kushal Tirumala and Dániel Simig and Surya Ganguli and Ari S. Morcos}, Title         = {SemDeDup: Data-efficient learning at web-scale through semantic deduplication}, Eprint        = {http://arxiv.org/abs/2303.09540v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {Progress in machine learning has been driven in large part by massive increases in data. However, large web-scale datasets such as LAION are largely uncurated beyond searches for exact duplicates, potentially leaving much redundancy. Here, we introduce SemDeDup, a method which leverages embeddings from pre-trained models to identify and remove semantic duplicates: data pairs which are semantically similar, but not exactly identical. Removing semantic duplicates preserves performance and speeds up learning. Analyzing a subset of LAION, we show that SemDeDup can remove 50% of the data with minimal performance loss, effectively halving training time. Moreover, performance increases out of distribution. Also, analyzing language models trained on C4, a partially curated dataset, we show that SemDeDup improves over prior approaches while providing efficiency gains. SemDeDup provides an example of how simple ways of leveraging quality embeddings can be used to make models learn faster with less data.}, Year          = {2023}, Month         = {3}, Url           = {http://arxiv.org/pdf/2303.09540v3}, File          = {2303.09540v3.pdf} }
 
@article{2309.06180v1, Author        = {Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica}, Title         = {Efficient Memory Management for Large Language Model Serving with PagedAttention}, Eprint        = {http://arxiv.org/abs/2309.06180v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm}, Year          = {2023}, Month         = {9}, Url           = {http://arxiv.org/pdf/2309.06180v1}, File          = {2309.06180v1.pdf} }
 
@article{2211.10438v7, Author        = {Guangxuan Xiao and Ji Lin and Mickael Seznec and Hao Wu and Julien Demouth and Song Han}, Title         = {SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models}, Eprint        = {http://arxiv.org/abs/2211.10438v7}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, Llama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at https://github.com/mit-han-lab/smoothquant.}, Year          = {2022}, Month         = {11}, Url           = {http://arxiv.org/pdf/2211.10438v7}, File          = {2211.10438v7.pdf} }
 
@article{2203.05482v3, Author        = {Mitchell Wortsman and Gabriel Ilharco and Samir Yitzhak Gadre and Rebecca Roelofs and Raphael Gontijo-Lopes and Ari S. Morcos and Hongseok Namkoong and Ali Farhadi and Yair Carmon and Simon Kornblith and Ludwig Schmidt}, Title         = {Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time}, Eprint        = {http://arxiv.org/abs/2203.05482v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results "model soups."" When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.}, Year          = {2022}, Month         = {3}, Url           = {http://arxiv.org/pdf/2203.05482v3}, File          = {2203.05482v3.pdf} }"
 
@article{2306.02858v4, Author        = {Hang Zhang and Xin Li and Lidong Bing}, Title         = {Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding}, Eprint        = {http://arxiv.org/abs/2306.02858v4}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We present Video-LLaMA a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual and audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual and audio encoders with LLM's embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.}, Year          = {2023}, Month         = {6}, Url           = {http://arxiv.org/pdf/2306.02858v4}, File          = {2306.02858v4.pdf} }
 
@article{2305.06500v2, Author        = {Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi}, Title         = {InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning}, Eprint        = {http://arxiv.org/abs/2305.06500v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at https://github.com/salesforce/LAVIS/tree/main/projects/instructblip.}, Year          = {2023}, Month         = {5}, Url           = {http://arxiv.org/pdf/2305.06500v2}, File          = {2305.06500v2.pdf} }
 
@article{2311.06242v1, Author        = {Bin Xiao and Haiping Wu and Weijian Xu and Xiyang Dai and Houdong Hu and Yumao Lu and Michael Zeng and Ce Liu and Lu Yuan}, Title         = {Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks}, Eprint        = {http://arxiv.org/abs/2311.06242v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {We introduce Florence-2, a novel vision foundation model with a unified, prompt-based representation for a variety of computer vision and vision-language tasks. While existing large vision models excel in transfer learning, they struggle to perform a diversity of tasks with simple instructions, a capability that implies handling the complexity of various spatial hierarchy and semantic granularity. Florence-2 was designed to take text-prompt as task instructions and generate desirable results in text forms, whether it be captioning, object detection, grounding or segmentation. This multi-task learning setup demands large-scale, high-quality annotated data. To this end, we co-developed FLD-5B that consists of 5.4 billion comprehensive visual annotations on 126 million images, using an iterative strategy of automated image annotation and model refinement. We adopted a sequence-to-sequence structure to train Florence-2 to perform versatile and comprehensive vision tasks. Extensive evaluations on numerous tasks demonstrated Florence-2 to be a strong vision foundation model contender with unprecedented zero-shot and fine-tuning capabilities.}, Year          = {2023}, Month         = {11}, Url           = {http://arxiv.org/pdf/2311.06242v1}, File          = {2311.06242v1.pdf} }
 
@article{2312.00784v2, Author        = {Mu Cai and Haotian Liu and Dennis Park and Siva Karthik Mustikovela and Gregory P. Meyer and Yuning Chai and Yong Jae Lee}, Title         = {ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts}, Eprint        = {http://arxiv.org/abs/2312.00784v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {While existing large vision-language multimodal models focus on whole image understanding, there is a prominent gap in achieving region-specific comprehension. Current approaches that use textual coordinates or spatial encodings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we introduce a novel multimodal model capable of decoding arbitrary visual prompts. This allows users to intuitively mark images and interact with the model using natural cues like a "red bounding box"" or ""pointed arrow"". Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encodings, yet achieves state-of-the-art performance on region-understanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present ViP-Bench, a comprehensive benchmark to assess the capability of models in understanding visual prompts across multiple dimensions, enabling future research in this domain. Code, data, and model are publicly available.}, Year          = {2023}, Month         = {12}, Url           = {http://arxiv.org/pdf/2312.00784v2}, File          = {2312.00784v2.pdf} }"
 
@article{2310.11441v2, Author        = {Jianwei Yang and Hao Zhang and Feng Li and Xueyan Zou and Chunyuan Li and Jianfeng Gao}, Title         = {Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V}, Eprint        = {http://arxiv.org/abs/2310.11441v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {We present Set-of-Mark (SoM), a new visual prompting method, to unleash the visual grounding abilities of large multimodal models (LMMs), such as GPT-4V. As illustrated in Fig. 1 (right), we employ off-the-shelf interactive segmentation models, such as SEEM/SAM, to partition an image into regions at different levels of granularity, and overlay these regions with a set of marks e.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V can answer the questions that require visual grounding. We perform a comprehensive empirical study to validate the effectiveness of SoM on a wide range of fine-grained vision and multimodal tasks. For example, our experiments show that GPT-4V with SoM in zero-shot setting outperforms the state-of-the-art fully-finetuned referring expression comprehension and segmentation model on RefCOCOg. Code for SoM prompting is made public at: https://github.com/microsoft/SoM.}, Year          = {2023}, Month         = {10}, Url           = {http://arxiv.org/pdf/2310.11441v2}, File          = {2310.11441v2.pdf} }
 
@article{2405.12205v1, Author        = {Aniket Didolkar and Anirudh Goyal and Nan Rosemary Ke and Siyuan Guo and Michal Valko and Timothy Lillicrap and Danilo Rezende and Yoshua Bengio and Michael Mozer and Sanjeev Arora}, Title         = {Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving}, Eprint        = {http://arxiv.org/abs/2405.12205v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.AI}, Abstract      = {Metacognitive knowledge refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans.   To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH. (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label. This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic, even though this article applies it to math problems.}, Year          = {2024}, Month         = {5}, Url           = {http://arxiv.org/pdf/2405.12205v1}, File          = {2405.12205v1.pdf} }
 
@article{2210.03629v3, Author        = {Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao}, Title         = {ReAct: Synergizing Reasoning and Acting in Language Models}, Eprint        = {http://arxiv.org/abs/2210.03629v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io}, Year          = {2022}, Month         = {10}, Url           = {http://arxiv.org/pdf/2210.03629v3}, File          = {2210.03629v3.pdf} }
 
@article{2103.03206v2, Author        = {Andrew Jaegle and Felix Gimeno and Andrew Brock and Andrew Zisserman and Oriol Vinyals and Joao Carreira}, Title         = {Perceiver: General Perception with Iterative Attention}, Eprint        = {http://arxiv.org/abs/2103.03206v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.}, Year          = {2021}, Month         = {3}, Url           = {http://arxiv.org/pdf/2103.03206v2}, File          = {2103.03206v2.pdf} }
 
@article{2304.11277v2, Author        = {Yanli Zhao and Andrew Gu and Rohan Varma and Liang Luo and Chien-Chin Huang and Min Xu and Less Wright and Hamid Shojanazeri and Myle Ott and Sam Shleifer and Alban Desmaison and Can Balioglu and Pritam Damania and Bernard Nguyen and Geeta Chauhan and Yuchen Hao and Ajit Mathews and Shen Li}, Title         = {PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel}, Eprint        = {http://arxiv.org/abs/2304.11277v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.DC}, Abstract      = {It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The experimental results demonstrate that FSDP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of TFLOPS.}, Year          = {2023}, Month         = {4}, Url           = {http://arxiv.org/pdf/2304.11277v2}, File          = {2304.11277v2.pdf} }
 
@article{2309.16039v3, Author        = {Wenhan Xiong and Jingyu Liu and Igor Molybog and Hejia Zhang and Prajjwal Bhargava and Rui Hou and Louis Martin and Rashi Rungta and Karthik Abinav Sankararaman and Barlas Oguz and Madian Khabsa and Han Fang and Yashar Mehdad and Sharan Narang and Kshitiz Malik and Angela Fan and Shruti Bhosale and Sergey Edunov and Mike Lewis and Sinong Wang and Hao Ma}, Title         = {Effective Long-Context Scaling of Foundation Models}, Eprint        = {http://arxiv.org/abs/2309.16039v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We present a series of long-context LLMs that support effective context windows of up to 32,768 tokens. Our model series are built through continual pretraining from Llama 2 with longer training sequences and on a dataset where long texts are upsampled. We perform extensive evaluation on language modeling, synthetic context probing tasks, and a wide range of research benchmarks. On research benchmarks, our models achieve consistent improvements on most regular tasks and significant improvements on long-context tasks over Llama 2. Notably, with a cost-effective instruction tuning procedure that does not require human-annotated long instruction data, the 70B variant can already surpass gpt-3.5-turbo-16k's overall performance on a suite of long-context tasks. Alongside these results, we provide an in-depth analysis on the individual components of our method. We delve into Llama's position encodings and discuss its limitation in modeling long dependencies. We also examine the impact of various design choices in the pretraining process, including the data mix and the training curriculum of sequence lengths -- our ablation experiments suggest that having abundant long texts in the pretrain dataset is not the key to achieving strong performance, and we empirically verify that long context continual pretraining is more efficient and similarly effective compared to pretraining from scratch with long sequences.}, Year          = {2023}, Month         = {9}, Url           = {http://arxiv.org/pdf/2309.16039v3}, File          = {2309.16039v3.pdf} }
 
@article{2112.09332v3, Author        = {Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman}, Title         = {WebGPT: Browser-assisted question-answering with human feedback}, Eprint        = {http://arxiv.org/abs/2112.09332v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.}, Year          = {2021}, Month         = {12}, Url           = {http://arxiv.org/pdf/2112.09332v3}, File          = {2112.09332v3.pdf} }
 
@article{2312.14925v2, Author        = {Timo Kaufmann and Paul Weng and Viktor Bengs and Eyke Hüllermeier}, Title         = {A Survey of Reinforcement Learning from Human Feedback}, Eprint        = {http://arxiv.org/abs/2312.14925v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {Reinforcement learning from human feedback (RLHF) is a variant of reinforcement learning (RL) that learns from human feedback instead of relying on an engineered reward function. Building on prior work on the related setting of preference-based reinforcement learning (PbRL), it stands at the intersection of artificial intelligence and human-computer interaction. This positioning offers a promising avenue to enhance the performance and adaptability of intelligent systems while also improving the alignment of their objectives with human values. The training of large language models (LLMs) has impressively demonstrated this potential in recent years, where RLHF played a decisive role in directing the model's capabilities toward human objectives. This article provides a comprehensive overview of the fundamentals of RLHF, exploring the intricate dynamics between RL agents and human input. While recent focus has been on RLHF for LLMs, our survey adopts a broader perspective, examining the diverse applications and wide-ranging impact of the technique. We delve into the core principles that underpin RLHF, shedding light on the symbiotic relationship between algorithms and human feedback, and discuss the main research trends in the field. By synthesizing the current landscape of RLHF research, this article aims to provide researchers as well as practitioners with a comprehensive understanding of this rapidly growing field of research.}, Year          = {2023}, Month         = {12}, Url           = {http://arxiv.org/pdf/2312.14925v2}, File          = {2312.14925v2.pdf} }
 
@article{2310.13486v1, Author        = {Lucas Weber and Elia Bruni and Dieuwke Hupkes}, Title         = {Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning}, Eprint        = {http://arxiv.org/abs/2310.13486v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Finding the best way of adapting pre-trained language models to a task is a big challenge in current NLP. Just like the previous generation of task-tuned models (TT), models that are adapted to tasks via in-context-learning (ICL) are robust in some setups but not in others. Here, we present a detailed analysis of which design choices cause instabilities and inconsistencies in LLM predictions. First, we show how spurious correlations between input distributions and labels -- a known issue in TT models -- form only a minor problem for prompted models. Then, we engage in a systematic, holistic evaluation of different factors that have been found to influence predictions in a prompting setup. We test all possible combinations of a range of factors on both vanilla and instruction-tuned (IT) LLMs of different scale and statistically analyse the results to show which factors are the most influential, interactive or stable. Our results show which factors can be used without precautions and which should be avoided or handled with care in most settings.}, Year          = {2023}, Month         = {10}, Url           = {http://arxiv.org/pdf/2310.13486v1}, File          = {2310.13486v1.pdf} }
 
@article{2405.03770v1, Author        = {Neelu Madan and Andreas Moegelmose and Rajat Modi and Yogesh S. Rawat and Thomas B. Moeslund}, Title         = {Foundation Models for Video Understanding: A Survey}, Eprint        = {http://arxiv.org/abs/2405.03770v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {Video Foundation Models (ViFMs) aim to learn a general-purpose representation for various video understanding tasks. Leveraging large-scale datasets and powerful models, ViFMs achieve this by capturing robust and generic features from video data. This survey analyzes over 200 video foundational models, offering a comprehensive overview of benchmarks and evaluation metrics across 14 distinct video tasks categorized into 3 main categories. Additionally, we offer an in-depth performance analysis of these models for the 6 most common video tasks. We categorize ViFMs into three categories: 1) Image-based ViFMs, which adapt existing image models for video tasks, 2) Video-Based ViFMs, which utilize video-specific encoding methods, and 3) Universal Foundational Models (UFMs), which combine multiple modalities (image, video, audio, and text etc.) within a single framework. By comparing the performance of various ViFMs on different tasks, this survey offers valuable insights into their strengths and weaknesses, guiding future advancements in video understanding. Our analysis surprisingly reveals that image-based foundation models consistently outperform video-based models on most video understanding tasks. Additionally, UFMs, which leverage diverse modalities, demonstrate superior performance on video tasks. We share the comprehensive list of ViFMs studied in this work at: \url{https://github.com/NeeluMadan/ViFM_Survey.git}}, Year          = {2024}, Month         = {5}, Url           = {http://arxiv.org/pdf/2405.03770v1}, File          = {2405.03770v1.pdf} }
 
@article{2403.03206v1, Author        = {Patrick Esser and Sumith Kulal and Andreas Blattmann and Rahim Entezari and Jonas Müller and Harry Saini and Yam Levi and Dominik Lorenz and Axel Sauer and Frederic Boesel and Dustin Podell and Tim Dockhorn and Zion English and Kyle Lacey and Alex Goodwin and Yannik Marek and Robin Rombach}, Title         = {Scaling Rectified Flow Transformers for High-Resolution Image Synthesis}, Eprint        = {http://arxiv.org/abs/2403.03206v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available.}, Year          = {2024}, Month         = {3}, Url           = {http://arxiv.org/pdf/2403.03206v1}, File          = {2403.03206v1.pdf} }
 
@article{2102.09690v2, Author        = {Tony Z. Zhao and Eric Wallace and Shi Feng and Dan Klein and Sameer Singh}, Title         = {Calibrate Before Use: Improving Few-Shot Performance of Language Models}, Eprint        = {http://arxiv.org/abs/2102.09690v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as "N/A"". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.}, Year          = {2021}, Month         = {2}, Url           = {http://arxiv.org/pdf/2102.09690v2}, File          = {2102.09690v2.pdf} }"
 
@article{2010.11929v2, Author        = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby}, Title         = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, Eprint        = {http://arxiv.org/abs/2010.11929v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.}, Year          = {2020}, Month         = {10}, Url           = {http://arxiv.org/pdf/2010.11929v2}, File          = {2010.11929v2.pdf} }
 
@article{2303.17651v2, Author        = {Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and Shashank Gupta and Bodhisattwa Prasad Majumder and Katherine Hermann and Sean Welleck and Amir Yazdanbakhsh and Peter Clark}, Title         = {Self-Refine: Iterative Refinement with Self-Feedback}, Eprint        = {http://arxiv.org/abs/2303.17651v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.}, Year          = {2023}, Month         = {3}, Url           = {http://arxiv.org/pdf/2303.17651v2}, File          = {2303.17651v2.pdf} }
 
@article{1811.00937v2, Author        = {Alon Talmor and Jonathan Herzig and Nicholas Lourie and Jonathan Berant}, Title         = {CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge}, Eprint        = {http://arxiv.org/abs/1811.00937v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy, well below human performance, which is 89%.}, Year          = {2018}, Month         = {11}, Url           = {http://arxiv.org/pdf/1811.00937v2}, File          = {1811.00937v2.pdf} }
 
@article{2306.05424v2, Author        = {Muhammad Maaz and Hanoona Rasheed and Salman Khan and Fahad Shahbaz Khan}, Title         = {Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models}, Eprint        = {http://arxiv.org/abs/2306.05424v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {Conversation agents fueled by Large Language Models (LLMs) are providing a new way to interact with visual data. While there have been initial attempts for image-based conversation models, this work addresses the under-explored field of \emph{video-based conversation} by introducing Video-ChatGPT. It is a multimodal model that merges a video-adapted visual encoder with an LLM. The resulting model is capable of understanding and generating detailed conversations about videos. We introduce a new dataset of 100,000 video-instruction pairs used to train Video-ChatGPT acquired via manual and semi-automated pipeline that is easily scalable and robust to label noise. We also develop a quantitative evaluation framework for video-based dialogue models to objectively analyze the strengths and weaknesses of video-based dialogue models. Code: https://github.com/mbzuai-oryx/Video-ChatGPT.}, Year          = {2023}, Month         = {6}, Url           = {http://arxiv.org/pdf/2306.05424v2}, File          = {2306.05424v2.pdf} }
 
@article{2406.03476v1, Author        = {Cody Blakeney and Mansheej Paul and Brett W. Larsen and Sean Owen and Jonathan Frankle}, Title         = {Does your data spark joy? Performance gains from domain upsampling at the end of training}, Eprint        = {http://arxiv.org/abs/2406.03476v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {Pretraining datasets for large language models (LLMs) have grown to trillions of tokens composed of large amounts of CommonCrawl (CC) web scrape along with smaller, domain-specific datasets. It is expensive to understand the impact of these domain-specific datasets on model capabilities as training at large FLOP scales is required to reveal significant changes to difficult and emergent benchmarks. Given the increasing cost of experimenting with pretraining data, how does one determine the optimal balance between the diversity in general web scrapes and the information density of domain specific data? In this work, we show how to leverage the smaller domain specific datasets by upsampling them relative to CC at the end of training to drive performance improvements on difficult benchmarks. This simple technique allows us to improve up to 6.90 pp on MMLU, 8.26 pp on GSM8K, and 6.17 pp on HumanEval relative to the base data mix for a 7B model trained for 1 trillion (T) tokens, thus rivaling Llama-2 (7B)$\unicode{x2014}$a model trained for twice as long. We experiment with ablating the duration of domain upsampling from 5% to 30% of training and find that 10% to 20% percent is optimal for navigating the tradeoff between general language modeling capabilities and targeted benchmarks. We also use domain upsampling to characterize at scale the utility of individual datasets for improving various benchmarks by removing them during this final phase of training. This tool opens up the ability to experiment with the impact of different pretraining datasets at scale, but at an order of magnitude lower cost compared to full pretraining runs.}, Year          = {2024}, Month         = {6}, Url           = {http://arxiv.org/pdf/2406.03476v1}, File          = {2406.03476v1.pdf} }
 
@article{2107.06499v2, Author        = {Katherine Lee and Daphne Ippolito and Andrew Nystrom and Chiyuan Zhang and Douglas Eck and Chris Callison-Burch and Nicholas Carlini}, Title         = {Deduplicating Training Data Makes Language Models Better}, Eprint        = {http://arxiv.org/abs/2107.06499v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings. As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data. We develop two tools that allow us to deduplicate training datasets -- for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times. Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer train steps to achieve the same or better accuracy. We can also reduce train-test overlap, which affects over 4% of the validation set of standard datasets, thus allowing for more accurate evaluation. We release code for reproducing our work and performing dataset deduplication at https://github.com/google-research/deduplicate-text-datasets.}, Year          = {2021}, Month         = {7}, Url           = {http://arxiv.org/pdf/2107.06499v2}, File          = {2107.06499v2.pdf} }
 
@article{2012.03411v2, Author        = {Vineel Pratap and Qiantong Xu and Anuroop Sriram and Gabriel Synnaeve and Ronan Collobert}, Title         = {MLS: A Large-Scale Multilingual Dataset for Speech Research}, Eprint        = {http://arxiv.org/abs/2012.03411v2}, DOI           = {10.21437/Interspeech.2020-2826}, ArchivePrefix = {arXiv}, PrimaryClass  = {eess.AS}, Abstract      = {This paper introduces Multilingual LibriSpeech (MLS) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. Additionally, we provide Language Models (LM) and baseline Automatic Speech Recognition (ASR) models and for all the languages in our dataset. We believe such a large transcribed dataset will open new avenues in ASR and Text-To-Speech (TTS) research. The dataset will be made freely available for anyone at http://www.openslr.org.}, Year          = {2020}, Month         = {12}, Url           = {http://arxiv.org/pdf/2012.03411v2}, File          = {2012.03411v2.pdf} }
 
@article{2101.00390v2, Author        = {Changhan Wang and Morgane Rivière and Ann Lee and Anne Wu and Chaitanya Talnikar and Daniel Haziza and Mary Williamson and Juan Pino and Emmanuel Dupoux}, Title         = {VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation}, Eprint        = {http://arxiv.org/abs/2101.00390v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We introduce VoxPopuli, a large-scale multilingual corpus providing 100K hours of unlabelled speech data in 23 languages. It is the largest open data to date for unsupervised representation learning as well as semi-supervised learning. VoxPopuli also contains 1.8K hours of transcribed speeches in 16 languages and their aligned oral interpretations into 5 other languages totaling 5.1K hours. We provide speech recognition baselines and validate the versatility of VoxPopuli unlabelled data in semi-supervised learning under challenging out-of-domain settings. We will release the corpus at https://github.com/facebookresearch/voxpopuli under an open license.}, Year          = {2021}, Month         = {1}, Url           = {http://arxiv.org/pdf/2101.00390v2}, File          = {2101.00390v2.pdf} }
 
@article{2104.06001v3, Author        = {Beatrice Savoldi and Marco Gaido and Luisa Bentivogli and Matteo Negri and Marco Turchi}, Title         = {Gender Bias in Machine Translation}, Eprint        = {http://arxiv.org/abs/2104.06001v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Machine translation (MT) technology has facilitated our daily tasks by providing accessible shortcuts for gathering, elaborating and communicating information. However, it can suffer from biases that harm users and society at large. As a relatively new field of inquiry, gender bias in MT still lacks internal cohesion, which advocates for a unified framework to ease future research. To this end, we: i) critically review current conceptualizations of bias in light of theoretical insights from related disciplines, ii) summarize previous analyses aimed at assessing gender bias in MT, iii) discuss the mitigating strategies proposed so far, and iv) point toward potential directions for future work.}, Year          = {2021}, Month         = {4}, Url           = {http://arxiv.org/pdf/2104.06001v3}, File          = {2104.06001v3.pdf} }
 
@article{1904.09728v3, Author        = {Maarten Sap and Hannah Rashkin and Derek Chen and Ronan LeBras and Yejin Choi}, Title         = {SocialIQA: Commonsense Reasoning about Social Interactions}, Eprint        = {http://arxiv.org/abs/1904.09728v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We introduce Social IQa, the first largescale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: "Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?"" A: ""Make sure no one else could hear""). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance (>20% gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).}, Year          = {2019}, Month         = {4}, Url           = {http://arxiv.org/pdf/1904.09728v3}, File          = {1904.09728v3.pdf} }"
 
@article{2406.01574v6, Author        = {Yubo Wang and Xueguang Ma and Ge Zhang and Yuansheng Ni and Abhranil Chandra and Shiguang Guo and Weiming Ren and Aaran Arulraj and Xuan He and Ziyan Jiang and Tianle Li and Max Ku and Kai Wang and Alex Zhuang and Rongqi Fan and Xiang Yue and Wenhu Chen}, Title         = {MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark}, Eprint        = {http://arxiv.org/abs/2406.01574v6}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. However, as models continue to improve, their performance on these benchmarks has begun to plateau, making it increasingly difficult to discern differences in model capabilities. This paper introduces MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. Additionally, MMLU-Pro eliminates the trivial and noisy questions in MMLU. Our experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering, which is in stark contrast to the findings on the original MMLU, indicating that MMLU-Pro includes more complex reasoning questions. Our assessments confirm that MMLU-Pro is a more discriminative benchmark to better track progress in the field.}, Year          = {2024}, Month         = {6}, Url           = {http://arxiv.org/pdf/2406.01574v6}, File          = {2406.01574v6.pdf} }
 
@article{2305.14196v3, Author        = {Uri Shaham and Maor Ivgi and Avia Efrat and Jonathan Berant and Omer Levy}, Title         = {ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding}, Eprint        = {http://arxiv.org/abs/2305.14196v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We introduce ZeroSCROLLS, a zero-shot benchmark for natural language understanding over long texts, which contains only test and small validation sets, without training data. We adapt six tasks from the SCROLLS benchmark, and add four new datasets, including two novel information fusing tasks, such as aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a comprehensive evaluation of both open-source and closed large language models, finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest average score. However, there is still room for improvement on multiple open challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to pass the naive baseline. As the state of the art is a moving target, we invite researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard.}, Year          = {2023}, Month         = {5}, Url           = {http://arxiv.org/pdf/2305.14196v3}, File          = {2305.14196v3.pdf} }
 
@article{2311.17035v1, Author        = {Milad Nasr and Nicholas Carlini and Jonathan Hayase and Matthew Jagielski and A. Feder Cooper and Daphne Ippolito and Christopher A. Choquette-Choo and Eric Wallace and Florian Tramèr and Katherine Lee}, Title         = {Scalable Extraction of Training Data from (Production) Language Models}, Eprint        = {http://arxiv.org/abs/2311.17035v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.}, Year          = {2023}, Month         = {11}, Url           = {http://arxiv.org/pdf/2311.17035v1}, File          = {2311.17035v1.pdf} }
 
@article{2312.15685v2, Author        = {Wei Liu and Weihao Zeng and Keqing He and Yong Jiang and Junxian He}, Title         = {What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning}, Eprint        = {http://arxiv.org/abs/2312.15685v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Instruction tuning is a standard technique employed to align large language models to end tasks and user preferences after the initial pretraining phase. Recent research indicates the critical role of data engineering in instruction tuning -- when appropriately selected, only limited data is necessary to achieve superior performance. However, we still lack a principled understanding of what makes good instruction tuning data for alignment, and how we should select data automatically and effectively. In this work, we delve deeply into automatic data selection strategies for alignment. We start with controlled studies to measure data across three dimensions: complexity, quality, and diversity, along which we examine existing methods and introduce novel techniques for enhanced data measurement. Subsequently, we propose a simple strategy to select data samples based on the measurement. We present deita (short for Data-Efficient Instruction Tuning for Alignment), a series of models fine-tuned from LLaMA and Mistral models using data samples automatically selected with our proposed approach. Empirically, deita performs better or on par with the state-of-the-art open-source alignment models with only 6K SFT training data samples -- over 10x less than the data used in the baselines. When further trained with direct preference optimization (DPO), deita-Mistral-7B + DPO trained with 6K SFT and 10K DPO samples achieve 7.55 MT-Bench and 90.06% AlpacaEval scores. We anticipate this work to provide tools on automatic data selection, facilitating data-efficient alignment. We release our models as well as the selected datasets for future researches to effectively align models more efficiently.}, Year          = {2023}, Month         = {12}, Url           = {http://arxiv.org/pdf/2312.15685v2}, File          = {2312.15685v2.pdf} }
 
@article{2212.03860v3, Author        = {Gowthami Somepalli and Vasu Singla and Micah Goldblum and Jonas Geiping and Tom Goldstein}, Title         = {Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models}, Eprint        = {http://arxiv.org/abs/2212.03860v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data.}, Year          = {2022}, Month         = {12}, Url           = {http://arxiv.org/pdf/2212.03860v3}, File          = {2212.03860v3.pdf} }
 
@article{2301.13188v1, Author        = {Nicholas Carlini and Jamie Hayes and Milad Nasr and Matthew Jagielski and Vikash Sehwag and Florian Tramèr and Borja Balle and Daphne Ippolito and Eric Wallace}, Title         = {Extracting Training Data from Diffusion Models}, Eprint        = {http://arxiv.org/abs/2301.13188v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CR}, Abstract      = {Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.}, Year          = {2023}, Month         = {1}, Url           = {http://arxiv.org/pdf/2301.13188v1}, File          = {2301.13188v1.pdf} }
 
@article{2202.07646v3, Author        = {Nicholas Carlini and Daphne Ippolito and Matthew Jagielski and Katherine Lee and Florian Tramer and Chiyuan Zhang}, Title         = {Quantifying Memorization Across Neural Language Models}, Eprint        = {http://arxiv.org/abs/2202.07646v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others).   We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes more complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.}, Year          = {2022}, Month         = {2}, Url           = {http://arxiv.org/pdf/2202.07646v3}, File          = {2202.07646v3.pdf} }
 
@article{2203.09509v4, Author        = {Thomas Hartvigsen and Saadia Gabriel and Hamid Palangi and Maarten Sap and Dipankar Ray and Ece Kamar}, Title         = {ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection}, Eprint        = {http://arxiv.org/abs/2203.09509v4}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language. To help mitigate these issues, we create ToxiGen, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model. Controlling machine generation in this way allows ToxiGen to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of ToxiGen and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5% of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that ToxiGen can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset. Our code and data can be found at https://github.com/microsoft/ToxiGen.}, Year          = {2022}, Month         = {3}, Url           = {http://arxiv.org/pdf/2203.09509v4}, File          = {2203.09509v4.pdf} }
 
@article{2312.04724v1, Author        = {Manish Bhatt and Sahana Chennabasappa and Cyrus Nikolaidis and Shengye Wan and Ivan Evtimov and Dominik Gabi and Daniel Song and Faizan Ahmad and Cornelius Aschermann and Lorenzo Fontana and Sasha Frolov and Ravi Prakash Giri and Dhaval Kapil and Yiannis Kozyrakis and David LeBlanc and James Milazzo and Aleksandar Straumann and Gabriel Synnaeve and Varun Vontimitta and Spencer Whitman and Joshua Saxe}, Title         = {Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models}, Eprint        = {http://arxiv.org/abs/2312.04724v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CR}, Abstract      = {This paper presents CyberSecEval, a comprehensive benchmark developed to help bolster the cybersecurity of Large Language Models (LLMs) employed as coding assistants. As what we believe to be the most extensive unified cybersecurity safety benchmark to date, CyberSecEval provides a thorough evaluation of LLMs in two crucial security domains: their propensity to generate insecure code and their level of compliance when asked to assist in cyberattacks. Through a case study involving seven models from the Llama 2, Code Llama, and OpenAI GPT large language model families, CyberSecEval effectively pinpointed key cybersecurity risks. More importantly, it offered practical insights for refining these models. A significant observation from the study was the tendency of more advanced models to suggest insecure code, highlighting the critical need for integrating security considerations in the development of sophisticated LLMs. CyberSecEval, with its automated test case generation and evaluation pipeline covers a broad scope and equips LLM designers and researchers with a tool to broadly measure and enhance the cybersecurity safety properties of LLMs, contributing to the development of more secure AI systems.}, Year          = {2023}, Month         = {12}, Url           = {http://arxiv.org/pdf/2312.04724v1}, File          = {2312.04724v1.pdf} }
 
@article{2411.03923v1, Author        = {Aaditya K. Singh and Muhammed Yusuf Kocyigit and Andrew Poulton and David Esiobu and Maria Lomeli and Gergely Szilvasy and Dieuwke Hupkes}, Title         = {Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?}, Eprint        = {http://arxiv.org/abs/2411.03923v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Hampering the interpretation of benchmark scores, evaluation data contamination has become a growing concern in the evaluation of LLMs, and an active area of research studies its effects. While evaluation data contamination is easily understood intuitively, it is surprisingly difficult to define precisely which samples should be considered contaminated and, consequently, how it impacts benchmark scores. We propose that these questions should be addressed together and that contamination metrics can be assessed based on whether models benefit from the examples they mark contaminated. We propose a novel analysis method called ConTAM, and show with a large scale survey of existing and novel n-gram based contamination metrics across 13 benchmarks and 7 models from 2 different families that ConTAM can be used to better understand evaluation data contamination and its effects. We find that contamination may have a much larger effect than reported in recent LLM releases and benefits models differently at different scales. We also find that considering only the longest contaminated substring provides a better signal than considering a union of all contaminated substrings, and that doing model and benchmark specific threshold analysis greatly increases the specificity of the results. Lastly, we investigate the impact of hyperparameter choices, finding that, among other things, both using larger values of n and disregarding matches that are infrequent in the pre-training data lead to many false negatives. With ConTAM, we provide a method to empirically ground evaluation data contamination metrics in downstream effects. With our exploration, we shed light on how evaluation data contamination can impact LLMs and provide insight into the considerations important when doing contamination analysis. We end our paper by discussing these in more detail and providing concrete suggestions for future work.}, Year          = {2024}, Month         = {11}, Url           = {http://arxiv.org/pdf/2411.03923v1}, File          = {2411.03923v1.pdf} }
 
@article{2308.01263v3, Author        = {Paul Röttger and Hannah Rose Kirk and Bertie Vidgen and Giuseppe Attanasio and Federico Bianchi and Dirk Hovy}, Title         = {XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models}, Eprint        = {http://arxiv.org/abs/2308.01263v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This risk motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way. XSTest comprises 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. We describe XSTest's creation and composition, and then use the test suite to highlight systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models.}, Year          = {2023}, Month         = {8}, Url           = {http://arxiv.org/pdf/2308.01263v3}, File          = {2308.01263v3.pdf} }
 
@article{2402.13228v2, Author        = {Arka Pal and Deep Karkhanis and Samuel Dooley and Manley Roberts and Siddartha Naidu and Colin White}, Title         = {Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive}, Eprint        = {http://arxiv.org/abs/2402.13228v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the relative probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a reduction of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we find that DPOP outperforms DPO and other fine-tuning procedures across a wide variety of datasets and downstream tasks, including datasets with high edit distances between completions. Furthermore, we find that the DPOP-tuned model outperforms the DPO-tuned model (all else equal) on benchmarks independent of the fine-tuning data, such as MT-Bench. Finally, using DPOP, we create and open-source Smaug-34B and Smaug-72B, with the latter becoming the first open-source LLM to surpass an average accuracy of 80% on the HuggingFace Open LLM Leaderboard.}, Year          = {2024}, Month         = {2}, Url           = {http://arxiv.org/pdf/2402.13228v2}, File          = {2402.13228v2.pdf} }
 
@article{2312.06674v1, Author        = {Hakan Inan and Kartikeya Upasani and Jianfeng Chi and Rashi Rungta and Krithika Iyer and Yuning Mao and Michael Tontchev and Qing Hu and Brian Fuller and Davide Testuggine and Madian Khabsa}, Title         = {Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations}, Eprint        = {http://arxiv.org/abs/2312.06674v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.}, Year          = {2023}, Month         = {12}, Url           = {http://arxiv.org/pdf/2312.06674v1}, File          = {2312.06674v1.pdf} }
 
@article{2402.19255v2, Author        = {Qintong Li and Leyang Cui and Xueliang Zhao and Lingpeng Kong and Wei Bi}, Title         = {GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers}, Eprint        = {http://arxiv.org/abs/2402.19255v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs' math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (GSM-Plus) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes when new statements are added or the question targets are altered. We also explore whether more robust performance can be achieved by composing existing prompting methods, in which we try an iterative method that generates and verifies each intermediate thought based on its reasoning goal and calculation result.}, Year          = {2024}, Month         = {2}, Url           = {http://arxiv.org/pdf/2402.19255v2}, File          = {2402.19255v2.pdf} }
 
@article{2309.03882v4, Author        = {Chujie Zheng and Hao Zhou and Fandong Meng and Jie Zhou and Minlie Huang}, Title         = {Large Language Models Are Not Robust Multiple Choice Selectors}, Eprint        = {http://arxiv.org/abs/2309.03882v4}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Multiple choice questions (MCQs) serve as a common yet important task format in the evaluation of large language models (LLMs). This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent "selection bias"", namely, they prefer to select specific option IDs as answers (like ""Option A""). Through extensive empirical analyses with 20 LLMs on three benchmarks, we pinpoint that this behavioral bias primarily stems from LLMs' token bias, where the model a priori assigns more probabilistic mass to specific option ID tokens (e.g., A/B/C/D) when predicting answers from the option IDs. To mitigate selection bias, we propose a label-free, inference-time debiasing method, called PriDe, which separates the model's prior bias for option IDs from the overall prediction distribution. PriDe first estimates the prior by permutating option contents on a small number of test samples, and then applies the estimated prior to debias the remaining samples. We demonstrate that it achieves interpretable and transferable debiasing with high computational efficiency. We hope this work can draw broader research attention to the bias and robustness of modern LLMs.}, Year          = {2023}, Month         = {9}, Url           = {http://arxiv.org/pdf/2309.03882v4}, File          = {2309.03882v4.pdf} }"
 
@article{2406.10229v1, Author        = {Lovish Madaan and Aaditya K. Singh and Rylan Schaeffer and Andrew Poulton and Sanmi Koyejo and Pontus Stenetorp and Sharan Narang and Dieuwke Hupkes}, Title         = {Quantifying Variance in Evaluation Benchmarks}, Eprint        = {http://arxiv.org/abs/2406.10229v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {Evaluation benchmarks are the cornerstone of measuring capabilities of large language models (LLMs), as well as driving progress in said capabilities. Originally designed to make claims about capabilities (or lack thereof) in fully pretrained models, evaluation benchmarks are now also extensively used to decide between various training choices. Despite this widespread usage, we rarely quantify the variance in our evaluation benchmarks, which dictates whether differences in performance are meaningful. Here, we define and measure a range of metrics geared towards measuring variance in evaluation benchmarks, including seed variance across initialisations, and monotonicity during training. By studying a large number of models -- both openly available and pretrained from scratch -- we provide empirical estimates for a variety of variance metrics, with considerations and recommendations for practitioners. We also evaluate the utility and tradeoffs of continuous versus discrete performance measures and explore options for better understanding and reducing this variance. We find that simple changes, such as framing choice tasks (like MMLU) as completion tasks, can often reduce variance for smaller scale ($\sim$7B) models, while more involved methods inspired from human testing literature (such as item analysis and item response theory) struggle to meaningfully reduce variance. Overall, our work provides insights into variance in evaluation benchmarks, suggests LM-specific techniques to reduce variance, and more generally encourages practitioners to carefully factor in variance when comparing models.}, Year          = {2024}, Month         = {6}, Url           = {http://arxiv.org/pdf/2406.10229v1}, File          = {2406.10229v1.pdf} }
 
@article{1808.07036v3, Author        = {Eunsol Choi and He He and Mohit Iyyer and Mark Yatskar and Wen-tau Yih and Yejin Choi and Percy Liang and Luke Zettlemoyer}, Title         = {QuAC : Question Answering in Context}, Eprint        = {http://arxiv.org/abs/1808.07036v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at http://quac.ai.}, Year          = {2018}, Month         = {8}, Url           = {http://arxiv.org/pdf/1808.07036v3}, File          = {1808.07036v3.pdf} }
 
@article{2210.03057v1, Author        = {Freda Shi and Mirac Suzgun and Markus Freitag and Xuezhi Wang and Suraj Srivats and Soroush Vosoughi and Hyung Won Chung and Yi Tay and Sebastian Ruder and Denny Zhou and Dipanjan Das and Jason Wei}, Title         = {Language Models are Multilingual Chain-of-Thought Reasoners}, Eprint        = {http://arxiv.org/abs/2210.03057v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at https://github.com/google-research/url-nlp.}, Year          = {2022}, Month         = {10}, Url           = {http://arxiv.org/pdf/2210.03057v1}, File          = {2210.03057v1.pdf} }
 
@article{2210.09261v1, Author        = {Mirac Suzgun and Nathan Scales and Nathanael Schärli and Sebastian Gehrmann and Yi Tay and Hyung Won Chung and Aakanksha Chowdhery and Quoc V. Le and Ed H. Chi and Denny Zhou and Jason Wei}, Title         = {Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them}, Eprint        = {http://arxiv.org/abs/2210.09261v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?   In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.}, Year          = {2022}, Month         = {10}, Url           = {http://arxiv.org/pdf/2210.09261v1}, File          = {2210.09261v1.pdf} }
 
@article{2402.01781v2, Author        = {Norah Alzahrani and Hisham Abdullah Alyahya and Yazeed Alnumay and Sultan Alrashed and Shaykhah Alsubaie and Yusef Almushaykeh and Faisal Mirza and Nouf Alotaibi and Nora Altwairesh and Areeb Alowisheq and M Saiful Bari and Haidar Khan}, Title         = {When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards}, Eprint        = {http://arxiv.org/abs/2402.01781v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value - we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple-choice question benchmarks (e.g., MMLU), minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a hybrid scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust evaluation schemes on the existing benchmarks. The code for this paper is available at https://github.com/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness.}, Year          = {2024}, Month         = {2}, Url           = {http://arxiv.org/pdf/2402.01781v2}, File          = {2402.01781v2.pdf} }
 
@article{360287970189639893} is missing info ,Title={Adversarial Examples for Evaluating Reading Comprehension Systems},
 
@article{2403.04132v1, Author        = {Wei-Lin Chiang and Lianmin Zheng and Ying Sheng and Anastasios Nikolas Angelopoulos and Tianle Li and Dacheng Li and Hao Zhang and Banghua Zhu and Michael Jordan and Joseph E. Gonzalez and Ion Stoica}, Title         = {Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference}, Eprint        = {http://arxiv.org/abs/2403.04132v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.AI}, Abstract      = {Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at \url{https://chat.lmsys.org}.}, Year          = {2024}, Month         = {3}, Url           = {http://arxiv.org/pdf/2403.04132v1}, File          = {2403.04132v1.pdf} }
 
@article{1809.02789v1, Author        = {Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal}, Title         = {Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering}, Eprint        = {http://arxiv.org/abs/1809.02789v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We present a new kind of question answering dataset, OpenBookQA, modeled after open book exams for assessing human understanding of a subject. The open book that comes with our questions is a set of 1329 elementary level science facts. Roughly 6000 questions probe an understanding of these facts and their application to novel situations. This requires combining an open book fact (e.g., metals conduct electricity) with broad common knowledge (e.g., a suit of armor is made of metal) obtained from other sources. While existing QA datasets over documents or knowledge bases, being generally self-contained, focus on linguistic understanding, OpenBookQA probes a deeper understanding of both the topic---in the context of common knowledge---and the language it is expressed in. Human performance on OpenBookQA is close to 92%, but many state-of-the-art pre-trained QA methods perform surprisingly poorly, worse than several simple neural baselines we develop. Our oracle experiments designed to circumvent the knowledge retrieval bottleneck demonstrate the value of both the open book and additional facts. We leave it as a challenge to solve the retrieval problem in this multi-hop setting and to close the large gap to human performance.}, Year          = {2018}, Month         = {9}, Url           = {http://arxiv.org/pdf/1809.02789v1}, File          = {1809.02789v1.pdf} }
 
@article{2311.12022v1, Author        = {David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman}, Title         = {GPQA: A Graduate-Level Google-Proof Q&A Benchmark}, Eprint        = {http://arxiv.org/abs/2311.12022v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.AI}, Abstract      = {We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are "Google-proof""). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.}, Year          = {2023}, Month         = {11}, Url           = {http://arxiv.org/pdf/2311.12022v1}, File          = {2311.12022v1.pdf} }"
 
@article{2308.03188v2, Author        = {Liangming Pan and Michael Saxon and Wenda Xu and Deepak Nathani and Xinyi Wang and William Yang Wang}, Title         = {Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies}, Eprint        = {http://arxiv.org/abs/2308.03188v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.}, Year          = {2023}, Month         = {8}, Url           = {http://arxiv.org/pdf/2308.03188v2}, File          = {2308.03188v2.pdf} }
 
@article{2406.19470v2, Author        = {Vipul Gupta and David Pantoja and Candace Ross and Adina Williams and Megan Ung}, Title         = {Changing Answer Order Can Decrease MMLU Accuracy}, Eprint        = {http://arxiv.org/abs/2406.19470v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {As large language models (LLMs) have grown in prevalence, particular benchmarks have become essential for the evaluation of these models and for understanding model capabilities. Most commonly, we use test accuracy averaged across multiple subtasks in order to rank models on leaderboards, to determine which model is best for our purposes. In this paper, we investigate the robustness of the accuracy measurement on a widely used multiple choice question answering dataset, MMLU. When shuffling the answer label contents, we find that all explored models decrease in accuracy on MMLU, but not every model is equally sensitive. These findings suggest a possible adjustment to the standard practice of leaderboard testing, where we additionally consider the percentage of examples each model answers correctly by random chance.}, Year          = {2024}, Month         = {6}, Url           = {http://arxiv.org/pdf/2406.19470v2}, File          = {2406.19470v2.pdf} }
 
@article{2206.07682v2, Author        = {Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus}, Title         = {Emergent Abilities of Large Language Models}, Eprint        = {http://arxiv.org/abs/2206.07682v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.}, Year          = {2022}, Month         = {6}, Url           = {http://arxiv.org/pdf/2206.07682v2}, File          = {2206.07682v2.pdf} }
 
@article{1603.07396v1, Author        = {Aniruddha Kembhavi and Mike Salvato and Eric Kolve and Minjoon Seo and Hannaneh Hajishirzi and Ali Farhadi}, Title         = {A Diagram Is Worth A Dozen Images}, Eprint        = {http://arxiv.org/abs/1603.07396v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {Diagrams are common tools for representing complex concepts, relationships and events, often when it would be difficult to portray the same information with natural images. Understanding natural images has been extensively studied in computer vision, while diagram understanding has received little attention. In this paper, we study the problem of diagram interpretation and reasoning, the challenging task of identifying the structure of a diagram and the semantics of its constituents and their relationships. We introduce Diagram Parse Graphs (DPG) as our representation to model the structure of diagrams. We define syntactic parsing of diagrams as learning to infer DPGs for diagrams and study semantic interpretation and reasoning of diagrams in the context of diagram question answering. We devise an LSTM-based method for syntactic parsing of diagrams and introduce a DPG-based attention model for diagram question answering. We compile a new dataset of diagrams with exhaustive annotations of constituents and relationships for over 5,000 diagrams and 15,000 questions and answers. Our results show the significance of our models for syntactic parsing and question answering in diagrams using DPGs.}, Year          = {2016}, Month         = {3}, Url           = {http://arxiv.org/pdf/1603.07396v1}, File          = {1603.07396v1.pdf} }
 
@article{1702.08734v1, Author        = {Jeff Johnson and Matthijs Douze and Hervé Jégou}, Title         = {Billion-scale similarity search with GPUs}, Eprint        = {http://arxiv.org/abs/1702.08734v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy.   We propose a design for k-selection that operates at up to 55% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art. We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.}, Year          = {2017}, Month         = {2}, Url           = {http://arxiv.org/pdf/1702.08734v1}, File          = {1702.08734v1.pdf} }
 
@article{2311.10122v3, Author        = {Bin Lin and Yang Ye and Bin Zhu and Jiaxi Cui and Munan Ning and Peng Jin and Li Yuan}, Title         = {Video-LLaVA: Learning United Visual Representation by Alignment Before Projection}, Eprint        = {http://arxiv.org/abs/2311.10122v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CV}, Abstract      = {The Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers. In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other. Video-LLaVA achieves superior performances on a broad range of 9 image benchmarks across 5 image question-answering datasets and 4 image benchmark toolkits. Additionally, our Video-LLaVA also outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos. We aim for this work to provide modest insights into the multi-modal inputs for the LLM. Code address: \href{https://github.com/PKU-YuanGroup/Video-LLaVA}}, Year          = {2023}, Month         = {11}, Url           = {http://arxiv.org/pdf/2311.10122v3}, File          = {2311.10122v3.pdf} }
 
@article{2310.08419v4, Author        = {Patrick Chao and Alexander Robey and Edgar Dobriban and Hamed Hassani and George J. Pappas and Eric Wong}, Title         = {Jailbreaking Black Box Large Language Models in Twenty Queries}, Eprint        = {http://arxiv.org/abs/2310.08419v4}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.LG}, Abstract      = {There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and Gemini.}, Year          = {2023}, Month         = {10}, Url           = {http://arxiv.org/pdf/2310.08419v4}, File          = {2310.08419v4.pdf} }
 
@article{2309.05653v3, Author        = {Xiang Yue and Xingwei Qu and Ge Zhang and Yao Fu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen}, Title         = {MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning}, Eprint        = {http://arxiv.org/abs/2309.05653v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16% and 32%. Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 23%, and the MAmmoTH-34B model achieves 44% accuracy on MATH, even surpassing GPT-4's CoT result. Our work underscores the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models.}, Year          = {2023}, Month         = {9}, Url           = {http://arxiv.org/pdf/2309.05653v3}, File          = {2309.05653v3.pdf} }
 
@article{2306.15687v2, Author        = {Matthew Le and Apoorv Vyas and Bowen Shi and Brian Karrer and Leda Sari and Rashel Moritz and Mary Williamson and Vimal Manohar and Yossi Adi and Jay Mahadeokar and Wei-Ning Hsu}, Title         = {Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale}, Eprint        = {http://arxiv.org/abs/2306.15687v2}, ArchivePrefix = {arXiv}, PrimaryClass  = {eess.AS}, Abstract      = {Large-scale generative models such as GPT and DALL-E have revolutionized the research community. These models not only generate high fidelity outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are not filtered or enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox outperforms the state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs 1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to 20 times faster. Audio samples can be found in \url{https://voicebox.metademolab.com}.}, Year          = {2023}, Month         = {6}, Url           = {http://arxiv.org/pdf/2306.15687v2}, File          = {2306.15687v2.pdf} }
 
@article{2309.12284v4, Author        = {Longhui Yu and Weisen Jiang and Han Shi and Jincheng Yu and Zhengying Liu and Yu Zhang and James T. Kwok and Zhenguo Li and Adrian Weller and Weiyang Liu}, Title         = {MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models}, Eprint        = {http://arxiv.org/abs/2309.12284v4}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use.}, Year          = {2023}, Month         = {9}, Url           = {http://arxiv.org/pdf/2309.12284v4}, File          = {2309.12284v4.pdf} }
 
@article{2104.14337v1, Author        = {Douwe Kiela and Max Bartolo and Yixin Nie and Divyansh Kaushik and Atticus Geiger and Zhengxuan Wu and Bertie Vidgen and Grusha Prasad and Amanpreet Singh and Pratik Ringshia and Zhiyi Ma and Tristan Thrush and Sebastian Riedel and Zeerak Waseem and Pontus Stenetorp and Robin Jia and Mohit Bansal and Christopher Potts and Adina Williams}, Title         = {Dynabench: Rethinking Benchmarking in NLP}, Eprint        = {http://arxiv.org/abs/2104.14337v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.}, Year          = {2021}, Month         = {4}, Url           = {http://arxiv.org/pdf/2104.14337v1}, File          = {2104.14337v1.pdf} }
 
@article{1504.04909v1, Author        = {Jean-Baptiste Mouret and Jeff Clune}, Title         = {Illuminating search spaces by mapping elites}, Eprint        = {http://arxiv.org/abs/1504.04909v1}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.AI}, Abstract      = {Many fields use search algorithms, which automatically explore a search space to find high-performing solutions: chemists search through the space of molecules to discover new drugs; engineers search for stronger, cheaper, safer designs, scientists search for models that best explain data, etc. The goal of search algorithms has traditionally been to return the single highest-performing solution in a search space. Here we describe a new, fundamentally different type of algorithm that is more useful because it provides a holistic view of how high-performing solutions are distributed throughout a search space. It creates a map of high-performing solutions at each point in a space defined by dimensions of variation that a user gets to choose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites) algorithm illuminates search spaces, allowing researchers to understand how interesting attributes of solutions combine to affect performance, either positively or, equally of interest, negatively. For example, a drug company may wish to understand how performance changes as the size of molecules and their cost-to-produce vary. MAP-Elites produces a large diversity of high-performing, yet qualitatively different solutions, which can be more helpful than a single, high-performing solution. Interestingly, because MAP-Elites explores more of the search space, it also tends to find a better overall solution than state-of-the-art search algorithms. We demonstrate the benefits of this new algorithm in three different problem domains ranging from producing modular neural networks to designing simulated and real soft robots. Because MAP- Elites (1) illuminates the relationship between performance and dimensions of interest in solutions, (2) returns a set of high-performing, yet diverse solutions, and (3) improves finding a single, best solution, it will advance science and engineering.}, Year          = {2015}, Month         = {4}, Url           = {http://arxiv.org/pdf/1504.04909v1}, File          = {1504.04909v1.pdf} }
 
@article{2210.12353v3, Author        = {Joshua Robinson and Christopher Michael Rytting and David Wingate}, Title         = {Leveraging Large Language Models for Multiple Choice Question Answering}, Eprint        = {http://arxiv.org/abs/2210.12353v3}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g., "A"") associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the LLM it is used with must be able to associate answer options with the symbols that represent them. The LLM needs what we term multiple choice symbol binding (MCSB) ability. This ability varies greatly by model. We show that a model with high MCSB ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been previously underestimated.}, Year          = {2022}, Month         = {10}, Url           = {http://arxiv.org/pdf/2210.12353v3}, File          = {2210.12353v3.pdf} }"
 
@article{1704.04683v5, Author        = {Guokun Lai and Qizhe Xie and Hanxiao Liu and Yiming Yang and Eduard Hovy}, Title         = {RACE: Large-scale ReAding Comprehension Dataset From Examinations}, Eprint        = {http://arxiv.org/abs/1704.04683v5}, ArchivePrefix = {arXiv}, PrimaryClass  = {cs.CL}, Abstract      = {We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43%) and the ceiling human performance (95%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/~glai1/data/race/ and the code is available at https://github.com/qizhex/RACE_AR_baselines.}, Year          = {2017}, Month         = {4}, Url           = {http://arxiv.org/pdf/1704.04683v5}, File          = {1704.04683v5.pdf} }