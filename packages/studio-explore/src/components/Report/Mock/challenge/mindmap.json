{
  "categories": [
    {
      "category_id": 1,
      "name": "Optimization and Convergence",
      "description": "Papers that focus on challenges related to the optimization process, including convergence properties of algorithms like SGD and the trade-offs between training loss and generalization.",
      "children": [
        {
          "id": "360287970189639682",
          "label": "Paper",
          "properties": {
            "title": "Averaging Weights Leads to Wider Optima and Better Generalization"
          }
        },
        {
          "id": "360287970189639708",
          "label": "Paper",
          "properties": {
            "title": "Common 7B Language Models Already Possess Strong Math Capabilities"
          }
        },
        {
          "id": "360287970189639718",
          "label": "Paper",
          "properties": {
            "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation"
          }
        },
        {
          "id": "360287970189639761",
          "label": "Paper",
          "properties": {
            "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
          }
        },
        {
          "id": "360287970189639767",
          "label": "Paper",
          "properties": {
            "title": "Proximal Policy Optimization Algorithms"
          }
        },
        {
          "id": "360287970189639798",
          "label": "Paper",
          "properties": {
            "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"
          }
        },
        {
          "id": "360287970189639797",
          "label": "Paper",
          "properties": {
            "title": "Training Verifiers to Solve Math Word Problems"
          }
        },
        {
          "id": "360287970189639825",
          "label": "Paper",
          "properties": {
            "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks"
          }
        },
        {
          "id": "360287970189639845",
          "label": "Paper",
          "properties": {
            "title": "Reframing Instructional Prompts to GPTk's Language"
          }
        },
        {
          "id": "360287970189639863",
          "label": "Paper",
          "properties": {
            "title": "Generating Sequences by Learning to Self-Correct"
          }
        }
      ]
    },
    {
      "category_id": 2,
      "name": "Resource and Scalability",
      "description": "Papers that address challenges associated with computational resources, scalability, and the practical feasibility of large-scale training and dataset requirements.",
      "children": [
        {
          "id": "360287970189639681",
          "label": "Paper",
          "properties": {
            "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"
          }
        },
        {
          "id": "360287970189639696",
          "label": "Paper",
          "properties": {
            "title": "TALM: Tool Augmented Language Models"
          }
        },
        {
          "id": "360287970189639686",
          "label": "Paper",
          "properties": {
            "title": "Self-supervised Learning with Random-projection Quantizer for Speech Recognition"
          }
        },
        {
          "id": "360287970189639721",
          "label": "Paper",
          "properties": {
            "title": "TOOLVERIFIER: Generalization to New Tools via Self-Verification"
          }
        },
        {
          "id": "360287970189639740",
          "label": "Paper",
          "properties": {
            "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"
          }
        },
        {
          "id": "360287970189639760",
          "label": "Paper",
          "properties": {
            "title": "MAmmoTH2: Scaling Instructions from the Web"
          }
        },
        {
          "id": "360287970189639680",
          "label": "Paper",
          "properties": {
            "title": "Bag of Tricks for Efficient Text Classification"
          }
        },
        {
          "id": "360287970189639766",
          "label": "Paper",
          "properties": {
            "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"
          }
        },
        {
          "id": "360287970189639775",
          "label": "Paper",
          "properties": {
            "title": "SemDeDup: Data-efficient learning at web-scale through semantic deduplication"
          }
        },
        {
          "id": "360287970189639776",
          "label": "Paper",
          "properties": {
            "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
          }
        },
        {
          "id": "360287970189639782",
          "label": "Paper",
          "properties": {
            "title": "OpenELM: An Efficient Language Model Family with Open Training and Inference Framework"
          }
        },
        {
          "id": "360287970189639784",
          "label": "Paper",
          "properties": {
            "title": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models"
          }
        },
        {
          "id": "360287970189639785",
          "label": "Paper",
          "properties": {
            "title": "Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models"
          }
        },
        {
          "id": "360287970189639791",
          "label": "Paper",
          "properties": {
            "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"
          }
        },
        {
          "id": "360287970189639795",
          "label": "Paper",
          "properties": {
            "title": "CoVoST 2 and Massively Multilingual Speech-to-Text Translation"
          }
        },
        {
          "id": "360287970189639801",
          "label": "Paper",
          "properties": {
            "title": "The Falcon Series of Open Language Models"
          }
        },
        {
          "id": "360287970189639754",
          "label": "Paper",
          "properties": {
            "title": "The Stack: 3 TB of permissively licensed source code"
          }
        },
        {
          "id": "360287970189639804",
          "label": "Paper",
          "properties": {
            "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"
          }
        },
        {
          "id": "360287970189639821",
          "label": "Paper",
          "properties": {
            "title": "Robust Speech Recognition via Large-Scale Weak Supervision"
          }
        },
        {
          "id": "360287970189639746",
          "label": "Paper",
          "properties": {
            "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"
          }
        },
        {
          "id": "360287970189639831",
          "label": "Paper",
          "properties": {
            "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"
          }
        },
        {
          "id": "360287970189639755",
          "label": "Paper",
          "properties": {
            "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models"
          }
        },
        {
          "id": "360287970189639691",
          "label": "Paper",
          "properties": {
            "title": "Exploring the Limits of Weakly Supervised Pretraining"
          }
        },
        {
          "id": "360287970189639852",
          "label": "Paper",
          "properties": {
            "title": "Attention Is All You Need"
          }
        },
        {
          "id": "360287970189639853",
          "label": "Paper",
          "properties": {
            "title": "WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models"
          }
        },
        {
          "id": "360287970189639778",
          "label": "Paper",
          "properties": {
            "title": "Efficient Estimation of Word Representations in Vector Space"
          }
        },
        {
          "id": "360287970189639869",
          "label": "Paper",
          "properties": {
            "title": "Scaling Instruction-Finetuned Language Models"
          }
        },
        {
          "id": "360287970189639884",
          "label": "Paper",
          "properties": {
            "title": "Efficient Neural Audio Synthesis"
          }
        },
        {
          "id": "360287970189639888",
          "label": "Paper",
          "properties": {
            "title": "PaLM: Scaling Language Modeling with Pathways"
          }
        },
        {
          "id": "360287970189639897",
          "label": "Paper",
          "properties": {
            "title": "Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning"
          }
        },
        {
          "id": "360287970189639898",
          "label": "Paper",
          "properties": {
            "title": "Demystifying CLIP Data"
          }
        },
        {
          "id": "360287970189639899",
          "label": "Paper",
          "properties": {
            "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"
          }
        },
        {
          "id": "360287970189639901",
          "label": "Paper",
          "properties": {
            "title": "Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs"
          }
        },
        {
          "id": "360287970189639807",
          "label": "Paper",
          "properties": {
            "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context"
          }
        },
        {
          "id": "360287970189639829",
          "label": "Paper",
          "properties": {
            "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models"
          }
        },
        {
          "id": "360287970189639904",
          "label": "Paper",
          "properties": {
            "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"
          }
        },
        {
          "id": "360287970189639892",
          "label": "Paper",
          "properties": {
            "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks"
          }
        },
        {
          "id": "360287970189639706",
          "label": "Paper",
          "properties": {
            "title": "Gemma: Open Models Based on Gemini Research and Technology"
          }
        },
        {
          "id": "360287970189639915",
          "label": "Paper",
          "properties": {
            "title": "BASE Layers: Simplifying Training of Large, Sparse Models"
          }
        }
      ]
    },
    {
      "category_id": 3,
      "name": "Model Architecture and Flexibility",
      "description": "Papers that discuss challenges in model architecture design, focusing on flexibility, context understanding, and the ability to generalize across different tasks and modalities.",
      "children": [
        {
          "id": "360287970189639683",
          "label": "Paper",
          "properties": {
            "title": "Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V"
          }
        },
        {
          "id": "360287970189639685",
          "label": "Paper",
          "properties": {
            "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"
          }
        },
        {
          "id": "360287970189639689",
          "label": "Paper",
          "properties": {
            "title": "WebGPT: Browser-assisted question-answering with human feedback"
          }
        },
        {
          "id": "360287970189639692",
          "label": "Paper",
          "properties": {
            "title": "Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving"
          }
        },
        {
          "id": "360287970189639693",
          "label": "Paper",
          "properties": {
            "title": "Foundation Models for Video Understanding: A Survey"
          }
        },
        {
          "id": "360287970189639695",
          "label": "Paper",
          "properties": {
            "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis"
          }
        },
        {
          "id": "360287970189639703",
          "label": "Paper",
          "properties": {
            "title": "Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning"
          }
        },
        {
          "id": "360287970189639715",
          "label": "Paper",
          "properties": {
            "title": "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks"
          }
        },
        {
          "id": "360287970189639717",
          "label": "Paper",
          "properties": {
            "title": "Perceiver: General Perception with Iterative Attention"
          }
        },
        {
          "id": "360287970189639725",
          "label": "Paper",
          "properties": {
            "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"
          }
        },
        {
          "id": "360287970189639727",
          "label": "Paper",
          "properties": {
            "title": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel"
          }
        },
        {
          "id": "360287970189639716",
          "label": "Paper",
          "properties": {
            "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"
          }
        },
        {
          "id": "360287970189639733",
          "label": "Paper",
          "properties": {
            "title": "Code Llama: Open Foundation Models for Code"
          }
        },
        {
          "id": "360287970189639756",
          "label": "Paper",
          "properties": {
            "title": "Spirit LM: Interleaved Spoken and Written Language Model"
          }
        },
        {
          "id": "360287970189639758",
          "label": "Paper",
          "properties": {
            "title": "A Survey of Reinforcement Learning from Human Feedback"
          }
        },
        {
          "id": "360287970189639769",
          "label": "Paper",
          "properties": {
            "title": "Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding"
          }
        },
        {
          "id": "360287970189639770",
          "label": "Paper",
          "properties": {
            "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints"
          }
        },
        {
          "id": "360287970189639774",
          "label": "Paper",
          "properties": {
            "title": "Reducing Activation Recomputation in Large Transformer Models"
          }
        },
        {
          "id": "360287970189639777",
          "label": "Paper",
          "properties": {
            "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
          }
        },
        {
          "id": "360287970189639713",
          "label": "Paper",
          "properties": {
            "title": "ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning"
          }
        },
        {
          "id": "360287970189639707",
          "label": "Paper",
          "properties": {
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
          }
        },
        {
          "id": "360287970189639750",
          "label": "Paper",
          "properties": {
            "title": "ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts"
          }
        },
        {
          "id": "360287970189639787",
          "label": "Paper",
          "properties": {
            "title": "ReAct: Synergizing Reasoning and Acting in Language Models"
          }
        },
        {
          "id": "360287970189639792",
          "label": "Paper",
          "properties": {
            "title": "NExT-QA:Next Phase of Question-Answering to Explaining Temporal Actions"
          }
        },
        {
          "id": "360287970189639793",
          "label": "Paper",
          "properties": {
            "title": "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI"
          }
        },
        {
          "id": "360287970189639796",
          "label": "Paper",
          "properties": {
            "title": "$\\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens"
          }
        },
        {
          "id": "360287970189639803",
          "label": "Paper",
          "properties": {
            "title": "Training Deep Neural Networks with Joint Quantization and Pruning of Weights and Activations"
          }
        },
        {
          "id": "360287970189639805",
          "label": "Paper",
          "properties": {
            "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"
          }
        },
        {
          "id": "360287970189639808",
          "label": "Paper",
          "properties": {
            "title": "Mixture-of-Experts with Expert Choice Routing"
          }
        },
        {
          "id": "360287970189639813",
          "label": "Paper",
          "properties": {
            "title": "Measuring Mathematical Problem Solving With the MATH Dataset"
          }
        },
        {
          "id": "360287970189639815",
          "label": "Paper",
          "properties": {
            "title": "VideoChat: Chat-Centric Video Understanding"
          }
        },
        {
          "id": "360287970189639816",
          "label": "Paper",
          "properties": {
            "title": "Iterative Reasoning Preference Optimization"
          }
        },
        {
          "id": "360287970189639823",
          "label": "Paper",
          "properties": {
            "title": "Know What You Don't Know: Unanswerable Questions for SQuAD"
          }
        },
        {
          "id": "360287970189639826",
          "label": "Paper",
          "properties": {
            "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"
          }
        },
        {
          "id": "360287970189639827",
          "label": "Paper",
          "properties": {
            "title": "DocVQA: A Dataset for VQA on Document Images"
          }
        },
        {
          "id": "360287970189639828",
          "label": "Paper",
          "properties": {
            "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"
          }
        },
        {
          "id": "360287970189639702",
          "label": "Paper",
          "properties": {
            "title": "Self-Refine: Iterative Refinement with Self-Feedback"
          }
        },
        {
          "id": "360287970189639837",
          "label": "Paper",
          "properties": {
            "title": "STaR: Bootstrapping Reasoning With Reasoning"
          }
        },
        {
          "id": "360287970189639843",
          "label": "Paper",
          "properties": {
            "title": "Effective Long-Context Scaling of Foundation Models"
          }
        },
        {
          "id": "360287970189639772",
          "label": "Paper",
          "properties": {
            "title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation"
          }
        },
        {
          "id": "360287970189639848",
          "label": "Paper",
          "properties": {
            "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling"
          }
        },
        {
          "id": "360287970189639850",
          "label": "Paper",
          "properties": {
            "title": "Learning Video Representations from Large Language Models"
          }
        },
        {
          "id": "360287970189639684",
          "label": "Paper",
          "properties": {
            "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity"
          }
        },
        {
          "id": "360287970189639857",
          "label": "Paper",
          "properties": {
            "title": "Bilateral Multi-Perspective Matching for Natural Language Sentences"
          }
        },
        {
          "id": "360287970189639744",
          "label": "Paper",
          "properties": {
            "title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"
          }
        },
        {
          "id": "360287970189639864",
          "label": "Paper",
          "properties": {
            "title": "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action"
          }
        },
        {
          "id": "360287970189639856",
          "label": "Paper",
          "properties": {
            "title": "#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models"
          }
        },
        {
          "id": "360287970189639847",
          "label": "Paper",
          "properties": {
            "title": "Solving math word problems with process- and outcome-based feedback"
          }
        },
        {
          "id": "360287970189639870",
          "label": "Paper",
          "properties": {
            "title": "PAL: Program-aided Language Models"
          }
        },
        {
          "id": "360287970189639872",
          "label": "Paper",
          "properties": {
            "title": "Toolformer: Language Models Can Teach Themselves to Use Tools"
          }
        },
        {
          "id": "360287970189639874",
          "label": "Paper",
          "properties": {
            "title": "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving"
          }
        },
        {
          "id": "360287970189639875",
          "label": "Paper",
          "properties": {
            "title": "Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset"
          }
        },
        {
          "id": "360287970189639879",
          "label": "Paper",
          "properties": {
            "title": "TVQA: Localized, Compositional Video Question Answering"
          }
        },
        {
          "id": "360287970189639883",
          "label": "Paper",
          "properties": {
            "title": "Improved Baselines with Visual Instruction Tuning"
          }
        },
        {
          "id": "360287970189639885",
          "label": "Paper",
          "properties": {
            "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
          }
        },
        {
          "id": "360287970189639751",
          "label": "Paper",
          "properties": {
            "title": "VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation"
          }
        },
        {
          "id": "360287970189639886",
          "label": "Paper",
          "properties": {
            "title": "Conformer: Convolution-augmented Transformer for Speech Recognition"
          }
        },
        {
          "id": "360287970189639836",
          "label": "Paper",
          "properties": {
            "title": "Gorilla: Large Language Model Connected with Massive APIs"
          }
        },
        {
          "id": "360287970189639820",
          "label": "Paper",
          "properties": {
            "title": "EXAMS: A Multi-Subject High School Examinations Dataset for Cross-Lingual and Multilingual Question Answering"
          }
        },
        {
          "id": "360287970189639724",
          "label": "Paper",
          "properties": {
            "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models"
          }
        },
        {
          "id": "360287970189639896",
          "label": "Paper",
          "properties": {
            "title": "Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations"
          }
        },
        {
          "id": "360287970189639700",
          "label": "Paper",
          "properties": {
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
          }
        },
        {
          "id": "360287970189639771",
          "label": "Paper",
          "properties": {
            "title": "Finetuned Language Models Are Zero-Shot Learners"
          }
        },
        {
          "id": "360287970189639902",
          "label": "Paper",
          "properties": {
            "title": "AudioChatLlama: Towards General-Purpose Speech Abilities for LLMs"
          }
        },
        {
          "id": "360287970189639909",
          "label": "Paper",
          "properties": {
            "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"
          }
        },
        {
          "id": "360287970189639698",
          "label": "Paper",
          "properties": {
            "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models"
          }
        },
        {
          "id": "360287970189639833",
          "label": "Paper",
          "properties": {
            "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
          }
        }
      ]
    },
    {
      "category_id": 4,
      "name": "Data Quality and Diversity",
      "description": "Papers that highlight issues related to data quality, diversity, and the impact of these factors on model performance and generalization.",
      "children": [
        {
          "id": "360287970189639711",
          "label": "Paper",
          "properties": {
            "title": "Does your data spark joy? Performance gains from domain upsampling at the end of training"
          }
        },
        {
          "id": "360287970189639722",
          "label": "Paper",
          "properties": {
            "title": "MLS: A Large-Scale Multilingual Dataset for Speech Research"
          }
        },
        {
          "id": "360287970189639729",
          "label": "Paper",
          "properties": {
            "title": "SocialIQA: Commonsense Reasoning about Social Interactions"
          }
        },
        {
          "id": "360287970189639730",
          "label": "Paper",
          "properties": {
            "title": "VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation"
          }
        },
        {
          "id": "360287970189639731",
          "label": "Paper",
          "properties": {
            "title": "PromptTTS: Controllable Text-to-Speech with Text Descriptions"
          }
        },
        {
          "id": "360287970189639732",
          "label": "Paper",
          "properties": {
            "title": "Learning Transferable Visual Models From Natural Language Supervision"
          }
        },
        {
          "id": "360287970189639736",
          "label": "Paper",
          "properties": {
            "title": "Toward Joint Language Modeling for Speech Units and Text"
          }
        },
        {
          "id": "360287970189639739",
          "label": "Paper",
          "properties": {
            "title": "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark"
          }
        },
        {
          "id": "360287970189639743",
          "label": "Paper",
          "properties": {
            "title": "ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding"
          }
        },
        {
          "id": "360287970189639690",
          "label": "Paper",
          "properties": {
            "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"
          }
        },
        {
          "id": "360287970189639694",
          "label": "Paper",
          "properties": {
            "title": "Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning"
          }
        },
        {
          "id": "360287970189639806",
          "label": "Paper",
          "properties": {
            "title": "Gender Bias in Machine Translation"
          }
        },
        {
          "id": "360287970189639819",
          "label": "Paper",
          "properties": {
            "title": "Scalable Extraction of Training Data from (Production) Language Models"
          }
        },
        {
          "id": "360287970189639824",
          "label": "Paper",
          "properties": {
            "title": "LIMA: Less Is More for Alignment"
          }
        },
        {
          "id": "360287970189639838",
          "label": "Paper",
          "properties": {
            "title": "ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering"
          }
        },
        {
          "id": "360287970189639841",
          "label": "Paper",
          "properties": {
            "title": "Deduplicating Training Data Makes Language Models Better"
          }
        },
        {
          "id": "360287970189639842",
          "label": "Paper",
          "properties": {
            "title": "Orca-Math: Unlocking the potential of SLMs in Grade School Math"
          }
        },
        {
          "id": "360287970189639832",
          "label": "Paper",
          "properties": {
            "title": "Magicoder: Empowering Code Generation with OSS-Instruct"
          }
        },
        {
          "id": "360287970189639855",
          "label": "Paper",
          "properties": {
            "title": "API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs"
          }
        },
        {
          "id": "360287970189639860",
          "label": "Paper",
          "properties": {
            "title": "LLaMA: Open and Efficient Foundation Language Models"
          }
        },
        {
          "id": "360287970189639861",
          "label": "Paper",
          "properties": {
            "title": "Verb Semantics and Lexical Selection"
          }
        },
        {
          "id": "360287970189639822",
          "label": "Paper",
          "properties": {
            "title": "FLEURS: Few-shot Learning Evaluation of Universal Representations of Speech"
          }
        },
        {
          "id": "360287970189639851",
          "label": "Paper",
          "properties": {
            "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"
          }
        },
        {
          "id": "360287970189639862",
          "label": "Paper",
          "properties": {
            "title": "What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning"
          }
        },
        {
          "id": "360287970189639712",
          "label": "Paper",
          "properties": {
            "title": "Perception Test: A Diagnostic Benchmark for Multimodal Video Models"
          }
        },
        {
          "id": "360287970189639877",
          "label": "Paper",
          "properties": {
            "title": "CyberSecEval 2: A Wide-Ranging Cybersecurity Evaluation Suite for Large Language Models"
          }
        },
        {
          "id": "360287970189639878",
          "label": "Paper",
          "properties": {
            "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"
          }
        },
        {
          "id": "360287970189639911",
          "label": "Paper",
          "properties": {
            "title": "PAWS: Paraphrase Adversaries from Word Scrambling"
          }
        },
        {
          "id": "360287970189639913",
          "label": "Paper",
          "properties": {
            "title": "A Self-Supervised Descriptor for Image Copy Detection"
          }
        },
        {
          "id": "360287970189639916",
          "label": "Paper",
          "properties": {
            "title": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?"
          }
        },
        {
          "id": "360287970189639840",
          "label": "Paper",
          "properties": {
            "title": "MuTox: Universal MUltilingual Audio-based TOXicity Dataset and Zero-shot Detector"
          }
        }
      ]
    },
    {
      "category_id": 5,
      "name": "Ethical and Legal Considerations",
      "description": "Papers that explore ethical and legal challenges, such as content replication, bias, and the implications of using large datasets for training models.",
      "children": [
        {
          "id": "360287970189639719",
          "label": "Paper",
          "properties": {
            "title": "Measuring Massive Multitask Language Understanding"
          }
        },
        {
          "id": "360287970189639720",
          "label": "Paper",
          "properties": {
            "title": "Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models"
          }
        },
        {
          "id": "360287970189639726",
          "label": "Paper",
          "properties": {
            "title": "Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts"
          }
        },
        {
          "id": "360287970189639765",
          "label": "Paper",
          "properties": {
            "title": "Augmented Language Models: a Survey"
          }
        },
        {
          "id": "360287970189639779",
          "label": "Paper",
          "properties": {
            "title": "Extracting Training Data from Diffusion Models"
          }
        },
        {
          "id": "360287970189639780",
          "label": "Paper",
          "properties": {
            "title": "Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?"
          }
        },
        {
          "id": "360287970189639786",
          "label": "Paper",
          "properties": {
            "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive"
          }
        },
        {
          "id": "360287970189639790",
          "label": "Paper",
          "properties": {
            "title": "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations"
          }
        },
        {
          "id": "360287970189639812",
          "label": "Paper",
          "properties": {
            "title": "GPT-4 Technical Report"
          }
        },
        {
          "id": "360287970189639814",
          "label": "Paper",
          "properties": {
            "title": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection"
          }
        },
        {
          "id": "360287970189639835",
          "label": "Paper",
          "properties": {
            "title": "Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models"
          }
        },
        {
          "id": "360287970189639802",
          "label": "Paper",
          "properties": {
            "title": "The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions"
          }
        },
        {
          "id": "360287970189639858",
          "label": "Paper",
          "properties": {
            "title": "Multi-Task Learning for Front-End Text Processing in TTS"
          }
        },
        {
          "id": "360287970189639867",
          "label": "Paper",
          "properties": {
            "title": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models"
          }
        },
        {
          "id": "360287970189639871",
          "label": "Paper",
          "properties": {
            "title": "WinoGrande: An Adversarial Winograd Schema Challenge at Scale"
          }
        },
        {
          "id": "360287970189639882",
          "label": "Paper",
          "properties": {
            "title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations"
          }
        },
        {
          "id": "360287970189639889",
          "label": "Paper",
          "properties": {
            "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
          }
        },
        {
          "id": "360287970189639745",
          "label": "Paper",
          "properties": {
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
          }
        },
        {
          "id": "360287970189639714",
          "label": "Paper",
          "properties": {
            "title": "Quantifying Memorization Across Neural Language Models"
          }
        },
        {
          "id": "360287970189639905",
          "label": "Paper",
          "properties": {
            "title": "Crosslingual Generalization through Multitask Finetuning"
          }
        },
        {
          "id": "360287970189639907",
          "label": "Paper",
          "properties": {
            "title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models"
          }
        }
      ]
    },
    {
      "category_id": 6,
      "name": "Evaluation and Benchmarking",
      "description": "Papers that address challenges related to evaluating model performance, benchmarking methodologies, and the reliability of evaluation metrics.",
      "children": [
        {
          "id": "360287970189639687",
          "label": "Paper",
          "properties": {
            "title": "Language Models are Multilingual Chain-of-Thought Reasoners"
          }
        },
        {
          "id": "360287970189639699",
          "label": "Paper",
          "properties": {
            "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"
          }
        },
        {
          "id": "360287970189639704",
          "label": "Paper",
          "properties": {
            "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"
          }
        },
        {
          "id": "360287970189639709",
          "label": "Paper",
          "properties": {
            "title": "GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers"
          }
        },
        {
          "id": "360287970189639723",
          "label": "Paper",
          "properties": {
            "title": "Jailbreaking Black Box Large Language Models in Twenty Queries"
          }
        },
        {
          "id": "360287970189639728",
          "label": "Paper",
          "properties": {
            "title": "Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies"
          }
        },
        {
          "id": "360287970189639734",
          "label": "Paper",
          "properties": {
            "title": "QuAC : Question Answering in Context"
          }
        },
        {
          "id": "360287970189639741",
          "label": "Paper",
          "properties": {
            "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference"
          }
        },
        {
          "id": "360287970189639752",
          "label": "Paper",
          "properties": {
            "title": "Towards VQA Models That Can Read"
          }
        },
        {
          "id": "360287970189639753",
          "label": "Paper",
          "properties": {
            "title": "Large Language Models Are Not Robust Multiple Choice Selectors"
          }
        },
        {
          "id": "360287970189639757",
          "label": "Paper",
          "properties": {
            "title": "Quantifying Variance in Evaluation Benchmarks"
          }
        },
        {
          "id": "360287970189639768",
          "label": "Paper",
          "properties": {
            "title": "A Diagram Is Worth A Dozen Images"
          }
        },
        {
          "id": "360287970189639705",
          "label": "Paper",
          "properties": {
            "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
          }
        },
        {
          "id": "360287970189639781",
          "label": "Paper",
          "properties": {
            "title": "Billion-scale similarity search with GPUs"
          }
        },
        {
          "id": "360287970189639783",
          "label": "Paper",
          "properties": {
            "title": "When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards"
          }
        },
        {
          "id": "360287970189639800",
          "label": "Paper",
          "properties": {
            "title": "Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale"
          }
        },
        {
          "id": "360287970189639735",
          "label": "Paper",
          "properties": {
            "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning"
          }
        },
        {
          "id": "360287970189639809",
          "label": "Paper",
          "properties": {
            "title": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models"
          }
        },
        {
          "id": "360287970189639810",
          "label": "Paper",
          "properties": {
            "title": "VQA: Visual Question Answering"
          }
        },
        {
          "id": "360287970189639811",
          "label": "Paper",
          "properties": {
            "title": "Learning From Mistakes Makes LLM Better Reasoner"
          }
        },
        {
          "id": "360287970189639817",
          "label": "Paper",
          "properties": {
            "title": "Changing Answer Order Can Decrease MMLU Accuracy"
          }
        },
        {
          "id": "360287970189639799",
          "label": "Paper",
          "properties": {
            "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"
          }
        },
        {
          "id": "360287970189639844",
          "label": "Paper",
          "properties": {
            "title": "Emergent Abilities of Large Language Models"
          }
        },
        {
          "id": "360287970189639846",
          "label": "Paper",
          "properties": {
            "title": "Dynabench: Rethinking Benchmarking in NLP"
          }
        },
        {
          "id": "360287970189639849",
          "label": "Paper",
          "properties": {
            "title": "Illuminating search spaces by mapping elites"
          }
        },
        {
          "id": "360287970189639880",
          "label": "Paper",
          "properties": {
            "title": "Leveraging Large Language Models for Multiple Choice Question Answering"
          }
        },
        {
          "id": "360287970189639893",
          "label": "Paper",
          "properties": {
            "title": "Adversarial Examples for Evaluating Reading Comprehension Systems"
          }
        },
        {
          "id": "360287970189639818",
          "label": "Paper",
          "properties": {
            "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model"
          }
        },
        {
          "id": "360287970189639900",
          "label": "Paper",
          "properties": {
            "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations"
          }
        },
        {
          "id": "360287970189639710",
          "label": "Paper",
          "properties": {
            "title": "Training language models to follow instructions with human feedback"
          }
        },
        {
          "id": "360287970189639763",
          "label": "Paper",
          "properties": {
            "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models"
          }
        },
        {
          "id": "360287970189639759",
          "label": "Paper",
          "properties": {
            "title": "GAIA: a benchmark for General AI Assistants"
          }
        }
      ]
    }
  ],
  "summary": "",
  "explain": []
}
