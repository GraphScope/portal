[
  {
    "category_id": 1,
    "name": "Language Model Improvements",
    "description": "Papers focusing on advancements in large language models, including fine-tuning, instruction tuning, and alignment with human intent.",
    "children": [
      {
        "id": "360287970189639681",
        "label": "Paper",
        "properties": {
          "title": "Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone",
          "authors": "['Marah Abdin', 'Jyoti Aneja', 'Hany Awadalla', 'Ahmed Awadallah', 'Ammar Ahmad Awan', 'Nguyen Bach', 'Amit Bahree', 'Arash Bakhtiari', 'Jianmin Bao', 'Harkirat Behl', 'Alon Benhaim', 'Misha Bilenko', 'Johan Bjorck', 'Sébastien Bubeck', 'Martin Cai', 'Qin Cai', 'Vishrav Chaudhary', 'Dong Chen', 'Dongdong Chen', 'Weizhu Chen', 'Yen-Chun Chen', 'Yi-Ling Chen', 'Hao Cheng', 'Parul Chopra', 'Xiyang Dai', 'Matthew Dixon', 'Ronen Eldan', 'Victor Fragoso', 'Jianfeng Gao', 'Mei Gao', 'Min Gao', 'Amit Garg', 'Allie Del Giorno', 'Abhishek Goswami', 'Suriya Gunasekar', 'Emman Haider', 'Junheng Hao', 'Russell J Hewett', 'Wenxiang Hu', 'Jamie Huynh', 'Dan Iter', 'Sam Ade Jacobs', 'Mojan Javaheripi', 'Xin Jin', 'Nikos Karampatziakis', 'Piero Kauffmann', 'Mahoud Khademi', 'Dongwoo Kim', 'Young Jin Kim', 'Lev Kurilenko', 'James R Lee', 'Yin Tat Lee', 'Yuanzhi Li', 'Yunsheng Li', 'Chen Liang', 'Lars Liden', 'Xihui Lin', 'Zeqi Lin', 'Ce Liu', 'Liyuan Liu', 'Mengchen Liu', 'Weishung Liu', 'Xiaodong Liu', 'Chong Luo', 'Piyush Madan', 'Ali Mahmoudzadeh', 'David Majercak', 'Matt Mazzola', 'Caio César Teodoro Mendes', 'Arindam Mitra', 'Hardik Modi', 'Anh Nguyen', 'Brandon Norick', 'Barun Patra', 'Daniel Perez-Becker', 'Thomas Portet', 'Reid Pryzant', 'Heyang Qin', 'Marko Radmilac', 'Liliang Ren', 'Gustavo de Rosa', 'Corby Rosset', 'Sambudha Roy', 'Olatunji Ruwase', 'Olli Saarikivi', 'Amin Saied', 'Adil Salim', 'Michael Santacroce', 'Shital Shah', 'Ning Shang', 'Hiteshi Sharma', 'Yelong Shen', 'Swadheen Shukla', 'Xia Song', 'Masahiro Tanaka', 'Andrea Tupini', 'Praneetha Vaddamanu', 'Chunyu Wang', 'Guanhua Wang', 'Lijuan Wang', 'Shuohang Wang', 'Xin Wang', 'Yu Wang', 'Rachel Ward', 'Wen Wen', 'Philipp Witte', 'Haiping Wu', 'Xiaoxia Wu', 'Michael Wyatt', 'Bin Xiao', 'Can Xu', 'Jiahang Xu', 'Weijian Xu', 'Jilong Xue', 'Sonali Yadav', 'Fan Yang', 'Jianwei Yang', 'Yifan Yang', 'Ziyi Yang', 'Donghan Yu', 'Lu Yuan', 'Chenruidong Zhang', 'Cyril Zhang', 'Jianwen Zhang', 'Li Lyna Zhang', 'Yi Zhang', 'Yue Zhang', 'Yunan Zhang', 'Xiren Zhou']",
          "year": "2024",
          "summary": "We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.",
          "primary_category": "cs.CL",
          "Topic": "Compact, powerful language modeling."
        }
      },
      {
        "id": "360287970189639682",
        "label": "Paper",
        "properties": {
          "title": "Averaging Weights Leads to Wider Optima and Better Generalization",
          "authors": "['Pavel Izmailov', 'Dmitrii Podoprikhin', 'Timur Garipov', 'Dmitry Vetrov', 'Andrew Gordon Wilson']",
          "year": "2018",
          "summary": "Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.",
          "primary_category": "cs.LG",
          "Topic": "Averaging weights for improved generalization."
        }
      },
      {
        "id": "360287970189639689",
        "label": "Paper",
        "properties": {
          "title": "WebGPT: Browser-assisted question-answering with human feedback",
          "authors": "['Reiichiro Nakano', 'Jacob Hilton', 'Suchir Balaji', 'Jeff Wu', 'Long Ouyang', 'Christina Kim', 'Christopher Hesse', 'Shantanu Jain', 'Vineet Kosaraju', 'William Saunders', 'Xu Jiang', 'Karl Cobbe', 'Tyna Eloundou', 'Gretchen Krueger', 'Kevin Button', 'Matthew Knight', 'Benjamin Chess', 'John Schulman']",
          "year": "2021",
          "summary": "We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.",
          "primary_category": "cs.CL",
          "Topic": "Browser-assisted long-form QA"
        }
      },
      {
        "id": "360287970189639695",
        "label": "Paper",
        "properties": {
          "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
          "authors": "['Patrick Esser', 'Sumith Kulal', 'Andreas Blattmann', 'Rahim Entezari', 'Jonas Müller', 'Harry Saini', 'Yam Levi', 'Dominik Lorenz', 'Axel Sauer', 'Frederic Boesel', 'Dustin Podell', 'Tim Dockhorn', 'Zion English', 'Kyle Lacey', 'Alex Goodwin', 'Yannik Marek', 'Robin Rombach']",
          "year": "2024",
          "summary": "Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available.",
          "primary_category": "cs.CV",
          "Topic": "High-resolution text-to-image synthesis"
        }
      },
      {
        "id": "360287970189639711",
        "label": "Paper",
        "properties": {
          "title": "Does your data spark joy? Performance gains from domain upsampling at the end of training",
          "authors": "['Cody Blakeney', 'Mansheej Paul', 'Brett W Larsen', 'Sean Owen', 'Jonathan Frankle']",
          "year": "2024",
          "summary": "Pretraining datasets for large language models (LLMs) have grown to trillions of tokens composed of large amounts of CommonCrawl (CC) web scrape along with smaller, domain-specific datasets. It is expensive to understand the impact of these domain-specific datasets on model capabilities as training at large FLOP scales is required to reveal significant changes to difficult and emergent benchmarks. Given the increasing cost of experimenting with pretraining data, how does one determine the optimal balance between the diversity in general web scrapes and the information density of domain specific data? In this work, we show how to leverage the smaller domain specific datasets by upsampling them relative to CC at the end of training to drive performance improvements on difficult benchmarks. This simple technique allows us to improve up to 6.90 pp on MMLU, 8.26 pp on GSM8K, and 6.17 pp on HumanEval relative to the base data mix for a 7B model trained for 1 trillion (T) tokens, thus rivaling Llama-2 (7B)$\\unicode{x2014}$a model trained for twice as long. We experiment with ablating the duration of domain upsampling from 5% to 30% of training and find that 10% to 20% percent is optimal for navigating the tradeoff between general language modeling capabilities and targeted benchmarks. We also use domain upsampling to characterize at scale the utility of individual datasets for improving various benchmarks by removing them during this final phase of training. This tool opens up the ability to experiment with the impact of different pretraining datasets at scale, but at an order of magnitude lower cost compared to full pretraining runs.",
          "primary_category": "cs.LG",
          "Topic": "Domain-specific data upsampling"
        }
      },
      {
        "id": "360287970189639718",
        "label": "Paper",
        "properties": {
          "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation",
          "authors": "['Li Dong', 'Nan Yang', 'Wenhui Wang', 'Furu Wei', 'Xiaodong Liu', 'Yu Wang', 'Jianfeng Gao', 'Ming Zhou', 'Hsiao-Wuen Hon']",
          "year": "2019",
          "summary": "This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https://github.com/microsoft/unilm.",
          "primary_category": "cs.CL",
          "Topic": "Unified Language Model Pre-training"
        }
      },
      {
        "id": "360287970189639721",
        "label": "Paper",
        "properties": {
          "title": "TOOLVERIFIER: Generalization to New Tools via Self-Verification",
          "authors": "['Dheeraj Mekala', 'Jason Weston', 'Jack Lanchantin', 'Roberta Raileanu', 'Maria Lomeli', 'Jingbo Shang', 'Jane Dwivedi-Yu']",
          "year": "2024",
          "summary": "Teaching language models to use tools is an important milestone towards building general assistants, but remains an open problem. While there has been significant progress on learning to use specific tools via fine-tuning, language models still struggle with learning how to robustly use new tools from only a few demonstrations. In this work we introduce a self-verification method which distinguishes between close candidates by self-asking contrastive questions during (1) tool selection; and (2) parameter generation. We construct synthetic, high-quality, self-generated data for this goal using Llama-2 70B, which we intend to release publicly. Extensive experiments on 4 tasks from the ToolBench benchmark, consisting of 17 unseen tools, demonstrate an average improvement of 22% over few-shot baselines, even in scenarios where the distinctions between candidate tools are finely nuanced.",
          "primary_category": "cs.CL",
          "Topic": "Tool generalization and verification"
        }
      },
      {
        "id": "360287970189639727",
        "label": "Paper",
        "properties": {
          "title": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel",
          "authors": "['Yanli Zhao', 'Andrew Gu', 'Rohan Varma', 'Liang Luo', 'Chien-Chin Huang', 'Min Xu', 'Less Wright', 'Hamid Shojanazeri', 'Myle Ott', 'Sam Shleifer', 'Alban Desmaison', 'Can Balioglu', 'Pritam Damania', 'Bernard Nguyen', 'Geeta Chauhan', 'Yuchen Hao', 'Ajit Mathews', 'Shen Li']",
          "year": "2023",
          "summary": "It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The experimental results demonstrate that FSDP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of TFLOPS.",
          "primary_category": "cs.DC",
          "Topic": "Scaling large model training"
        }
      },
      {
        "id": "360287970189639728",
        "label": "Paper",
        "properties": {
          "title": "Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies",
          "authors": "['Liangming Pan', 'Michael Saxon', 'Wenda Xu', 'Deepak Nathani', 'Xinyi Wang', 'William Yang Wang']",
          "year": "2023",
          "summary": "Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.",
          "primary_category": "cs.CL",
          "Topic": "Automated LLM Self-Correction"
        }
      },
      {
        "id": "360287970189639716",
        "label": "Paper",
        "properties": {
          "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge",
          "authors": "['Alon Talmor', 'Jonathan Herzig', 'Nicholas Lourie', 'Jonathan Berant']",
          "year": "2018",
          "summary": "When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy, well below human performance, which is 89%.",
          "primary_category": "cs.CL",
          "Topic": "commonsense question answering"
        }
      },
      {
        "id": "360287970189639732",
        "label": "Paper",
        "properties": {
          "title": "Learning Transferable Visual Models From Natural Language Supervision",
          "authors": "['Alec Radford', 'Jong Wook Kim', 'Chris Hallacy', 'Aditya Ramesh', 'Gabriel Goh', 'Sandhini Agarwal', 'Girish Sastry', 'Amanda Askell', 'Pamela Mishkin', 'Jack Clark', 'Gretchen Krueger', 'Ilya Sutskever']",
          "year": "2021",
          "summary": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
          "primary_category": "cs.CV",
          "Topic": "Contrastive Language Image Pre-training"
        }
      },
      {
        "id": "360287970189639734",
        "label": "Paper",
        "properties": {
          "title": "QuAC : Question Answering in Context",
          "authors": "['Eunsol Choi', 'He He', 'Mohit Iyyer', 'Mark Yatskar', 'Wen-tau Yih', 'Yejin Choi', 'Percy Liang', 'Luke Zettlemoyer']",
          "year": "2018",
          "summary": "We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at http://quac.ai.",
          "primary_category": "cs.CL",
          "Topic": "Contextual Question Answering Dialogs"
        }
      },
      {
        "id": "360287970189639741",
        "label": "Paper",
        "properties": {
          "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
          "authors": "['Wei-Lin Chiang', 'Lianmin Zheng', 'Ying Sheng', 'Anastasios Nikolas Angelopoulos', 'Tianle Li', 'Dacheng Li', 'Hao Zhang', 'Banghua Zhu', 'Michael Jordan', 'Joseph E Gonzalez', 'Ion Stoica']",
          "year": "2024",
          "summary": "Large Language Models (LLMs) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating LLMs based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. Our demo is publicly available at \\url{https://chat.lmsys.org}.",
          "primary_category": "cs.AI",
          "Topic": "Crowdsourced LLM Evaluation Platform"
        }
      },
      {
        "id": "360287970189639758",
        "label": "Paper",
        "properties": {
          "title": "A Survey of Reinforcement Learning from Human Feedback",
          "authors": "['Timo Kaufmann', 'Paul Weng', 'Viktor Bengs', 'Eyke Hüllermeier']",
          "year": "2023",
          "summary": "Reinforcement learning from human feedback (RLHF) is a variant of reinforcement learning (RL) that learns from human feedback instead of relying on an engineered reward function. Building on prior work on the related setting of preference-based reinforcement learning (PbRL), it stands at the intersection of artificial intelligence and human-computer interaction. This positioning offers a promising avenue to enhance the performance and adaptability of intelligent systems while also improving the alignment of their objectives with human values. The training of large language models (LLMs) has impressively demonstrated this potential in recent years, where RLHF played a decisive role in directing the model's capabilities toward human objectives. This article provides a comprehensive overview of the fundamentals of RLHF, exploring the intricate dynamics between RL agents and human input. While recent focus has been on RLHF for LLMs, our survey adopts a broader perspective, examining the diverse applications and wide-ranging impact of the technique. We delve into the core principles that underpin RLHF, shedding light on the symbiotic relationship between algorithms and human feedback, and discuss the main research trends in the field. By synthesizing the current landscape of RLHF research, this article aims to provide researchers as well as practitioners with a comprehensive understanding of this rapidly growing field of research.",
          "primary_category": "cs.LG",
          "Topic": "Reinforcement Learning with Human Feedback"
        }
      },
      {
        "id": "360287970189639760",
        "label": "Paper",
        "properties": {
          "title": "MAmmoTH2: Scaling Instructions from the Web",
          "authors": "['Xiang Yue', 'Tuney Zheng', 'Ge Zhang', 'Wenhu Chen']",
          "year": "2024",
          "summary": "Instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors. Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation. We propose a paradigm to efficiently harvest 10 million naturally existing instruction data from the pre-training web corpus to enhance LLM reasoning. Our approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on this dataset, we build MAmmoTH2 models, which significantly boost performance on reasoning benchmarks. Notably, MAmmoTH2-7B's (Mistral) performance increases from 11% to 36.7% on MATH and from 36% to 68.4% on GSM8K without training on any in-domain data. Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks. Our work demonstrates how to harvest large-scale, high-quality instruction data without costly human annotation or GPT-4 distillation, providing a new paradigm for building better instruction tuning data.",
          "primary_category": "cs.CL",
          "Topic": "Enhancing LLM Reasoning Abilities"
        }
      },
      {
        "id": "360287970189639761",
        "label": "Paper",
        "properties": {
          "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
          "authors": "['Yinhan Liu', 'Myle Ott', 'Naman Goyal', 'Jingfei Du', 'Mandar Joshi', 'Danqi Chen', 'Omer Levy', 'Mike Lewis', 'Luke Zettlemoyer', 'Veselin Stoyanov']",
          "year": "2019",
          "summary": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",
          "primary_category": "cs.CL",
          "Topic": "Robust BERT Pretraining Optimization"
        }
      },
      {
        "id": "360287970189639680",
        "label": "Paper",
        "properties": {
          "title": "Bag of Tricks for Efficient Text Classification",
          "authors": "['Armand Joulin', 'Edouard Grave', 'Piotr Bojanowski', 'Tomas Mikolov']",
          "year": "2016",
          "summary": "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute.",
          "primary_category": "cs.CL",
          "Topic": "Efficient Text Classification Methods"
        }
      },
      {
        "id": "360287970189639766",
        "label": "Paper",
        "properties": {
          "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
          "authors": "['Stella Biderman', 'Hailey Schoelkopf', 'Quentin Anthony', 'Herbie Bradley', \"Kyle O'Brien\"\", 'Eric Hallahan', 'Mohammad Aflah Khan', 'Shivanshu Purohit', 'USVSN Sai Prashanth', 'Edward Raff', 'Aviya Skowron', 'Lintang Sutawika', 'Oskar van der Wal']\"",
          "year": "2023",
          "summary": "How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \\url{https://github.com/EleutherAI/pythia}.",
          "primary_category": "cs.CL",
          "Topic": "Training Dynamics of Large Language Models"
        }
      },
      {
        "id": "360287970189639769",
        "label": "Paper",
        "properties": {
          "title": "Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding",
          "authors": "['Kenton Lee', 'Mandar Joshi', 'Iulia Turc', 'Hexiang Hu', 'Fangyu Liu', 'Julian Eisenschlos', 'Urvashi Khandelwal', 'Peter Shaw', 'Ming-Wei Chang', 'Kristina Toutanova']",
          "year": "2022",
          "summary": "Visually-situated language is ubiquitous -- sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and forms. Perhaps due to this diversity, previous work has typically relied on domain-specific recipes with limited sharing of the underlying data, model architectures, and objectives. We present Pix2Struct, a pretrained image-to-text model for purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions are rendered directly on top of the input image. For the first time, we show that a single pretrained model can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.",
          "primary_category": "cs.CL",
          "Topic": "Visually Situated Language Understanding"
        }
      },
      {
        "id": "360287970189639770",
        "label": "Paper",
        "properties": {
          "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
          "authors": "['Joshua Ainslie', 'James Lee-Thorp', 'Michiel de Jong', 'Yury Zemlyanskiy', 'Federico Lebrón', 'Sumit Sanghai']",
          "year": "2023",
          "summary": "Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.",
          "primary_category": "cs.CL",
          "Topic": "Optimizing Multi Query Attention Efficiency"
        }
      },
      {
        "id": "360287970189639777",
        "label": "Paper",
        "properties": {
          "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
          "authors": "['Jianlin Su', 'Yu Lu', 'Shengfeng Pan', 'Ahmed Murtadha', 'Bo Wen', 'Yunfeng Liu']",
          "year": "2021",
          "summary": "Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: \\url{https://huggingface.co/docs/transformers/model_doc/roformer}.",
          "primary_category": "cs.CL",
          "Topic": "Rotary Position Embedding in Transformers"
        }
      },
      {
        "id": "360287970189639707",
        "label": "Paper",
        "properties": {
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "authors": "['Jacob Devlin', 'Ming-Wei Chang', 'Kenton Lee', 'Kristina Toutanova']",
          "year": "2018",
          "summary": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.   BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
          "primary_category": "cs.CL",
          "Topic": "Bidirectional Transformer Language Model"
        }
      },
      {
        "id": "360287970189639779",
        "label": "Paper",
        "properties": {
          "title": "Extracting Training Data from Diffusion Models",
          "authors": "['Nicholas Carlini', 'Jamie Hayes', 'Milad Nasr', 'Matthew Jagielski', 'Vikash Sehwag', 'Florian Tramèr', 'Borja Balle', 'Daphne Ippolito', 'Eric Wallace']",
          "year": "2023",
          "summary": "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.",
          "primary_category": "cs.CR",
          "Topic": "Diffusion model privacy vulnerabilities"
        }
      },
      {
        "id": "360287970189639782",
        "label": "Paper",
        "properties": {
          "title": "OpenELM: An Efficient Language Model Family with Open Training and Inference Framework",
          "authors": "['Sachin Mehta', 'Mohammad Hossein Sekhavat', 'Qingqing Cao', 'Maxwell Horton', 'Yanzi Jin', 'Chenfan Sun', 'Iman Mirzadeh', 'Mahyar Najibi', 'Dmitry Belenko', 'Peter Zatloukal', 'Mohammad Rastegari']",
          "year": "2024",
          "summary": "The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring $2\\times$ fewer pre-training tokens.   Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors.   Our source code along with pre-trained model weights and training recipes is available at \\url{https://github.com/apple/corenet}. Additionally, \\model models can be found on HuggingFace at: \\url{https://huggingface.co/apple/OpenELM}.",
          "primary_category": "cs.CL",
          "Topic": "Efficient Language Model Scaling"
        }
      },
      {
        "id": "360287970189639785",
        "label": "Paper",
        "properties": {
          "title": "Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models",
          "authors": "['Margaret Li', 'Suchin Gururangan', 'Tim Dettmers', 'Mike Lewis', 'Tim Althoff', 'Noah A Smith', 'Luke Zettlemoyer']",
          "year": "2022",
          "summary": "We present Branch-Train-Merge (BTM), a communication-efficient algorithm for embarrassingly parallel training of large language models (LLMs). We show it is possible to independently train subparts of a new class of LLMs on different subsets of the data, eliminating the massive multi-node synchronization currently required to train LLMs. BTM learns a set of independent expert LMs (ELMs), each specialized to a different textual domain, such as scientific or legal text. These ELMs can be added and removed to update data coverage, ensembled to generalize to new domains, or averaged to collapse back to a single LM for efficient inference. New ELMs are learned by branching from (mixtures of) ELMs in the current set, further training the parameters on data for the new domain, and then merging the resulting model back into the set for future use. Experiments show that BTM improves in- and out-of-domain perplexities as compared to GPT-style Transformer LMs, when controlling for training cost. Through extensive analysis, we show that these results are robust to different ELM initialization schemes, but require expert domain specialization; LM ensembles with random data splits do not perform well. We also present a study of scaling BTM into a new corpus of 64 domains (192B whitespace-separated tokens in total); the resulting LM (22.4B total parameters) performs as well as a Transformer LM trained with 2.5 times more compute. These gains grow with the number of domains, suggesting more aggressive parallelism could be used to efficiently train larger models in future work.",
          "primary_category": "cs.CL",
          "Topic": "Embarrassingly Parallel Language Modeling"
        }
      },
      {
        "id": "360287970189639786",
        "label": "Paper",
        "properties": {
          "title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive",
          "authors": "['Arka Pal', 'Deep Karkhanis', 'Samuel Dooley', 'Manley Roberts', 'Siddartha Naidu', 'Colin White']",
          "year": "2024",
          "summary": "Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the relative probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a reduction of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we find that DPOP outperforms DPO and other fine-tuning procedures across a wide variety of datasets and downstream tasks, including datasets with high edit distances between completions. Furthermore, we find that the DPOP-tuned model outperforms the DPO-tuned model (all else equal) on benchmarks independent of the fine-tuning data, such as MT-Bench. Finally, using DPOP, we create and open-source Smaug-34B and Smaug-72B, with the latter becoming the first open-source LLM to surpass an average accuracy of 80% on the HuggingFace Open LLM Leaderboard.",
          "primary_category": "cs.CL",
          "Topic": "Preference optimisation in LLMs"
        }
      },
      {
        "id": "360287970189639801",
        "label": "Paper",
        "properties": {
          "title": "The Falcon Series of Open Language Models",
          "authors": "['Ebtesam Almazrouei', 'Hamza Alobeidli', 'Abdulaziz Alshamsi', 'Alessandro Cappelli', 'Ruxandra Cojocaru', 'Mérouane Debbah', 'Étienne Goffinet', 'Daniel Hesslow', 'Julien Launay', 'Quentin Malartic', 'Daniele Mazzotta', 'Badreddine Noune', 'Baptiste Pannier', 'Guilherme Penedo']",
          "year": "2023",
          "summary": "We introduce the Falcon series: 7B, 40B, and 180B parameters causal decoder-only models trained on a diverse high-quality corpora predominantly assembled from web data. The largest model, Falcon-180B, has been trained on over 3.5 trillion tokens of text--the largest openly documented pretraining run. Falcon-180B significantly outperforms models such as PaLM or Chinchilla, and improves upon concurrently developed models such as LLaMA 2 or Inflection-1. It nears the performance of PaLM-2-Large at a reduced pretraining and inference cost, making it, to our knowledge, one of the three best language models in the world along with GPT-4 and PaLM-2-Large. We report detailed evaluations, as well as a deep dive into the methods and custom tooling employed to pretrain Falcon. Notably, we report on our custom distributed training codebase, allowing us to efficiently pretrain these models on up to 4,096 A100s on cloud AWS infrastructure with limited interconnect. We release a 600B tokens extract of our web dataset, as well as the Falcon-7/40/180B models under a permissive license to foster open-science and accelerate the development of an open ecosystem of large language models.",
          "primary_category": "cs.CL",
          "Topic": "Large language model scaling and performance."
        }
      },
      {
        "id": "360287970189639735",
        "label": "Paper",
        "properties": {
          "title": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning",
          "authors": "['Xiang Yue', 'Xingwei Qu', 'Ge Zhang', 'Yao Fu', 'Wenhao Huang', 'Huan Sun', 'Yu Su', 'Wenhu Chen']",
          "year": "2023",
          "summary": "We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16% and 32%. Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 23%, and the MAmmoTH-34B model achieves 44% accuracy on MATH, even surpassing GPT-4's CoT result. Our work underscores the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models.",
          "primary_category": "cs.CL",
          "Topic": "Hybrid Math Reasoning Models"
        }
      },
      {
        "id": "360287970189639812",
        "label": "Paper",
        "properties": {
          "title": "GPT-4 Technical Report",
          "authors": "['OpenAI', 'Josh Achiam', 'Steven Adler', 'Sandhini Agarwal', 'Lama Ahmad', 'Ilge Akkaya', 'Florencia Leoni Aleman', 'Diogo Almeida', 'Janko Altenschmidt', 'Sam Altman', 'Shyamal Anadkat', 'Red Avila', 'Igor Babuschkin', 'Suchir Balaji', 'Valerie Balcom', 'Paul Baltescu', 'Haiming Bao', 'Mohammad Bavarian', 'Jeff Belgum', 'Irwan Bello', 'Jake Berdine', 'Gabriel Bernadett-Shapiro', 'Christopher Berner', 'Lenny Bogdonoff', 'Oleg Boiko', 'Madelaine Boyd', 'Anna-Luisa Brakman', 'Greg Brockman', 'Tim Brooks', 'Miles Brundage', 'Kevin Button', 'Trevor Cai', 'Rosie Campbell', 'Andrew Cann', 'Brittany Carey', 'Chelsea Carlson', 'Rory Carmichael', 'Brooke Chan', 'Che Chang', 'Fotis Chantzis', 'Derek Chen', 'Sully Chen', 'Ruby Chen', 'Jason Chen', 'Mark Chen', 'Ben Chess', 'Chester Cho', 'Casey Chu', 'Hyung Won Chung', 'Dave Cummings', 'Jeremiah Currier', 'Yunxing Dai', 'Cory Decareaux', 'Thomas Degry', 'Noah Deutsch', 'Damien Deville', 'Arka Dhar', 'David Dohan', 'Steve Dowling', 'Sheila Dunning', 'Adrien Ecoffet', 'Atty Eleti', 'Tyna Eloundou', 'David Farhi', 'Liam Fedus', 'Niko Felix', 'Simón Posada Fishman', 'Juston Forte', 'Isabella Fulford', 'Leo Gao', 'Elie Georges', 'Christian Gibson', 'Vik Goel', 'Tarun Gogineni', 'Gabriel Goh', 'Rapha Gontijo-Lopes', 'Jonathan Gordon', 'Morgan Grafstein', 'Scott Gray', 'Ryan Greene', 'Joshua Gross', 'Shixiang Shane Gu', 'Yufei Guo', 'Chris Hallacy', 'Jesse Han', 'Jeff Harris', 'Yuchen He', 'Mike Heaton', 'Johannes Heidecke', 'Chris Hesse', 'Alan Hickey', 'Wade Hickey', 'Peter Hoeschele', 'Brandon Houghton', 'Kenny Hsu', 'Shengli Hu', 'Xin Hu', 'Joost Huizinga', 'Shantanu Jain', 'Shawn Jain', 'Joanne Jang', 'Angela Jiang', 'Roger Jiang', 'Haozhun Jin', 'Denny Jin', 'Shino Jomoto', 'Billie Jonn', 'Heewoo Jun', 'Tomer Kaftan', 'Łukasz Kaiser', 'Ali Kamali', 'Ingmar Kanitscheider', 'Nitish Shirish Keskar', 'Tabarak Khan', 'Logan Kilpatrick', 'Jong Wook Kim', 'Christina Kim', 'Yongjik Kim', 'Jan Hendrik Kirchner', 'Jamie Kiros', 'Matt Knight', 'Daniel Kokotajlo', 'Łukasz Kondraciuk', 'Andrew Kondrich', 'Aris Konstantinidis', 'Kyle Kosic', 'Gretchen Krueger', 'Vishal Kuo', 'Michael Lampe', 'Ikai Lan', 'Teddy Lee', 'Jan Leike', 'Jade Leung', 'Daniel Levy', 'Chak Ming Li', 'Rachel Lim', 'Molly Lin', 'Stephanie Lin', 'Mateusz Litwin', 'Theresa Lopez', 'Ryan Lowe', 'Patricia Lue', 'Anna Makanju', 'Kim Malfacini', 'Sam Manning', 'Todor Markov', 'Yaniv Markovski', 'Bianca Martin', 'Katie Mayer', 'Andrew Mayne', 'Bob McGrew', 'Scott Mayer McKinney', 'Christine McLeavey', 'Paul McMillan', 'Jake McNeil', 'David Medina', 'Aalok Mehta', 'Jacob Menick', 'Luke Metz', 'Andrey Mishchenko', 'Pamela Mishkin', 'Vinnie Monaco', 'Evan Morikawa', 'Daniel Mossing', 'Tong Mu', 'Mira Murati', 'Oleg Murk', 'David Mély', 'Ashvin Nair', 'Reiichiro Nakano', 'Rajeev Nayak', 'Arvind Neelakantan', 'Richard Ngo', 'Hyeonwoo Noh', 'Long Ouyang', \"Cullen O'Keefe\"\", 'Jakub Pachocki', 'Alex Paino', 'Joe Palermo', 'Ashley Pantuliano', 'Giambattista Parascandolo', 'Joel Parish', 'Emy Parparita', 'Alex Passos', 'Mikhail Pavlov', 'Andrew Peng', 'Adam Perelman', 'Filipe de Avila Belbute Peres', 'Michael Petrov', 'Henrique Ponde de Oliveira Pinto', 'Michael', 'Pokorny', 'Michelle Pokrass', 'Vitchyr H Pong', 'Tolly Powell', 'Alethea Power', 'Boris Power', 'Elizabeth Proehl', 'Raul Puri', 'Alec Radford', 'Jack Rae', 'Aditya Ramesh', 'Cameron Raymond', 'Francis Real', 'Kendra Rimbach', 'Carl Ross', 'Bob Rotsted', 'Henri Roussez', 'Nick Ryder', 'Mario Saltarelli', 'Ted Sanders', 'Shibani Santurkar', 'Girish Sastry', 'Heather Schmidt', 'David Schnurr', 'John Schulman', 'Daniel Selsam', 'Kyla Sheppard', 'Toki Sherbakov', 'Jessica Shieh', 'Sarah Shoker', 'Pranav Shyam', 'Szymon Sidor', 'Eric Sigler', 'Maddie Simens', 'Jordan Sitkin', 'Katarina Slama', 'Ian Sohl', 'Benjamin Sokolowsky', 'Yang Song', 'Natalie Staudacher', 'Felipe Petroski Such', 'Natalie Summers', 'Ilya Sutskever', 'Jie Tang', 'Nikolas Tezak', 'Madeleine B Thompson', 'Phil Tillet', 'Amin Tootoonchian', 'Elizabeth Tseng', 'Preston Tuggle', 'Nick Turley', 'Jerry Tworek', 'Juan Felipe Cerón Uribe', 'Andrea Vallone', 'Arun Vijayvergiya', 'Chelsea Voss', 'Carroll Wainwright', 'Justin Jay Wang', 'Alvin Wang', 'Ben Wang', 'Jonathan Ward', 'Jason Wei', 'CJ Weinmann', 'Akila Welihinda', 'Peter Welinder', 'Jiayi Weng', 'Lilian Weng', 'Matt Wiethoff', 'Dave Willner', 'Clemens Winter', 'Samuel Wolrich', 'Hannah Wong', 'Lauren Workman', 'Sherwin Wu', 'Jeff Wu', 'Michael Wu', 'Kai Xiao', 'Tao Xu', 'Sarah Yoo', 'Kevin Yu', 'Qiming Yuan', 'Wojciech Zaremba', 'Rowan Zellers', 'Chong Zhang', 'Marvin Zhang', 'Shengjia Zhao', 'Tianhao Zheng', 'Juntang Zhuang', 'William Zhuk', 'Barret Zoph']\"",
          "year": "2023",
          "summary": "We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",
          "primary_category": "cs.CL",
          "Topic": "Complex AI Risk Analysis"
        }
      },
      {
        "id": "360287970189639816",
        "label": "Paper",
        "properties": {
          "title": "Iterative Reasoning Preference Optimization",
          "authors": "['Richard Yuanzhe Pang', 'Weizhe Yuan', 'Kyunghyun Cho', 'He He', 'Sainbayar Sukhbaatar', 'Jason Weston']",
          "year": "2024",
          "summary": "Iterative preference optimization methods have recently been shown to perform well for general instruction tuning tasks, but typically make little improvement on reasoning tasks (Yuan et al., 2024, Chen et al., 2024). In this work we develop an iterative approach that optimizes the preference between competing generated Chain-of-Thought (CoT) candidates by optimizing for winning vs. losing reasoning steps that lead to the correct answer. We train using a modified DPO loss (Rafailov et al., 2023) with an additional negative log-likelihood term, which we find to be crucial. We show reasoning improves across repeated iterations of this scheme. While only relying on examples in the training set, our approach results in increasing accuracy on GSM8K, MATH, and ARC-Challenge for Llama-2-70B-Chat, outperforming other Llama-2-based models not relying on additionally sourced datasets. For example, we see a large improvement from 55.6% to 81.6% on GSM8K and an accuracy of 88.7% with majority voting out of 32 samples.",
          "primary_category": "cs.CL",
          "Topic": "Iterative Reasoning Preference Optimization"
        }
      },
      {
        "id": "360287970189639824",
        "label": "Paper",
        "properties": {
          "title": "LIMA: Less Is More for Alignment",
          "authors": "['Chunting Zhou', 'Pengfei Liu', 'Puxin Xu', 'Srini Iyer', 'Jiao Sun', 'Yuning Mao', 'Xuezhe Ma', 'Avia Efrat', 'Ping Yu', 'Lili Yu', 'Susan Zhang', 'Gargi Ghosh', 'Mike Lewis', 'Luke Zettlemoyer', 'Omer Levy']",
          "year": "2023",
          "summary": "Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.",
          "primary_category": "cs.CL",
          "Topic": "Alignment of Language Models"
        }
      },
      {
        "id": "360287970189639825",
        "label": "Paper",
        "properties": {
          "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks",
          "authors": "['Suchin Gururangan', 'Ana Marasović', 'Swabha Swayamdipta', 'Kyle Lo', 'Iz Beltagy', 'Doug Downey', 'Noah A Smith']",
          "year": "2020",
          "summary": "Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.",
          "primary_category": "cs.CL",
          "Topic": "Domain Adaptive Pretraining"
        }
      },
      {
        "id": "360287970189639746",
        "label": "Paper",
        "properties": {
          "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
          "authors": "['Mitchell Wortsman', 'Gabriel Ilharco', 'Samir Yitzhak Gadre', 'Rebecca Roelofs', 'Raphael Gontijo-Lopes', 'Ari S Morcos', 'Hongseok Namkoong', 'Ali Farhadi', 'Yair Carmon', 'Simon Kornblith', 'Ludwig Schmidt']",
          "year": "2022",
          "summary": "The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results \"model soups.\"\" When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.\"",
          "primary_category": "cs.LG",
          "Topic": "Averaging fine-tuned model weights"
        }
      },
      {
        "id": "360287970189639831",
        "label": "Paper",
        "properties": {
          "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
          "authors": "['Woosuk Kwon', 'Zhuohan Li', 'Siyuan Zhuang', 'Ying Sheng', 'Lianmin Zheng', 'Cody Hao Yu', 'Joseph E Gonzalez', 'Hao Zhang', 'Ion Stoica']",
          "year": "2023",
          "summary": "High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm",
          "primary_category": "cs.LG",
          "Topic": "Efficient KV Cache Management"
        }
      },
      {
        "id": "360287970189639755",
        "label": "Paper",
        "properties": {
          "title": "SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models",
          "authors": "['Guangxuan Xiao', 'Ji Lin', 'Mickael Seznec', 'Hao Wu', 'Julien Demouth', 'Song Han']",
          "year": "2022",
          "summary": "Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, Llama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at https://github.com/mit-han-lab/smoothquant.",
          "primary_category": "cs.CL",
          "Topic": "Post-training quantization for LLMs"
        }
      },
      {
        "id": "360287970189639691",
        "label": "Paper",
        "properties": {
          "title": "Exploring the Limits of Weakly Supervised Pretraining",
          "authors": "['Dhruv Mahajan', 'Ross Girshick', 'Vignesh Ramanathan', 'Kaiming He', 'Manohar Paluri', 'Yixuan Li', 'Ashwin Bharambe', 'Laurens van der Maaten']",
          "year": "2018",
          "summary": "State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards \"small\"\". Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4% (97.6% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.\"",
          "primary_category": "cs.CV",
          "Topic": "Weakly Supervised Pretraining"
        }
      },
      {
        "id": "360287970189639702",
        "label": "Paper",
        "properties": {
          "title": "Self-Refine: Iterative Refinement with Self-Feedback",
          "authors": "['Aman Madaan', 'Niket Tandon', 'Prakhar Gupta', 'Skyler Hallinan', 'Luyu Gao', 'Sarah Wiegreffe', 'Uri Alon', 'Nouha Dziri', 'Shrimai Prabhumoye', 'Yiming Yang', 'Shashank Gupta', 'Bodhisattwa Prasad Majumder', 'Katherine Hermann', 'Sean Welleck', 'Amir Yazdanbakhsh', 'Peter Clark']",
          "year": "2023",
          "summary": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.",
          "primary_category": "cs.CL",
          "Topic": "Iterative self-refinement in LLMs"
        }
      },
      {
        "id": "360287970189639837",
        "label": "Paper",
        "properties": {
          "title": "STaR: Bootstrapping Reasoning With Reasoning",
          "authors": "['Eric Zelikman', 'Yuhuai Wu', 'Jesse Mu', 'Noah D Goodman']",
          "year": "2022",
          "summary": "Generating step-by-step \"chain-of-thought\"\" rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the \"\"Self-Taught Reasoner\"\" (STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30$\\times$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.\"",
          "primary_category": "cs.LG",
          "Topic": "Iterative Rationale Generation"
        }
      },
      {
        "id": "360287970189639843",
        "label": "Paper",
        "properties": {
          "title": "Effective Long-Context Scaling of Foundation Models",
          "authors": "['Wenhan Xiong', 'Jingyu Liu', 'Igor Molybog', 'Hejia Zhang', 'Prajjwal Bhargava', 'Rui Hou', 'Louis Martin', 'Rashi Rungta', 'Karthik Abinav Sankararaman', 'Barlas Oguz', 'Madian Khabsa', 'Han Fang', 'Yashar Mehdad', 'Sharan Narang', 'Kshitiz Malik', 'Angela Fan', 'Shruti Bhosale', 'Sergey Edunov', 'Mike Lewis', 'Sinong Wang', 'Hao Ma']",
          "year": "2023",
          "summary": "We present a series of long-context LLMs that support effective context windows of up to 32,768 tokens. Our model series are built through continual pretraining from Llama 2 with longer training sequences and on a dataset where long texts are upsampled. We perform extensive evaluation on language modeling, synthetic context probing tasks, and a wide range of research benchmarks. On research benchmarks, our models achieve consistent improvements on most regular tasks and significant improvements on long-context tasks over Llama 2. Notably, with a cost-effective instruction tuning procedure that does not require human-annotated long instruction data, the 70B variant can already surpass gpt-3.5-turbo-16k's overall performance on a suite of long-context tasks. Alongside these results, we provide an in-depth analysis on the individual components of our method. We delve into Llama's position encodings and discuss its limitation in modeling long dependencies. We also examine the impact of various design choices in the pretraining process, including the data mix and the training curriculum of sequence lengths -- our ablation experiments suggest that having abundant long texts in the pretrain dataset is not the key to achieving strong performance, and we empirically verify that long context continual pretraining is more efficient and similarly effective compared to pretraining from scratch with long sequences.",
          "primary_category": "cs.CL",
          "Topic": "Long Context Language Modeling"
        }
      },
      {
        "id": "360287970189639845",
        "label": "Paper",
        "properties": {
          "title": "Reframing Instructional Prompts to GPTk's Language",
          "authors": "['Swaroop Mishra', 'Daniel Khashabi', 'Chitta Baral', 'Yejin Choi', 'Hannaneh Hajishirzi']",
          "year": "2021",
          "summary": "What kinds of instructional prompts are easier to follow for Language Models (LMs)? We study this question by conducting extensive empirical analysis that shed light on important features of successful instructional prompts. Specifically, we study several classes of reframing techniques for manual reformulation of prompts into more effective ones. Some examples include decomposing a complex task instruction into multiple simpler tasks or itemizing instructions into sequential steps. Our experiments compare the zero-shot and few-shot performance of LMs prompted with reframed instructions on 12 NLP tasks across 6 categories. Compared with original instructions, our reframed instructions lead to significant improvements across LMs with different sizes. For example, the same reframed prompts boost few-shot performance of GPT3-series and GPT2-series by 12.5% and 6.7% respectively averaged over all tasks. Furthermore, reframed instructions reduce the number of examples required to prompt LMs in the few-shot setting. We hope these empirically-driven techniques will pave the way towards more effective future prompting algorithms.",
          "primary_category": "cs.CL",
          "Topic": "Reframing instructional prompts"
        }
      },
      {
        "id": "360287970189639772",
        "label": "Paper",
        "properties": {
          "title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation",
          "authors": "['Jiawei Liu', 'Chunqiu Steven Xia', 'Yuyao Wang', 'Lingming Zhang']",
          "year": "2023",
          "summary": "Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code. Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis. However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies. While EvalPlus is general, we extend the test-cases of the popular HumanEval benchmark by 80x to build HumanEval+. Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that test insufficiency can lead to mis-ranking. For example, both WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+, while none of them could on HumanEval. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis, but also opens up a new direction to improve such programming benchmarks through automated testing. We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.",
          "primary_category": "cs.SE",
          "Topic": "Code synthesis evaluation rigor"
        }
      },
      {
        "id": "360287970189639684",
        "label": "Paper",
        "properties": {
          "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity",
          "authors": "['Yao Lu', 'Max Bartolo', 'Alastair Moore', 'Sebastian Riedel', 'Pontus Stenetorp']",
          "year": "2021",
          "summary": "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are \"fantastic\"\" and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.\"",
          "primary_category": "cs.CL",
          "Topic": "Few-shot learning prompt optimization"
        }
      },
      {
        "id": "360287970189639852",
        "label": "Paper",
        "properties": {
          "title": "Attention Is All You Need",
          "authors": "['Ashish Vaswani', 'Noam Shazeer', 'Niki Parmar', 'Jakob Uszkoreit', 'Llion Jones', 'Aidan N Gomez', 'Lukasz Kaiser', 'Illia Polosukhin']",
          "year": "2017",
          "summary": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
          "primary_category": "cs.CL",
          "Topic": "Attention-based sequence modeling"
        }
      },
      {
        "id": "360287970189639832",
        "label": "Paper",
        "properties": {
          "title": "Magicoder: Empowering Code Generation with OSS-Instruct",
          "authors": "['Yuxiang Wei', 'Zhe Wang', 'Jiawei Liu', 'Yifeng Ding', 'Lingming Zhang']",
          "year": "2023",
          "summary": "We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open-source references for the production of more realistic and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a new direction for crafting diverse synthetic instruction data for code using abundant open-source references.",
          "primary_category": "cs.CL",
          "Topic": "Empowering Code Generation with OSS"
        }
      },
      {
        "id": "360287970189639855",
        "label": "Paper",
        "properties": {
          "title": "API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs",
          "authors": "['Minghao Li', 'Yingxiu Zhao', 'Bowen Yu', 'Feifan Song', 'Hangyu Li', 'Haiyang Yu', 'Zhoujun Li', 'Fei Huang', 'Yongbin Li']",
          "year": "2023",
          "summary": "Recent research has demonstrated that Large Language Models (LLMs) can enhance their capabilities by utilizing external tools. However, three pivotal questions remain unanswered: (1) How effective are current LLMs in utilizing tools? (2) How can we enhance LLMs' ability to utilize tools? (3) What obstacles need to be overcome to leverage tools? To address these questions, we introduce API-Bank, a groundbreaking benchmark, specifically designed for tool-augmented LLMs. For the first question, we develop a runnable evaluation system consisting of 73 API tools. We annotate 314 tool-use dialogues with 753 API calls to assess the existing LLMs' capabilities in planning, retrieving, and calling APIs. For the second question, we construct a comprehensive training set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000 distinct domains. Using this dataset, we train Lynx, a tool-augmented LLM initialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits improved tool utilization compared to GPT-3, while GPT-4 excels in planning. However, there is still significant potential for further improvement. Moreover, Lynx surpasses Alpaca's tool utilization performance by more than 26 pts and approaches the effectiveness of GPT-3.5. Through error analysis, we highlight the key challenges for future research in this field to answer the third question.",
          "primary_category": "cs.CL",
          "Topic": "Tool-Augmented LLM Evaluation"
        }
      },
      {
        "id": "360287970189639856",
        "label": "Paper",
        "properties": {
          "title": "#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models",
          "authors": "['Keming Lu', 'Hongyi Yuan', 'Zheng Yuan', 'Runji Lin', 'Junyang Lin', 'Chuanqi Tan', 'Chang Zhou', 'Jingren Zhou']",
          "year": "2023",
          "summary": "Foundation language models obtain the instruction-following ability through supervised fine-tuning (SFT). Diversity and complexity are considered critical factors of a successful SFT dataset, while their definitions remain obscure and lack quantitative analyses. In this work, we propose InsTag, an open-set fine-grained tagger, to tag samples within SFT datasets based on semantics and intentions and define instruction diversity and complexity regarding tags. We obtain 6.6K tags to describe comprehensive user queries. Then we analyze popular open-sourced SFT datasets and find that the model ability grows with more diverse and complex data. Based on this observation, we propose a data selector based on InsTag to select 6K diverse and complex samples from open-source datasets and fine-tune models on InsTag-selected data. The resulting models, TagLM, outperform open-source models based on considerably larger SFT data evaluated by MT-Bench, echoing the importance of query diversity and complexity. We open-source InsTag in https://github.com/OFA-Sys/InsTag.",
          "primary_category": "cs.CL",
          "Topic": "Diverse and Complex Query Analysis"
        }
      },
      {
        "id": "360287970189639851",
        "label": "Paper",
        "properties": {
          "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
          "authors": "['William Fedus', 'Barret Zoph', 'Noam Shazeer']",
          "year": "2021",
          "summary": "In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the \"Colossal Clean Crawled Corpus\"\" and achieve a 4x speedup over the T5-XXL model.\"",
          "primary_category": "cs.LG",
          "Topic": "Sparsity in Large Language Models"
        }
      },
      {
        "id": "360287970189639863",
        "label": "Paper",
        "properties": {
          "title": "Generating Sequences by Learning to Self-Correct",
          "authors": "['Sean Welleck', 'Ximing Lu', 'Peter West', 'Faeze Brahman', 'Tianxiao Shen', 'Daniel Khashabi', 'Yejin Choi']",
          "year": "2022",
          "summary": "Sequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. We present Self-Correction, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. To train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. We show that Self-Correction improves upon the base generator in three diverse generation tasks - mathematical program synthesis, lexically-constrained generation, and toxicity control - even when the corrector is much smaller than the base generator.",
          "primary_category": "cs.CL",
          "Topic": "Iterative sequence refinement"
        }
      },
      {
        "id": "360287970189639882",
        "label": "Paper",
        "properties": {
          "title": "Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations",
          "authors": "['Peiyi Wang', 'Lei Li', 'Zhihong Shao', 'R X Xu', 'Damai Dai', 'Yifei Li', 'Deli Chen', 'Y Wu', 'Zhifang Sui']",
          "year": "2023",
          "summary": "In this paper, we present an innovative process-oriented math process reward model called \\textbf{Math-Shepherd}, which assigns a reward score to each step of math problem solutions. The training of Math-Shepherd is achieved using automatically constructed process-wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. We explore the effectiveness of Math-Shepherd in two scenarios: 1) \\textit{Verification}: Math-Shepherd is utilized for reranking multiple outputs generated by Large Language Models (LLMs); 2) \\textit{Reinforcement Learning}: Math-Shepherd is employed to reinforce LLMs with step-by-step Proximal Policy Optimization (PPO). With Math-Shepherd, a series of open-source LLMs demonstrates exceptional performance. For instance, the step-by-step PPO with Math-Shepherd significantly improves the accuracy of Mistral-7B (77.9\\%$\\to$84.1\\% on GSM8K and 28.6\\%$\\to$33.0\\% on MATH). The accuracy can be further enhanced to 89.1\\% and 43.5\\% on GSM8K and MATH with the verification of Math-Shepherd, respectively. We believe that automatic process supervision holds significant potential for the future evolution of LLMs.",
          "primary_category": "cs.AI",
          "Topic": "Automated Math Reasoning Verification"
        }
      },
      {
        "id": "360287970189639862",
        "label": "Paper",
        "properties": {
          "title": "What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning",
          "authors": "['Wei Liu', 'Weihao Zeng', 'Keqing He', 'Yong Jiang', 'Junxian He']",
          "year": "2023",
          "summary": "Instruction tuning is a standard technique employed to align large language models to end tasks and user preferences after the initial pretraining phase. Recent research indicates the critical role of data engineering in instruction tuning -- when appropriately selected, only limited data is necessary to achieve superior performance. However, we still lack a principled understanding of what makes good instruction tuning data for alignment, and how we should select data automatically and effectively. In this work, we delve deeply into automatic data selection strategies for alignment. We start with controlled studies to measure data across three dimensions: complexity, quality, and diversity, along which we examine existing methods and introduce novel techniques for enhanced data measurement. Subsequently, we propose a simple strategy to select data samples based on the measurement. We present deita (short for Data-Efficient Instruction Tuning for Alignment), a series of models fine-tuned from LLaMA and Mistral models using data samples automatically selected with our proposed approach. Empirically, deita performs better or on par with the state-of-the-art open-source alignment models with only 6K SFT training data samples -- over 10x less than the data used in the baselines. When further trained with direct preference optimization (DPO), deita-Mistral-7B + DPO trained with 6K SFT and 10K DPO samples achieve 7.55 MT-Bench and 90.06% AlpacaEval scores. We anticipate this work to provide tools on automatic data selection, facilitating data-efficient alignment. We release our models as well as the selected datasets for future researches to effectively align models more efficiently.",
          "primary_category": "cs.CL",
          "Topic": "Complex, high-quality, diverse data."
        }
      },
      {
        "id": "360287970189639889",
        "label": "Paper",
        "properties": {
          "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
          "authors": "['Noam Shazeer', 'Azalia Mirhoseini', 'Krzysztof Maziarz', 'Andy Davis', 'Quoc Le', 'Geoffrey Hinton', 'Jeff Dean']",
          "year": "2017",
          "summary": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",
          "primary_category": "cs.LG",
          "Topic": "Sparse Conditional Computation"
        }
      },
      {
        "id": "360287970189639745",
        "label": "Paper",
        "properties": {
          "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
          "authors": "['Colin Raffel', 'Noam Shazeer', 'Adam Roberts', 'Katherine Lee', 'Sharan Narang', 'Michael Matena', 'Yanqi Zhou', 'Wei Li', 'Peter J Liu']",
          "year": "2019",
          "summary": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
          "primary_category": "cs.LG",
          "Topic": "Text-to-Text Transfer Learning"
        }
      },
      {
        "id": "360287970189639818",
        "label": "Paper",
        "properties": {
          "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
          "authors": "['Rafael Rafailov', 'Archit Sharma', 'Eric Mitchell', 'Stefano Ermon', 'Christopher D Manning', 'Chelsea Finn']",
          "year": "2023",
          "summary": "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.",
          "primary_category": "cs.LG",
          "Topic": "Direct Preference Optimization"
        }
      },
      {
        "id": "360287970189639771",
        "label": "Paper",
        "properties": {
          "title": "Finetuned Language Models Are Zero-Shot Learners",
          "authors": "['Jason Wei', 'Maarten Bosma', 'Vincent Y Zhao', 'Kelvin Guu', 'Adams Wei Yu', 'Brian Lester', 'Nan Du', 'Andrew M Dai', 'Quoc V Le']",
          "year": "2021",
          "summary": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks.   We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",
          "primary_category": "cs.CL",
          "Topic": "Instruction Tuning for Zero-Shot Learning"
        }
      },
      {
        "id": "360287970189639710",
        "label": "Paper",
        "properties": {
          "title": "Training language models to follow instructions with human feedback",
          "authors": "['Long Ouyang', 'Jeff Wu', 'Xu Jiang', 'Diogo Almeida', 'Carroll L Wainwright', 'Pamela Mishkin', 'Chong Zhang', 'Sandhini Agarwal', 'Katarina Slama', 'Alex Ray', 'John Schulman', 'Jacob Hilton', 'Fraser Kelton', 'Luke Miller', 'Maddie Simens', 'Amanda Askell', 'Peter Welinder', 'Paul Christiano', 'Jan Leike', 'Ryan Lowe']",
          "year": "2022",
          "summary": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
          "primary_category": "cs.CL",
          "Topic": "Aligning language models with instructions"
        }
      },
      {
        "id": "360287970189639807",
        "label": "Paper",
        "properties": {
          "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context",
          "authors": "['Hao Liu', 'Matei Zaharia', 'Pieter Abbeel']",
          "year": "2023",
          "summary": "Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.",
          "primary_category": "cs.CL",
          "Topic": "Near Infinite Context Transformers"
        }
      },
      {
        "id": "360287970189639904",
        "label": "Paper",
        "properties": {
          "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
          "authors": "[]",
          "year": "2020",
          "summary": "",
          "primary_category": "",
          "Topic": "Scaling large language models"
        }
      },
      {
        "id": "360287970189639892",
        "label": "Paper",
        "properties": {
          "title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks",
          "authors": "['Yizhong Wang', 'Swaroop Mishra', 'Pegah Alipoormolabashi', 'Yeganeh Kordi', 'Amirreza Mirzaei', 'Anjana Arunkumar', 'Arjun Ashok', 'Arut Selvan Dhanasekaran', 'Atharva Naik', 'David Stap', 'Eshaan Pathak', 'Giannis Karamanolakis', 'Haizhi Gary Lai', 'Ishan Purohit', 'Ishani Mondal', 'Jacob Anderson', 'Kirby Kuznia', 'Krima Doshi', 'Maitreya Patel', 'Kuntal Kumar Pal', 'Mehrad Moradshahi', 'Mihir Parmar', 'Mirali Purohit', 'Neeraj Varshney', 'Phani Rohitha Kaza', 'Pulkit Verma', 'Ravsehaj Singh Puri', 'Rushang Karia', 'Shailaja Keyur Sampat', 'Savan Doshi', 'Siddhartha Mishra', 'Sujan Reddy', 'Sumanta Patro', 'Tanay Dixit', 'Xudong Shen', 'Chitta Baral', 'Yejin Choi', 'Noah A Smith', 'Hannaneh Hajishirzi', 'Daniel Khashabi']",
          "year": "2022",
          "summary": "How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions -- training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones. Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.",
          "primary_category": "cs.CL",
          "Topic": "Diverse NLP Task Generalization"
        }
      },
      {
        "id": "360287970189639698",
        "label": "Paper",
        "properties": {
          "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models",
          "authors": "['Tony Z Zhao', 'Eric Wallace', 'Shi Feng', 'Dan Klein', 'Sameer Singh']",
          "year": "2021",
          "summary": "GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as \"N/A\"\". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.\"",
          "primary_category": "cs.CL",
          "Topic": "Few-shot learning instability mitigation"
        }
      },
      {
        "id": "360287970189639911",
        "label": "Paper",
        "properties": {
          "title": "PAWS: Paraphrase Adversaries from Word Scrambling",
          "authors": "['Yuan Zhang', 'Jason Baldridge', 'Luheng He']",
          "year": "2019",
          "summary": "Existing paraphrase identification datasets lack sentence pairs that have high lexical overlap without being paraphrases. Models trained on such data fail to distinguish pairs like flights from New York to Florida and flights from Florida to New York. This paper introduces PAWS (Paraphrase Adversaries from Word Scrambling), a new dataset with 108,463 well-formed paraphrase and non-paraphrase pairs with high lexical overlap. Challenging pairs are generated by controlled word swapping and back translation, followed by fluency and paraphrase judgments by human raters. State-of-the-art models trained on existing datasets have dismal performance on PAWS (<40% accuracy); however, including PAWS training data for these models improves their accuracy to 85% while maintaining performance on existing tasks. In contrast, models that do not capture non-local contextual information fail even with PAWS training examples. As such, PAWS provides an effective instrument for driving further progress on models that better exploit structure, context, and pairwise comparisons.",
          "primary_category": "cs.CL",
          "Topic": "Word Order Sensitivity Evaluation"
        }
      },
      {
        "id": "360287970189639706",
        "label": "Paper",
        "properties": {
          "title": "Gemma: Open Models Based on Gemini Research and Technology",
          "authors": "['Gemma Team', 'Thomas Mesnard', 'Cassidy Hardin', 'Robert Dadashi', 'Surya Bhupatiraju', 'Shreya Pathak', 'Laurent Sifre', 'Morgane Rivière', 'Mihir Sanjay Kale', 'Juliette Love', 'Pouya Tafti', 'Léonard Hussenot', 'Pier Giuseppe Sessa', 'Aakanksha Chowdhery', 'Adam Roberts', 'Aditya Barua', 'Alex Botev', 'Alex Castro-Ros', 'Ambrose Slone', 'Amélie Héliou', 'Andrea Tacchetti', 'Anna Bulanova', 'Antonia Paterson', 'Beth Tsai', 'Bobak Shahriari', 'Charline Le Lan', 'Christopher A Choquette-Choo', 'Clément Crepy', 'Daniel Cer', 'Daphne Ippolito', 'David Reid', 'Elena Buchatskaya', 'Eric Ni', 'Eric Noland', 'Geng Yan', 'George Tucker', 'George-Christian Muraru', 'Grigory Rozhdestvenskiy', 'Henryk Michalewski', 'Ian Tenney', 'Ivan Grishchenko', 'Jacob Austin', 'James Keeling', 'Jane Labanowski', 'Jean-Baptiste Lespiau', 'Jeff Stanway', 'Jenny Brennan', 'Jeremy Chen', 'Johan Ferret', 'Justin Chiu', 'Justin Mao-Jones', 'Katherine Lee', 'Kathy Yu', 'Katie Millican', 'Lars Lowe Sjoesund', 'Lisa Lee', 'Lucas Dixon', 'Machel Reid', 'Maciej Mikuła', 'Mateo Wirth', 'Michael Sharman', 'Nikolai Chinaev', 'Nithum Thain', 'Olivier Bachem', 'Oscar Chang', 'Oscar Wahltinez', 'Paige Bailey', 'Paul Michel', 'Petko Yotov', 'Rahma Chaabouni', 'Ramona Comanescu', 'Reena Jana', 'Rohan Anil', 'Ross McIlroy', 'Ruibo Liu', 'Ryan Mullins', 'Samuel L Smith', 'Sebastian Borgeaud', 'Sertan Girgin', 'Sholto Douglas', 'Shree Pandya', 'Siamak Shakeri', 'Soham De', 'Ted Klimenko', 'Tom Hennigan', 'Vlad Feinberg', 'Wojciech Stokowiec', 'Yu-hui Chen', 'Zafarali Ahmed', 'Zhitao Gong', 'Tris Warkentin', 'Ludovic Peran', 'Minh Giang', 'Clément Farabet', 'Oriol Vinyals', 'Jeff Dean', 'Koray Kavukcuoglu', 'Demis Hassabis', 'Zoubin Ghahramani', 'Douglas Eck', 'Joelle Barral', 'Fernando Pereira', 'Eli Collins', 'Armand Joulin', 'Noah Fiedel', 'Evan Senter', 'Alek Andreev', 'Kathleen Kenealy']",
          "year": "2024",
          "summary": "This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.",
          "primary_category": "cs.CL",
          "Topic": "Language Model Performance and Safety"
        }
      },
      {
        "id": "360287970189639915",
        "label": "Paper",
        "properties": {
          "title": "BASE Layers: Simplifying Training of Large, Sparse Models",
          "authors": "['Mike Lewis', 'Shruti Bhosale', 'Tim Dettmers', 'Naman Goyal', 'Luke Zettlemoyer']",
          "year": "2021",
          "summary": "We introduce a new balanced assignment of experts (BASE) layer for large language models that greatly simplifies existing high capacity sparse layers. Sparse layers can dramatically improve the efficiency of training and inference by routing each token to specialized expert modules that contain only a small fraction of the model parameters. However, it can be difficult to learn balanced routing functions that make full use of the available experts; existing approaches typically use routing heuristics or auxiliary expert-balancing loss functions. In contrast, we formulate token-to-expert allocation as a linear assignment problem, allowing an optimal assignment in which each expert receives an equal number of tokens. This optimal assignment scheme improves efficiency by guaranteeing balanced compute loads, and also simplifies training by not requiring any new hyperparameters or auxiliary losses. Code is publicly released at https://github.com/pytorch/fairseq/",
          "primary_category": "cs.CL",
          "Topic": "Balanced Expert Assignment"
        }
      },
      {
        "id": "360287970189639916",
        "label": "Paper",
        "properties": {
          "title": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?",
          "authors": "['Zorik Gekhman', 'Gal Yona', 'Roee Aharoni', 'Matan Eyal', 'Amir Feder', 'Roi Reichart', 'Jonathan Herzig']",
          "year": "2024",
          "summary": "When large language models are aligned via supervised fine-tuning, they may encounter new factual information that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that introduce new knowledge are learned significantly slower than those consistent with the model's knowledge. However, we also find that as the examples with new knowledge are eventually learned, they linearly increase the model's tendency to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning teaches them to use it more efficiently.",
          "primary_category": "cs.CL",
          "Topic": "Fine-tuning LLMs and hallucinations."
        }
      }
    ]
  },
  {
    "category_id": 2,
    "name": "Mathematical Reasoning and Problem Solving",
    "description": "Papers exploring the capabilities of AI models in mathematical reasoning, problem-solving, and metacognitive skills.",
    "children": [
      {
        "id": "360287970189639687",
        "label": "Paper",
        "properties": {
          "title": "Language Models are Multilingual Chain-of-Thought Reasoners",
          "authors": "['Freda Shi', 'Mirac Suzgun', 'Markus Freitag', 'Xuezhi Wang', 'Suraj Srivats', 'Soroush Vosoughi', 'Hyung Won Chung', 'Yi Tay', 'Sebastian Ruder', 'Denny Zhou', 'Dipanjan Das', 'Jason Wei']",
          "year": "2022",
          "summary": "We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at https://github.com/google-research/url-nlp.",
          "primary_category": "cs.CL",
          "Topic": "Multilingual Chain-of-Thought Reasoning"
        }
      },
      {
        "id": "360287970189639692",
        "label": "Paper",
        "properties": {
          "title": "Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving",
          "authors": "['Aniket Didolkar', 'Anirudh Goyal', 'Nan Rosemary Ke', 'Siyuan Guo', 'Michal Valko', 'Timothy Lillicrap', 'Danilo Rezende', 'Yoshua Bengio', 'Michael Mozer', 'Sanjeev Arora']",
          "year": "2024",
          "summary": "Metacognitive knowledge refers to humans' intuitive knowledge of their own thinking and reasoning processes. Today's best LLMs clearly possess some reasoning processes. The paper gives evidence that they also have metacognitive knowledge, including ability to name skills and procedures to apply given a task. We explore this primarily in context of math reasoning, developing a prompt-guided interaction procedure to get a powerful LLM to assign sensible skill labels to math questions, followed by having it perform semantic clustering to obtain coarser families of skill labels. These coarse skill labels look interpretable to humans.   To validate that these skill labels are meaningful and relevant to the LLM's reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH. (b) When using an LLM to solve the test questions, we present it with the full list of skill labels and ask it to identify the skill needed. Then it is presented with randomly selected exemplar solved questions associated with that skill label. This improves accuracy on GSM8k and MATH for several strong LLMs, including code-assisted models. The methodology presented is domain-agnostic, even though this article applies it to math problems.",
          "primary_category": "cs.AI",
          "Topic": "Metacognitive Skill Labeling in LLMs"
        }
      },
      {
        "id": "360287970189639699",
        "label": "Paper",
        "properties": {
          "title": "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them",
          "authors": "['Mirac Suzgun', 'Nathan Scales', 'Nathanael Schärli', 'Sebastian Gehrmann', 'Yi Tay', 'Hyung Won Chung', 'Aakanksha Chowdhery', 'Quoc V Le', 'Ed H Chi', 'Denny Zhou', 'Jason Wei']",
          "year": "2022",
          "summary": "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models?   In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.",
          "primary_category": "cs.CL",
          "Topic": "Chain of Thought on Hard Tasks"
        }
      },
      {
        "id": "360287970189639708",
        "label": "Paper",
        "properties": {
          "title": "Common 7B Language Models Already Possess Strong Math Capabilities",
          "authors": "['Chen Li', 'Weiqi Wang', 'Jingcheng Hu', 'Yixuan Wei', 'Nanning Zheng', 'Han Hu', 'Zheng Zhang', 'Houwen Peng']",
          "year": "2024",
          "summary": "Mathematical capabilities were previously believed to emerge in common language models only at a very large scale or require extensive math-related pre-training. This paper shows that the LLaMA-2 7B model with common pre-training already exhibits strong mathematical abilities, as evidenced by its impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks, respectively, when selecting the best response from 256 random generations. The primary issue with the current base model is the difficulty in consistently eliciting its inherent mathematical capabilities. Notably, the accuracy for the first answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks, respectively. We find that simply scaling up the SFT data can significantly enhance the reliability of generating correct answers. However, the potential for extensive scaling is constrained by the scarcity of publicly available math questions. To overcome this limitation, we employ synthetic data, which proves to be nearly as effective as real data and shows no clear saturation when scaled up to approximately one million samples. This straightforward approach achieves an accuracy of 82.6% on GSM8K and 40.6% on MATH using LLaMA-2 7B models, surpassing previous models by 14.2% and 20.8%, respectively. We also provide insights into scaling behaviors across different reasoning complexities and error types.",
          "primary_category": "cs.CL",
          "Topic": "Mathematical capabilities in language models"
        }
      },
      {
        "id": "360287970189639709",
        "label": "Paper",
        "properties": {
          "title": "GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers",
          "authors": "['Qintong Li', 'Leyang Cui', 'Xueliang Zhao', 'Lingpeng Kong', 'Wei Bi']",
          "year": "2024",
          "summary": "Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs' math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (GSM-Plus) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes when new statements are added or the question targets are altered. We also explore whether more robust performance can be achieved by composing existing prompting methods, in which we try an iterative method that generates and verifies each intermediate thought based on its reasoning goal and calculation result.",
          "primary_category": "cs.CL",
          "Topic": "Mathematical Reasoning Robustness Evaluation"
        }
      },
      {
        "id": "360287970189639729",
        "label": "Paper",
        "properties": {
          "title": "SocialIQA: Commonsense Reasoning about Social Interactions",
          "authors": "['Maarten Sap', 'Hannah Rashkin', 'Derek Chen', 'Ronan LeBras', 'Yejin Choi']",
          "year": "2019",
          "summary": "We introduce Social IQa, the first largescale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: \"Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?\"\" A: \"\"Make sure no one else could hear\"\"). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance (>20% gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).\"",
          "primary_category": "cs.CL",
          "Topic": "Commonsense Social Reasoning"
        }
      },
      {
        "id": "360287970189639740",
        "label": "Paper",
        "properties": {
          "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
          "authors": "['Dheeru Dua', 'Yizhong Wang', 'Pradeep Dasigi', 'Gabriel Stanovsky', 'Sameer Singh', 'Matt Gardner']",
          "year": "2019",
          "summary": "Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 96k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literature on this dataset and show that the best systems only achieve 32.7% F1 on our generalized accuracy metric, while expert human performance is 96.0%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0% F1.",
          "primary_category": "cs.CL",
          "Topic": "Discrete Reasoning Over Paragraphs"
        }
      },
      {
        "id": "360287970189639765",
        "label": "Paper",
        "properties": {
          "title": "Augmented Language Models: a Survey",
          "authors": "['Grégoire Mialon', 'Roberto Dessì', 'Maria Lomeli', 'Christoforos Nalmpantis', 'Ram Pasunuru', 'Roberta Raileanu', 'Baptiste Rozière', 'Timo Schick', 'Jane Dwivedi-Yu', 'Asli Celikyilmaz', 'Edouard Grave', 'Yann LeCun', 'Thomas Scialom']",
          "year": "2023",
          "summary": "This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.",
          "primary_category": "cs.CL",
          "Topic": "Augmented Language Models"
        }
      },
      {
        "id": "360287970189639768",
        "label": "Paper",
        "properties": {
          "title": "A Diagram Is Worth A Dozen Images",
          "authors": "['Aniruddha Kembhavi', 'Mike Salvato', 'Eric Kolve', 'Minjoon Seo', 'Hannaneh Hajishirzi', 'Ali Farhadi']",
          "year": "2016",
          "summary": "Diagrams are common tools for representing complex concepts, relationships and events, often when it would be difficult to portray the same information with natural images. Understanding natural images has been extensively studied in computer vision, while diagram understanding has received little attention. In this paper, we study the problem of diagram interpretation and reasoning, the challenging task of identifying the structure of a diagram and the semantics of its constituents and their relationships. We introduce Diagram Parse Graphs (DPG) as our representation to model the structure of diagrams. We define syntactic parsing of diagrams as learning to infer DPGs for diagrams and study semantic interpretation and reasoning of diagrams in the context of diagram question answering. We devise an LSTM-based method for syntactic parsing of diagrams and introduce a DPG-based attention model for diagram question answering. We compile a new dataset of diagrams with exhaustive annotations of constituents and relationships for over 5,000 diagrams and 15,000 questions and answers. Our results show the significance of our models for syntactic parsing and question answering in diagrams using DPGs.",
          "primary_category": "cs.CV",
          "Topic": "Complex Diagram Interpretation"
        }
      },
      {
        "id": "360287970189639787",
        "label": "Paper",
        "properties": {
          "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
          "authors": "['Shunyu Yao', 'Jeffrey Zhao', 'Dian Yu', 'Nan Du', 'Izhak Shafran', 'Karthik Narasimhan', 'Yuan Cao']",
          "year": "2022",
          "summary": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",
          "primary_category": "cs.CL",
          "Topic": "Synergizing Reasoning and Acting"
        }
      },
      {
        "id": "360287970189639796",
        "label": "Paper",
        "properties": {
          "title": "$\\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens",
          "authors": "['Xinrong Zhang', 'Yingfa Chen', 'Shengding Hu', 'Zihang Xu', 'Junhao Chen', 'Moo Khai Hao', 'Xu Han', 'Zhen Leng Thai', 'Shuo Wang', 'Zhiyuan Liu', 'Maosong Sun']",
          "year": "2024",
          "summary": "Processing and reasoning over long contexts is crucial for many practical applications of Large Language Models (LLMs), such as document comprehension and agent construction. Despite recent strides in making LLMs process contexts with more than 100K tokens, there is currently a lack of a standardized benchmark to evaluate this long-context capability. Existing public benchmarks typically focus on contexts around 10K tokens, limiting the assessment and comparison of LLMs in processing longer contexts. In this paper, we propose $\\infty$Bench, the first LLM benchmark featuring an average data length surpassing 100K tokens. $\\infty$Bench comprises synthetic and realistic tasks spanning diverse domains, presented in both English and Chinese. The tasks in $\\infty$Bench are designed to require well understanding of long dependencies in contexts, and make simply retrieving a limited number of passages from contexts not sufficient for these tasks. In our experiments, based on $\\infty$Bench, we evaluate the state-of-the-art proprietary and open-source LLMs tailored for processing long contexts. The results indicate that existing long context LLMs still require significant advancements to effectively process 100K+ context. We further present three intriguing analyses regarding the behavior of LLMs processing long context.",
          "primary_category": "cs.CL",
          "Topic": "Long Context Language Model Evaluation"
        }
      },
      {
        "id": "360287970189639804",
        "label": "Paper",
        "properties": {
          "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training",
          "authors": "['Jie Ren', 'Samyam Rajbhandari', 'Reza Yazdani Aminabadi', 'Olatunji Ruwase', 'Shuangyan Yang', 'Minjia Zhang', 'Dong Li', 'Yuxiong He']",
          "year": "2021",
          "summary": "Large-scale model training has been a playing ground for a limited few requiring complex model refactoring and access to prohibitively expensive GPU clusters. ZeRO-Offload changes the large model training landscape by making large model training accessible to nearly everyone. It can train models with over 13 billion parameters on a single GPU, a 10x increase in size compared to popular framework such as PyTorch, and it does so without requiring any model change from the data scientists or sacrificing computational efficiency. ZeRO-Offload enables large model training by offloading data and compute to CPU. To preserve compute efficiency, it is designed to minimize the data movement to/from GPU, and reduce CPU compute time while maximizing memory savings on GPU. As a result, ZeRO-Offload can achieve 40 TFlops/GPU on a single NVIDIA V100 GPU for 10B parameter model compared to 30TF using PyTorch alone for a 1.4B parameter model, the largest that can be trained without running out of memory. ZeRO-Offload is also designed to scale on multiple-GPUs when available, offering near linear speedup on up to 128 GPUs. Additionally, it can work together with model parallelism to train models with over 70 billion parameters on a single DGX-2 box, a 4.5x increase in model size compared to using model parallelism alone. By combining compute and memory efficiency with ease-of-use, ZeRO-Offload democratizes large-scale model training making it accessible to even data scientists with access to just a single GPU.",
          "primary_category": "cs.DC",
          "Topic": "Democratizing Large-Scale Model Training"
        }
      },
      {
        "id": "360287970189639810",
        "label": "Paper",
        "properties": {
          "title": "VQA: Visual Question Answering",
          "authors": "['Aishwarya Agrawal', 'Jiasen Lu', 'Stanislaw Antol', 'Margaret Mitchell', 'C Lawrence Zitnick', 'Dhruv Batra', 'Devi Parikh']",
          "year": "2015",
          "summary": "We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ~0.25M images, ~0.76M questions, and ~10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (http://cloudcv.org/vqa).",
          "primary_category": "cs.CL",
          "Topic": "Visual Question Answering (VQA)"
        }
      },
      {
        "id": "360287970189639853",
        "label": "Paper",
        "properties": {
          "title": "WorldSense: A Synthetic Benchmark for Grounded Reasoning in Large Language Models",
          "authors": "['Youssef Benchekroun', 'Megi Dervishi', 'Mark Ibrahim', 'Jean-Baptiste Gaya', 'Xavier Martinet', 'Grégoire Mialon', 'Thomas Scialom', 'Emmanuel Dupoux', 'Dieuwke Hupkes', 'Pascal Vincent']",
          "year": "2023",
          "summary": "We propose WorldSense, a benchmark designed to assess the extent to which LLMs are consistently able to sustain tacit world models, by testing how they draw simple inferences from descriptions of simple arrangements of entities. Worldsense is a synthetic benchmark with three problem types, each with their own trivial control, which explicitly avoids bias by decorrelating the abstract structure of problems from the vocabulary and expressions, and by decorrelating all problem subparts with the correct response. We run our benchmark on three state-of-the-art chat-LLMs (GPT3.5, GPT4 and Llama2-chat) and show that these models make errors even with as few as three objects. Furthermore, they have quite heavy response biases, preferring certain responses irrespective of the question. Errors persist even with chain-of-thought prompting and in-context learning. Lastly, we show that while finetuning on similar problems does result in substantial improvements -- within- and out-of-distribution -- the finetuned models do not generalise beyond a constraint problem space.",
          "primary_category": "cs.CL",
          "Topic": "Grounded reasoning in LLMs"
        }
      },
      {
        "id": "360287970189639847",
        "label": "Paper",
        "properties": {
          "title": "Solving math word problems with process- and outcome-based feedback",
          "authors": "['Jonathan Uesato', 'Nate Kushman', 'Ramana Kumar', 'Francis Song', 'Noah Siegel', 'Lisa Wang', 'Antonia Creswell', 'Geoffrey Irving', 'Irina Higgins']",
          "year": "2022",
          "summary": "Recent work has shown that asking language models to generate reasoning steps improves performance on many reasoning tasks. When moving beyond prompting, this raises the question of how we should supervise such models: outcome-based approaches which supervise the final result, or process-based approaches which supervise the reasoning process itself? Differences between these approaches might naturally be expected not just in final-answer errors but also in reasoning errors, which can be difficult to detect and are problematic in many real-world domains such as education. We run the first comprehensive comparison between process- and outcome-based approaches trained on a natural language task, GSM8K. We find that pure outcome-based supervision produces similar final-answer error rates with less label supervision. However, for correct reasoning steps we find it necessary to use process-based supervision or supervision from learned reward models that emulate process-based feedback. In total, we improve the previous best results from 16.8% $\\to$ 12.7% final-answer error and 14.0% $\\to$ 3.4% reasoning error among final-answer-correct solutions.",
          "primary_category": "cs.LG",
          "Topic": "Process-based reasoning supervision"
        }
      },
      {
        "id": "360287970189639822",
        "label": "Paper",
        "properties": {
          "title": "FLEURS: Few-shot Learning Evaluation of Universal Representations of Speech",
          "authors": "['Alexis Conneau', 'Min Ma', 'Simran Khanuja', 'Yu Zhang', 'Vera Axelrod', 'Siddharth Dalmia', 'Jason Riesa', 'Clara Rivera', 'Ankur Bapna']",
          "year": "2022",
          "summary": "We introduce FLEURS, the Few-shot Learning Evaluation of Universal Representations of Speech benchmark. FLEURS is an n-way parallel speech dataset in 102 languages built on top of the machine translation FLoRes-101 benchmark, with approximately 12 hours of speech supervision per language. FLEURS can be used for a variety of speech tasks, including Automatic Speech Recognition (ASR), Speech Language Identification (Speech LangID), Translation and Retrieval. In this paper, we provide baselines for the tasks based on multilingual pre-trained models like mSLAM. The goal of FLEURS is to enable speech technology in more languages and catalyze research in low-resource speech understanding.",
          "primary_category": "cs.CL",
          "Topic": "Multilingual Speech Benchmark Evaluation"
        }
      },
      {
        "id": "360287970189639874",
        "label": "Paper",
        "properties": {
          "title": "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving",
          "authors": "['Zhibin Gou', 'Zhihong Shao', 'Yeyun Gong', 'Yelong Shen', 'Yujiu Yang', 'Minlie Huang', 'Nan Duan', 'Weizhu Chen']",
          "year": "2023",
          "summary": "Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by 22% absolute. ToRA-Code-34B is also the first open-source model that achieves an accuracy exceeding 50% on MATH, which significantly outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems with programs. Additionally, we conduct a comprehensive analysis of the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future research.",
          "primary_category": "cs.CL",
          "Topic": "Tool-Integrated Mathematical Reasoning"
        }
      },
      {
        "id": "360287970189639875",
        "label": "Paper",
        "properties": {
          "title": "Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset",
          "authors": "['Haoyi Wu', 'Wenyang Hui', 'Yezeng Chen', 'Weiqi Wu', 'Kewei Tu', 'Yi Zhou']",
          "year": "2023",
          "summary": "Mathematical understanding and reasoning are crucial tasks for assessing the capabilities of artificial intelligence (AI). However, existing benchmarks either require just a few steps of reasoning, or only contain a small amount of data in one specific topic, making it hard to analyse AI's behaviour with reference to different problems within a specific topic in detail. In this work, we propose Conic10K, a challenging math problem dataset on conic sections in Chinese senior high school education. Our dataset contains various problems with different reasoning depths, while only the knowledge from conic sections is required. Since the dataset only involves a narrow range of knowledge, it is easy to separately analyse the knowledge a model possesses and the reasoning ability it has. For each problem, we provide a high-quality formal representation, the reasoning steps, and the final solution. Experiments show that existing large language models, including GPT-4, exhibit weak performance on complex reasoning. We hope that our findings could inspire more advanced techniques for precise natural language understanding and reasoning. Our dataset and codes are available at https://github.com/whyNLP/Conic10K.",
          "primary_category": "cs.CL",
          "Topic": "Mathematical reasoning in conic sections."
        }
      },
      {
        "id": "360287970189639896",
        "label": "Paper",
        "properties": {
          "title": "Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations",
          "authors": "['Nuo Chen', 'Zinan Zheng', 'Ning Wu', 'Ming Gong', 'Dongmei Zhang', 'Jia Li']",
          "year": "2023",
          "summary": "Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6% accuracy which exceeds ChatGPT 46.3% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When extending the rejection sampling strategy to the multilingual context, it proves effective for model performances, albeit limited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT) across multiple languages not only significantly enhances model performance multilingually but also elevates their monolingual performance. This indicates that crafting multilingual corpora can be regarded as a vital strategy for enhancing model performance in a specific language, especially in mathematical reasoning tasks. For instance, MathOctopus-7B improves its counterparts that trained on English from 42.2% to 50.8% on GSM8K testset. Codes are available at https://github.com/microsoft/MathOctopus.",
          "primary_category": "cs.CL",
          "Topic": "Multilingual Mathematical Reasoning Enhancement"
        }
      },
      {
        "id": "360287970189639763",
        "label": "Paper",
        "properties": {
          "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models",
          "authors": "['Longhui Yu', 'Weisen Jiang', 'Han Shi', 'Jincheng Yu', 'Zhengying Liu', 'Yu Zhang', 'James T Kwok', 'Zhenguo Li', 'Adrian Weller', 'Weiyang Liu']",
          "year": "2023",
          "summary": "Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use.",
          "primary_category": "cs.CL",
          "Topic": "Mathematical Question Bootstrapping"
        }
      },
      {
        "id": "360287970189639829",
        "label": "Paper",
        "properties": {
          "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
          "authors": "['Zhihong Shao', 'Peiyi Wang', 'Qihao Zhu', 'Runxin Xu', 'Junxiao Song', 'Xiao Bi', 'Haowei Zhang', 'Mingchuan Zhang', 'Y K Li', 'Y Wu', 'Daya Guo']",
          "year": "2024",
          "summary": "Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",
          "primary_category": "cs.CL",
          "Topic": "Mathematical Reasoning in Language Models"
        }
      },
      {
        "id": "360287970189639833",
        "label": "Paper",
        "properties": {
          "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
          "authors": "['Jason Wei', 'Xuezhi Wang', 'Dale Schuurmans', 'Maarten Bosma', 'Brian Ichter', 'Fei Xia', 'Ed Chi', 'Quoc Le', 'Denny Zhou']",
          "year": "2022",
          "summary": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
          "primary_category": "cs.CL",
          "Topic": "Chain of Thought Prompting"
        }
      }
    ]
  },
  {
    "category_id": 3,
    "name": "Multimodal Learning",
    "description": "Papers discussing models that integrate multiple modalities such as text, images, and speech for improved understanding and generation.",
    "children": [
      {
        "id": "360287970189639683",
        "label": "Paper",
        "properties": {
          "title": "Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V",
          "authors": "['Jianwei Yang', 'Hao Zhang', 'Feng Li', 'Xueyan Zou', 'Chunyuan Li', 'Jianfeng Gao']",
          "year": "2023",
          "summary": "We present Set-of-Mark (SoM), a new visual prompting method, to unleash the visual grounding abilities of large multimodal models (LMMs), such as GPT-4V. As illustrated in Fig. 1 (right), we employ off-the-shelf interactive segmentation models, such as SEEM/SAM, to partition an image into regions at different levels of granularity, and overlay these regions with a set of marks e.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V can answer the questions that require visual grounding. We perform a comprehensive empirical study to validate the effectiveness of SoM on a wide range of fine-grained vision and multimodal tasks. For example, our experiments show that GPT-4V with SoM in zero-shot setting outperforms the state-of-the-art fully-finetuned referring expression comprehension and segmentation model on RefCOCOg. Code for SoM prompting is made public at: https://github.com/microsoft/SoM.",
          "primary_category": "cs.CV",
          "Topic": "Visual Grounding with SoM"
        }
      },
      {
        "id": "360287970189639685",
        "label": "Paper",
        "properties": {
          "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
          "authors": "['Hang Zhang', 'Xin Li', 'Lidong Bing']",
          "year": "2023",
          "summary": "We present Video-LLaMA a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual and audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual and audio encoders with LLM's embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.",
          "primary_category": "cs.CL",
          "Topic": "Audio-Visual Video Comprehension"
        }
      },
      {
        "id": "360287970189639693",
        "label": "Paper",
        "properties": {
          "title": "Foundation Models for Video Understanding: A Survey",
          "authors": "['Neelu Madan', 'Andreas Moegelmose', 'Rajat Modi', 'Yogesh S Rawat', 'Thomas B Moeslund']",
          "year": "2024",
          "summary": "Video Foundation Models (ViFMs) aim to learn a general-purpose representation for various video understanding tasks. Leveraging large-scale datasets and powerful models, ViFMs achieve this by capturing robust and generic features from video data. This survey analyzes over 200 video foundational models, offering a comprehensive overview of benchmarks and evaluation metrics across 14 distinct video tasks categorized into 3 main categories. Additionally, we offer an in-depth performance analysis of these models for the 6 most common video tasks. We categorize ViFMs into three categories: 1) Image-based ViFMs, which adapt existing image models for video tasks, 2) Video-Based ViFMs, which utilize video-specific encoding methods, and 3) Universal Foundational Models (UFMs), which combine multiple modalities (image, video, audio, and text etc.) within a single framework. By comparing the performance of various ViFMs on different tasks, this survey offers valuable insights into their strengths and weaknesses, guiding future advancements in video understanding. Our analysis surprisingly reveals that image-based foundation models consistently outperform video-based models on most video understanding tasks. Additionally, UFMs, which leverage diverse modalities, demonstrate superior performance on video tasks. We share the comprehensive list of ViFMs studied in this work at: \\url{https://github.com/NeeluMadan/ViFM_Survey.git}",
          "primary_category": "cs.CV",
          "Topic": "Video understanding foundational models"
        }
      },
      {
        "id": "360287970189639696",
        "label": "Paper",
        "properties": {
          "title": "TALM: Tool Augmented Language Models",
          "authors": "['Aaron Parisi', 'Yao Zhao', 'Noah Fiedel']",
          "year": "2022",
          "summary": "Transformer based language models (LMs) demonstrate increasing performance with scale across a wide variety of tasks. Scale alone however cannot enable models to solve tasks that require access to ephemeral, changing, or private data that was unavailable at training time. Many useful tasks may also benefit from LMs being able to access APIs that read or modify state. In this work, we present Tool Augmented Language Models (TALM), combining a text-only approach to augment language models with non-differentiable tools, and an iterative \"self-play\"\" technique to bootstrap performance starting from few tool demonstrations. TALM exhibits strong performance on both a knowledge-heavy QA task and a reasoning oriented math task with simple tools. At a given model scale, TALM significantly outperforms non-augmented LMs. We further demonstrate that TALM successfully performs out-of-distribution inferences on both QA and math tasks, where non-augmented LMs fail. Our results suggest that Tool Augmented Language Models are a promising direction to enrich LMs' capabilities, with less dependence on scale.\"",
          "primary_category": "cs.CL",
          "Topic": "Tool-Augmented Language Models"
        }
      },
      {
        "id": "360287970189639686",
        "label": "Paper",
        "properties": {
          "title": "Self-supervised Learning with Random-projection Quantizer for Speech Recognition",
          "authors": "['Chung-Cheng Chiu', 'James Qin', 'Yu Zhang', 'Jiahui Yu', 'Yonghui Wu']",
          "year": "2022",
          "summary": "We present a simple and effective self-supervised learning approach for speech recognition. The approach learns a model to predict the masked speech signals, in the form of discrete labels generated with a random-projection quantizer. In particular the quantizer projects speech inputs with a randomly initialized matrix, and does a nearest-neighbor lookup in a randomly-initialized codebook. Neither the matrix nor the codebook is updated during self-supervised learning. Since the random-projection quantizer is not trained and is separated from the speech recognition model, the design makes the approach flexible and is compatible with universal speech recognition architecture. On LibriSpeech our approach achieves similar word-error-rates as previous work using self-supervised learning with non-streaming models, and provides lower word-error-rates and latency than wav2vec 2.0 and w2v-BERT with streaming models. On multilingual tasks the approach also provides significant improvement over wav2vec 2.0 and w2v-BERT.",
          "primary_category": "cs.CL",
          "Topic": "Self-supervised speech representation learning"
        }
      },
      {
        "id": "360287970189639704",
        "label": "Paper",
        "properties": {
          "title": "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection",
          "authors": "['Bin Lin', 'Yang Ye', 'Bin Zhu', 'Jiaxi Cui', 'Munan Ning', 'Peng Jin', 'Li Yuan']",
          "year": "2023",
          "summary": "The Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers. In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other. Video-LLaVA achieves superior performances on a broad range of 9 image benchmarks across 5 image question-answering datasets and 4 image benchmark toolkits. Additionally, our Video-LLaVA also outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos. We aim for this work to provide modest insights into the multi-modal inputs for the LLM. Code address: \\href{https://github.com/PKU-YuanGroup/Video-LLaVA}",
          "primary_category": "cs.CV",
          "Topic": "Unified Visual Representation Learning"
        }
      },
      {
        "id": "360287970189639715",
        "label": "Paper",
        "properties": {
          "title": "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks",
          "authors": "['Bin Xiao', 'Haiping Wu', 'Weijian Xu', 'Xiyang Dai', 'Houdong Hu', 'Yumao Lu', 'Michael Zeng', 'Ce Liu', 'Lu Yuan']",
          "year": "2023",
          "summary": "We introduce Florence-2, a novel vision foundation model with a unified, prompt-based representation for a variety of computer vision and vision-language tasks. While existing large vision models excel in transfer learning, they struggle to perform a diversity of tasks with simple instructions, a capability that implies handling the complexity of various spatial hierarchy and semantic granularity. Florence-2 was designed to take text-prompt as task instructions and generate desirable results in text forms, whether it be captioning, object detection, grounding or segmentation. This multi-task learning setup demands large-scale, high-quality annotated data. To this end, we co-developed FLD-5B that consists of 5.4 billion comprehensive visual annotations on 126 million images, using an iterative strategy of automated image annotation and model refinement. We adopted a sequence-to-sequence structure to train Florence-2 to perform versatile and comprehensive vision tasks. Extensive evaluations on numerous tasks demonstrated Florence-2 to be a strong vision foundation model contender with unprecedented zero-shot and fine-tuning capabilities.",
          "primary_category": "cs.CV",
          "Topic": "Unified Vision Representation"
        }
      },
      {
        "id": "360287970189639717",
        "label": "Paper",
        "properties": {
          "title": "Perceiver: General Perception with Iterative Attention",
          "authors": "['Andrew Jaegle', 'Felix Gimeno', 'Andrew Brock', 'Andrew Zisserman', 'Oriol Vinyals', 'Joao Carreira']",
          "year": "2021",
          "summary": "Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.",
          "primary_category": "cs.CV",
          "Topic": "Iterative Attention for Multimodal Perception"
        }
      },
      {
        "id": "360287970189639725",
        "label": "Paper",
        "properties": {
          "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
          "authors": "['Wenliang Dai', 'Junnan Li', 'Dongxu Li', 'Anthony Meng Huat Tiong', 'Junqi Zhao', 'Weisheng Wang', 'Boyang Li', 'Pascale Fung', 'Steven Hoi']",
          "year": "2023",
          "summary": "Large-scale pre-training and instruction tuning have been successful at creating general-purpose language models with broad competence. However, building general-purpose vision-language models is challenging due to the rich input distributions and task diversity resulting from the additional visual input. Although vision-language pretraining has been widely studied, vision-language instruction tuning remains under-explored. In this paper, we conduct a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models. We gather 26 publicly available datasets, covering a wide variety of tasks and capabilities, and transform them into instruction tuning format. Additionally, we introduce an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction. Trained on 13 held-in datasets, InstructBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and larger Flamingo models. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models. All InstructBLIP models are open-sourced at https://github.com/salesforce/LAVIS/tree/main/projects/instructblip.",
          "primary_category": "cs.CV",
          "Topic": "Vision-Language Instruction Tuning"
        }
      },
      {
        "id": "360287970189639731",
        "label": "Paper",
        "properties": {
          "title": "PromptTTS: Controllable Text-to-Speech with Text Descriptions",
          "authors": "['Zhifang Guo', 'Yichong Leng', 'Yihan Wu', 'Sheng Zhao', 'Xu Tan']",
          "year": "2022",
          "summary": "Using a text description as prompt to guide the generation of text or images (e.g., GPT-3 or DALLE-2) has drawn wide attention recently. Beyond text and image generation, in this work, we explore the possibility of utilizing text descriptions to guide speech synthesis. Thus, we develop a text-to-speech (TTS) system (dubbed as PromptTTS) that takes a prompt with both style and content descriptions as input to synthesize the corresponding speech. Specifically, PromptTTS consists of a style encoder and a content encoder to extract the corresponding representations from the prompt, and a speech decoder to synthesize speech according to the extracted style and content representations. Compared with previous works in controllable TTS that require users to have acoustic knowledge to understand style factors such as prosody and pitch, PromptTTS is more user-friendly since text descriptions are a more natural way to express speech style (e.g., ''A lady whispers to her friend slowly''). Given that there is no TTS dataset with prompts, to benchmark the task of PromptTTS, we construct and release a dataset containing prompts with style and content information and the corresponding speech. Experiments show that PromptTTS can generate speech with precise style control and high speech quality. Audio samples and our dataset are publicly available.",
          "primary_category": "eess.AS",
          "Topic": "Text-to-Speech with Natural Language Prompts"
        }
      },
      {
        "id": "360287970189639733",
        "label": "Paper",
        "properties": {
          "title": "Code Llama: Open Foundation Models for Code",
          "authors": "['Baptiste Rozière', 'Jonas Gehring', 'Fabian Gloeckle', 'Sten Sootla', 'Itai Gat', 'Xiaoqing Ellen Tan', 'Yossi Adi', 'Jingyu Liu', 'Romain Sauvestre', 'Tal Remez', 'Jérémy Rapin', 'Artyom Kozhevnikov', 'Ivan Evtimov', 'Joanna Bitton', 'Manish Bhatt', 'Cristian Canton Ferrer', 'Aaron Grattafiori', 'Wenhan Xiong', 'Alexandre Défossez', 'Jade Copet', 'Faisal Azhar', 'Hugo Touvron', 'Louis Martin', 'Nicolas Usunier', 'Thomas Scialom', 'Gabriel Synnaeve']",
          "year": "2023",
          "summary": "We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.",
          "primary_category": "cs.CL",
          "Topic": "Code Generation and Infilling"
        }
      },
      {
        "id": "360287970189639736",
        "label": "Paper",
        "properties": {
          "title": "Toward Joint Language Modeling for Speech Units and Text",
          "authors": "['Ju-Chieh Chou', 'Chung-Ming Chien', 'Wei-Ning Hsu', 'Karen Livescu', 'Arun Babu', 'Alexis Conneau', 'Alexei Baevski', 'Michael Auli']",
          "year": "2023",
          "summary": "Speech and text are two major forms of human language. The research community has been focusing on mapping speech to text or vice versa for many years. However, in the field of language modeling, very little effort has been made to model them jointly. In light of this, we explore joint language modeling for speech units and text. Specifically, we compare different speech tokenizers to transform continuous speech signals into discrete units and use different methods to construct mixed speech-text data. We introduce automatic metrics to evaluate how well the joint LM mixes speech and text. We also fine-tune the LM on downstream spoken language understanding (SLU) tasks with different modalities (speech or text) and test its performance to assess the model's learning of shared representations. Our results show that by mixing speech units and text with our proposed mixing techniques, the joint LM improves over a speech-only baseline on SLU tasks and shows zero-shot cross-modal transferability.",
          "primary_category": "cs.CL",
          "Topic": "Joint Speech-Text Language Modeling"
        }
      },
      {
        "id": "360287970189639743",
        "label": "Paper",
        "properties": {
          "title": "ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding",
          "authors": "['Uri Shaham', 'Maor Ivgi', 'Avia Efrat', 'Jonathan Berant', 'Omer Levy']",
          "year": "2023",
          "summary": "We introduce ZeroSCROLLS, a zero-shot benchmark for natural language understanding over long texts, which contains only test and small validation sets, without training data. We adapt six tasks from the SCROLLS benchmark, and add four new datasets, including two novel information fusing tasks, such as aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a comprehensive evaluation of both open-source and closed large language models, finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest average score. However, there is still room for improvement on multiple open challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to pass the naive baseline. As the state of the art is a moving target, we invite researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard.",
          "primary_category": "cs.CL",
          "Topic": "Long Text Understanding Benchmark"
        }
      },
      {
        "id": "360287970189639752",
        "label": "Paper",
        "properties": {
          "title": "Towards VQA Models That Can Read",
          "authors": "['Amanpreet Singh', 'Vivek Natarajan', 'Meet Shah', 'Yu Jiang', 'Xinlei Chen', 'Dhruv Batra', 'Devi Parikh', 'Marcus Rohrbach']",
          "year": "2019",
          "summary": "Studies have shown that a dominant class of questions asked by visually impaired users on images of their surroundings involves reading text in the image. But today's VQA models can not read! Our paper takes a first step towards addressing this problem. First, we introduce a new \"TextVQA\"\" dataset to facilitate progress on this important problem. Existing datasets either have a small proportion of questions about text (e.g., the VQA dataset) or are too small (e.g., the VizWiz dataset). TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Second, we introduce a novel model architecture that reads text in the image, reasons about it in the context of the image and the question, and predicts an answer which might be a deduction based on the text and the image or composed of the strings found in the image. Consequently, we call our approach Look, Read, Reason & Answer (LoRRA). We show that LoRRA outperforms existing state-of-the-art VQA models on our TextVQA dataset. We find that the gap between human performance and machine performance is significantly larger on TextVQA than on VQA 2.0, suggesting that TextVQA is well-suited to benchmark progress along directions complementary to VQA 2.0.\"",
          "primary_category": "cs.CL",
          "Topic": "Reading and Reasoning with TextVQA"
        }
      },
      {
        "id": "360287970189639756",
        "label": "Paper",
        "properties": {
          "title": "Spirit LM: Interleaved Spoken and Written Language Model",
          "authors": "['Tu Anh Nguyen', 'Benjamin Muller', 'Bokai Yu', 'Marta R Costa-jussa', 'Maha Elbayad', 'Sravya Popuri', 'Christophe Ropers', 'Paul-Ambroise Duquenne', 'Robin Algayres', 'Ruslan Mavlyutov', 'Itai Gat', 'Mary Williamson', 'Gabriel Synnaeve', 'Juan Pino', 'Benoit Sagot', 'Emmanuel Dupoux']",
          "year": "2024",
          "summary": "We introduce Spirit LM, a foundation multimodal language model that freely mixes text and speech. Our model is based on a 7B pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single stream of tokens, and trained with a word-level interleaving method using a small automatically-curated speech-text parallel corpus. Spirit LM comes in two versions: a Base version that uses speech phonetic units (HuBERT) and an Expressive version that models expressivity using pitch and style units in addition to the phonetic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that Spirit LM can learn new tasks in a few-shot fashion across modalities (i.e. ASR, TTS, Speech Classification). We make available model weights and inference code.",
          "primary_category": "cs.CL",
          "Topic": "Interleaved Speech and Text Modeling"
        }
      },
      {
        "id": "360287970189639705",
        "label": "Paper",
        "properties": {
          "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
          "authors": "['David Rein', 'Betty Li Hou', 'Asa Cooper Stickland', 'Jackson Petty', 'Richard Yuanzhe Pang', 'Julien Dirani', 'Julian Michael', 'Samuel R Bowman']",
          "year": "2023",
          "summary": "We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are \"Google-proof\"\"). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.\"",
          "primary_category": "cs.AI",
          "Topic": "Challenging graduate-level questions"
        }
      },
      {
        "id": "360287970189639750",
        "label": "Paper",
        "properties": {
          "title": "ViP-LLaVA: Making Large Multimodal Models Understand Arbitrary Visual Prompts",
          "authors": "['Mu Cai', 'Haotian Liu', 'Dennis Park', 'Siva Karthik Mustikovela', 'Gregory P Meyer', 'Yuning Chai', 'Yong Jae Lee']",
          "year": "2023",
          "summary": "While existing large vision-language multimodal models focus on whole image understanding, there is a prominent gap in achieving region-specific comprehension. Current approaches that use textual coordinates or spatial encodings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we introduce a novel multimodal model capable of decoding arbitrary visual prompts. This allows users to intuitively mark images and interact with the model using natural cues like a \"red bounding box\"\" or \"\"pointed arrow\"\". Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encodings, yet achieves state-of-the-art performance on region-understanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present ViP-Bench, a comprehensive benchmark to assess the capability of models in understanding visual prompts across multiple dimensions, enabling future research in this domain. Code, data, and model are publicly available.\"",
          "primary_category": "cs.CV",
          "Topic": "Region-specific visual comprehension"
        }
      },
      {
        "id": "360287970189639792",
        "label": "Paper",
        "properties": {
          "title": "NExT-QA:Next Phase of Question-Answering to Explaining Temporal Actions",
          "authors": "['Junbin Xiao', 'Xindi Shang', 'Angela Yao', 'Tat-Seng Chua']",
          "year": "2021",
          "summary": "We introduce NExT-QA, a rigorously designed video question answering (VideoQA) benchmark to advance video understanding from describing to explaining the temporal actions. Based on the dataset, we set up multi-choice and open-ended QA tasks targeting causal action reasoning, temporal action reasoning, and common scene comprehension. Through extensive analysis of baselines and established VideoQA techniques, we find that top-performing methods excel at shallow scene descriptions but are weak in causal and temporal action reasoning. Furthermore, the models that are effective on multi-choice QA, when adapted to open-ended QA, still struggle in generalizing the answers. This raises doubt on the ability of these models to reason and highlights possibilities for improvement. With detailed results for different question types and heuristic observations for future works, we hope NExT-QA will guide the next generation of VQA research to go beyond superficial scene description towards a deeper understanding of videos. (The dataset and related resources are available at https://github.com/doc-doc/NExT-QA.git)",
          "primary_category": "cs.CV",
          "Topic": "Causal and Temporal Action Reasoning"
        }
      },
      {
        "id": "360287970189639793",
        "label": "Paper",
        "properties": {
          "title": "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI",
          "authors": "['Xiang Yue', 'Yuansheng Ni', 'Kai Zhang', 'Tianyu Zheng', 'Ruoqi Liu', 'Ge Zhang', 'Samuel Stevens', 'Dongfu Jiang', 'Weiming Ren', 'Yuxuan Sun', 'Cong Wei', 'Botao Yu', 'Ruibin Yuan', 'Renliang Sun', 'Ming Yin', 'Boyuan Zheng', 'Zhenzhu Yang', 'Yibo Liu', 'Wenhao Huang', 'Huan Sun', 'Yu Su', 'Wenhu Chen']",
          "year": "2023",
          "summary": "We introduce MMMU: a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering. These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. Unlike existing benchmarks, MMMU focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. The evaluation of 14 open-source LMMs as well as the proprietary GPT-4V(ision) and Gemini highlights the substantial challenges posed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve accuracies of 56% and 59% respectively, indicating significant room for improvement. We believe MMMU will stimulate the community to build next-generation multimodal foundation models towards expert artificial general intelligence.",
          "primary_category": "cs.CL",
          "Topic": "College-Level Multimodal Reasoning"
        }
      },
      {
        "id": "360287970189639754",
        "label": "Paper",
        "properties": {
          "title": "The Stack: 3 TB of permissively licensed source code",
          "authors": "['Denis Kocetkov', 'Raymond Li', 'Loubna Ben Allal', 'Jia Li', 'Chenghao Mou', 'Carlos Muñoz Ferrandis', 'Yacine Jernite', 'Margaret Mitchell', 'Sean Hughes', 'Thomas Wolf', 'Dzmitry Bahdanau', 'Leandro von Werra', 'Harm de Vries']",
          "year": "2022",
          "summary": "Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)--not only for natural language processing but also for code understanding and generation. To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets. We find that (1) near-deduplicating the data significantly boosts performance across all experiments, and (2) it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data. We make the dataset available at https://hf.co/BigCode, provide a tool called \"Am I in The Stack\"\" (https://hf.co/spaces/bigcode/in-the-stack) for developers to search The Stack for copies of their code, and provide a process for code to be removed from the dataset by following the instructions at https://www.bigcode-project.org/docs/about/the-stack/.\"",
          "primary_category": "cs.CL",
          "Topic": "Open Code Dataset Development"
        }
      },
      {
        "id": "360287970189639805",
        "label": "Paper",
        "properties": {
          "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct",
          "authors": "['Haipeng Luo', 'Qingfeng Sun', 'Can Xu', 'Pu Zhao', 'Jianguang Lou', 'Chongyang Tao', 'Xiubo Geng', 'Qingwei Lin', 'Shifeng Chen', 'Yansong Tang', 'Dongmei Zhang']",
          "year": "2023",
          "summary": "Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical CoT reasoning abilities of LLMs without using external python tools, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency. Furthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini Pro and GPT-4-early-version. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance. For more details refer to https://github.com/nlpxucan/WizardLM",
          "primary_category": "cs.CL",
          "Topic": "Mathematical Reasoning Enhancement"
        }
      },
      {
        "id": "360287970189639815",
        "label": "Paper",
        "properties": {
          "title": "VideoChat: Chat-Centric Video Understanding",
          "authors": "['KunChang Li', 'Yinan He', 'Yi Wang', 'Yizhuo Li', 'Wenhai Wang', 'Ping Luo', 'Yali Wang', 'Limin Wang', 'Yu Qiao']",
          "year": "2023",
          "summary": "In this paper, we initiate an attempt of developing an end-to-end chat-centric video understanding system, coined as VideoChat. It integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference. To instructively tune this system, we build a video-centric instruction dataset, composed of thousands of videos associated with detailed descriptions and conversations. This dataset emphasizes spatiotemporal reasoning and captures causal relationships, providing a valuable asset for training our chat-centric video understanding system. Preliminary qualitative experiments demonstrate the potential of our system across a broad spectrum of video applications, which could serve as a simple prototype system for future research on chat-centric video understanding. Access our code and data at https://github.com/OpenGVLab/Ask-Anything",
          "primary_category": "cs.CV",
          "Topic": "Chat-centric video understanding"
        }
      },
      {
        "id": "360287970189639823",
        "label": "Paper",
        "properties": {
          "title": "Know What You Don't Know: Unanswerable Questions for SQuAD",
          "authors": "['Pranav Rajpurkar', 'Robin Jia', 'Percy Liang']",
          "year": "2018",
          "summary": "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0.",
          "primary_category": "cs.CL",
          "Topic": "Unanswerable Questions in Comprehension"
        }
      },
      {
        "id": "360287970189639838",
        "label": "Paper",
        "properties": {
          "title": "ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering",
          "authors": "['Zhou Yu', 'Dejing Xu', 'Jun Yu', 'Ting Yu', 'Zhou Zhao', 'Yueting Zhuang', 'Dacheng Tao']",
          "year": "2019",
          "summary": "Recent developments in modeling language and vision have been successfully applied to image question answering. It is both crucial and natural to extend this research direction to the video domain for video question answering (VideoQA). Compared to the image domain where large scale and fully annotated benchmark datasets exists, VideoQA datasets are limited to small scale and are automatically generated, etc. These limitations restrict their applicability in practice. Here we introduce ActivityNet-QA, a fully annotated and large scale VideoQA dataset. The dataset consists of 58,000 QA pairs on 5,800 complex web videos derived from the popular ActivityNet dataset. We present a statistical analysis of our ActivityNet-QA dataset and conduct extensive experiments on it by comparing existing VideoQA baselines. Moreover, we explore various video representation strategies to improve VideoQA performance, especially for long videos. The dataset is available at https://github.com/MILVLG/activitynet-qa",
          "primary_category": "cs.CV",
          "Topic": "Video Question Answering"
        }
      },
      {
        "id": "360287970189639848",
        "label": "Paper",
        "properties": {
          "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling",
          "authors": "['Eugene Kharitonov', 'Ann Lee', 'Adam Polyak', 'Yossi Adi', 'Jade Copet', 'Kushal Lakhotia', 'Tu-Anh Nguyen', 'Morgane Rivière', 'Abdelrahman Mohamed', 'Emmanuel Dupoux', 'Wei-Ning Hsu']",
          "year": "2021",
          "summary": "Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) \\cite{Lakhotia2021} is the only prior work addressing the generative aspects of speech pre-training, which replaces text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences. Unfortunately, despite eliminating the need of text, the units used in GSLM discard most of the prosodic information. Hence, GSLM fails to leverage prosody for better comprehension, and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. We devise a series of metrics for prosody modeling and generation, and re-use metrics from GSLM for content modeling. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm. Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.",
          "primary_category": "cs.CL",
          "Topic": "Prosody-aware speech generation"
        }
      },
      {
        "id": "360287970189639778",
        "label": "Paper",
        "properties": {
          "title": "Efficient Estimation of Word Representations in Vector Space",
          "authors": "['Tomas Mikolov', 'Kai Chen', 'Greg Corrado', 'Jeffrey Dean']",
          "year": "2013",
          "summary": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
          "primary_category": "cs.CL",
          "Topic": "Efficient Word Vector Learning"
        }
      },
      {
        "id": "360287970189639858",
        "label": "Paper",
        "properties": {
          "title": "Multi-Task Learning for Front-End Text Processing in TTS",
          "authors": "['Wonjune Kang', 'Yun Wang', 'Shun Zhang', 'Arthur Hinsvark', 'Qing He']",
          "year": "2024",
          "summary": "We propose a multi-task learning (MTL) model for jointly performing three tasks that are commonly solved in a text-to-speech (TTS) front-end: text normalization (TN), part-of-speech (POS) tagging, and homograph disambiguation (HD). Our framework utilizes a tree-like structure with a trunk that learns shared representations, followed by separate task-specific heads. We further incorporate a pre-trained language model to utilize its built-in lexical and contextual knowledge, and study how to best use its embeddings so as to most effectively benefit our multi-task model. Through task-wise ablations, we show that our full model trained on all three tasks achieves the strongest overall performance compared to models trained on individual or sub-combinations of tasks, confirming the advantages of our MTL framework. Finally, we introduce a new HD dataset containing a balanced number of sentences in diverse contexts for a variety of homographs and their pronunciations. We demonstrate that incorporating this dataset into training significantly improves HD performance over only using a commonly used, but imbalanced, pre-existing dataset.",
          "primary_category": "cs.CL",
          "Topic": "Multi-task Learning for TTS"
        }
      },
      {
        "id": "360287970189639860",
        "label": "Paper",
        "properties": {
          "title": "LLaMA: Open and Efficient Foundation Language Models",
          "authors": "['Hugo Touvron', 'Thibaut Lavril', 'Gautier Izacard', 'Xavier Martinet', 'Marie-Anne Lachaux', 'Timothée Lacroix', 'Baptiste Rozière', 'Naman Goyal', 'Eric Hambro', 'Faisal Azhar', 'Aurelien Rodriguez', 'Armand Joulin', 'Edouard Grave', 'Guillaume Lample']",
          "year": "2023",
          "summary": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
          "primary_category": "cs.CL",
          "Topic": "Scalable language model performance"
        }
      },
      {
        "id": "360287970189639864",
        "label": "Paper",
        "properties": {
          "title": "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action",
          "authors": "['Zhengyuan Yang', 'Linjie Li', 'Jianfeng Wang', 'Kevin Lin', 'Ehsan Azarnasab', 'Faisal Ahmed', 'Zicheng Liu', 'Ce Liu', 'Michael Zeng', 'Lijuan Wang']",
          "year": "2023",
          "summary": "We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action. In this paper, we define and explore a comprehensive list of advanced vision tasks that are intriguing to solve, but may exceed the capabilities of existing vision and vision-language models. To achieve such advanced visual intelligence, MM-REACT introduces a textual prompt design that can represent text descriptions, textualized spatial coordinates, and aligned file names for dense visual signals such as images and videos. MM-REACT's prompt design allows language models to accept, associate, and process multimodal information, thereby facilitating the synergetic combination of ChatGPT and various vision experts. Zero-shot experiments demonstrate MM-REACT's effectiveness in addressing the specified capabilities of interests and its wide application in different scenarios that require advanced visual understanding. Furthermore, we discuss and compare MM-REACT's system paradigm with an alternative approach that extends language models for multimodal scenarios through joint finetuning. Code, demo, video, and visualization are available at https://multimodal-react.github.io/",
          "primary_category": "cs.CV",
          "Topic": "Multimodal Reasoning and Action"
        }
      },
      {
        "id": "360287970189639867",
        "label": "Paper",
        "properties": {
          "title": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models",
          "authors": "['Paul Röttger', 'Hannah Rose Kirk', 'Bertie Vidgen', 'Giuseppe Attanasio', 'Federico Bianchi', 'Dirk Hovy']",
          "year": "2023",
          "summary": "Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This risk motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way. XSTest comprises 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. We describe XSTest's creation and composition, and then use the test suite to highlight systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models.",
          "primary_category": "cs.CL",
          "Topic": "Exaggerated Safety in Language Models"
        }
      },
      {
        "id": "360287970189639870",
        "label": "Paper",
        "properties": {
          "title": "PAL: Program-aided Language Models",
          "authors": "['Luyu Gao', 'Aman Madaan', 'Shuyan Zhou', 'Uri Alon', 'Pengfei Liu', 'Yiming Yang', 'Jamie Callan', 'Graham Neubig']",
          "year": "2022",
          "summary": "Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (\"few-shot prompting\"\"). Much of this success can be attributed to prompting methods such as \"\"chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .\"",
          "primary_category": "cs.CL",
          "Topic": "Program-Aided Mathematical Reasoning"
        }
      },
      {
        "id": "360287970189639872",
        "label": "Paper",
        "properties": {
          "title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
          "authors": "['Timo Schick', 'Jane Dwivedi-Yu', 'Roberto Dessì', 'Roberta Raileanu', 'Maria Lomeli', 'Luke Zettlemoyer', 'Nicola Cancedda', 'Thomas Scialom']",
          "year": "2023",
          "summary": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",
          "primary_category": "cs.CL",
          "Topic": "Tool Use in Language Models"
        }
      },
      {
        "id": "360287970189639883",
        "label": "Paper",
        "properties": {
          "title": "Improved Baselines with Visual Instruction Tuning",
          "authors": "['Haotian Liu', 'Chunyuan Li', 'Yuheng Li', 'Yong Jae Lee']",
          "year": "2023",
          "summary": "Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available.",
          "primary_category": "cs.CV",
          "Topic": "Multimodal Model Efficiency and Scaling"
        }
      },
      {
        "id": "360287970189639884",
        "label": "Paper",
        "properties": {
          "title": "Efficient Neural Audio Synthesis",
          "authors": "['Nal Kalchbrenner', 'Erich Elsen', 'Karen Simonyan', 'Seb Noury', 'Norman Casagrande', 'Edward Lockhart', 'Florian Stimberg', 'Aaron van den Oord', 'Sander Dieleman', 'Koray Kavukcuoglu']",
          "year": "2018",
          "summary": "Sequential models achieve state-of-the-art results in audio, visual and textual domains with respect to both estimating the data distribution and generating high-quality samples. Efficient sampling for this class of models has however remained an elusive problem. With a focus on text-to-speech synthesis, we describe a set of general techniques for reducing sampling time while maintaining high output quality. We first describe a single-layer recurrent neural network, the WaveRNN, with a dual softmax layer that matches the quality of the state-of-the-art WaveNet model. The compact form of the network makes it possible to generate 24kHz 16-bit audio 4x faster than real time on a GPU. Second, we apply a weight pruning technique to reduce the number of weights in the WaveRNN. We find that, for a constant number of parameters, large sparse networks perform better than small dense networks and this relationship holds for sparsity levels beyond 96%. The small number of weights in a Sparse WaveRNN makes it possible to sample high-fidelity audio on a mobile CPU in real time. Finally, we propose a new generation scheme based on subscaling that folds a long sequence into a batch of shorter sequences and allows one to generate multiple samples at once. The Subscale WaveRNN produces 16 samples per step without loss of quality and offers an orthogonal method for increasing sampling efficiency.",
          "primary_category": "cs.SD",
          "Topic": "Efficient Neural Audio Generation"
        }
      },
      {
        "id": "360287970189639751",
        "label": "Paper",
        "properties": {
          "title": "VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation",
          "authors": "['Tianrui Wang', 'Long Zhou', 'Ziqiang Zhang', 'Yu Wu', 'Shujie Liu', 'Yashesh Gaur', 'Zhuo Chen', 'Jinyu Li', 'Furu Wei']",
          "year": "2023",
          "summary": "Recent research shows a big convergence in model architecture, training objectives, and inference methods across various tasks for different modalities. In this paper, we propose VioLA, a single auto-regressive Transformer decoder-only network that unifies various cross-modal tasks involving speech and text, such as speech-to-text, text-to-text, text-to-speech, and speech-to-speech tasks, as a conditional codec language model task via multi-task learning framework. To accomplish this, we first convert all the speech utterances to discrete tokens (similar to the textual data) using an offline neural codec encoder. In such a way, all these tasks are converted to token-based sequence conversion problems, which can be naturally handled with one conditional language model. We further integrate task IDs (TID) and language IDs (LID) into the proposed model to enhance the modeling capability of handling different languages and tasks. Experimental results demonstrate that the proposed VioLA model can support both single-modal and cross-modal tasks well, and the decoder-only model achieves a comparable and even better performance than the strong baselines.",
          "primary_category": "cs.CL",
          "Topic": "Unified Speech and Text Processing"
        }
      },
      {
        "id": "360287970189639886",
        "label": "Paper",
        "properties": {
          "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
          "authors": "['Anmol Gulati', 'James Qin', 'Chung-Cheng Chiu', 'Niki Parmar', 'Yu Zhang', 'Jiahui Yu', 'Wei Han', 'Shibo Wang', 'Zhengdong Zhang', 'Yonghui Wu', 'Ruoming Pang']",
          "year": "2020",
          "summary": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.",
          "primary_category": "eess.AS",
          "Topic": "Combining Convolution and Self-Attention"
        }
      },
      {
        "id": "360287970189639712",
        "label": "Paper",
        "properties": {
          "title": "Perception Test: A Diagnostic Benchmark for Multimodal Video Models",
          "authors": "['Viorica Pătrăucean', 'Lucas Smaira', 'Ankush Gupta', 'Adrià Recasens Continente', 'Larisa Markeeva', 'Dylan Banarse', 'Skanda Koppula', 'Joseph Heyward', 'Mateusz Malinowski', 'Yi Yang', 'Carl Doersch', 'Tatiana Matejovicova', 'Yury Sulsky', 'Antoine Miech', 'Alex Frechette', 'Hanna Klimczak', 'Raphael Koster', 'Junlin Zhang', 'Stephanie Winkler', 'Yusuf Aytar', 'Simon Osindero', 'Dima Damen', 'Andrew Zisserman', 'João Carreira']",
          "year": "2023",
          "summary": "We propose a novel multimodal video benchmark - the Perception Test - to evaluate the perception and reasoning skills of pre-trained multimodal models (e.g. Flamingo, SeViLA, or GPT-4). Compared to existing benchmarks that focus on computational tasks (e.g. classification, detection or tracking), the Perception Test focuses on skills (Memory, Abstraction, Physics, Semantics) and types of reasoning (descriptive, explanatory, predictive, counterfactual) across video, audio, and text modalities, to provide a comprehensive and efficient evaluation tool. The benchmark probes pre-trained models for their transfer capabilities, in a zero-shot / few-shot or limited finetuning regime. For these purposes, the Perception Test introduces 11.6k real-world videos, 23s average length, designed to show perceptually interesting situations, filmed by around 100 participants worldwide. The videos are densely annotated with six types of labels (multiple-choice and grounded video question-answers, object and point tracks, temporal action and sound segments), enabling both language and non-language evaluations. The fine-tuning and validation splits of the benchmark are publicly available (CC-BY license), in addition to a challenge server with a held-out test split. Human baseline results compared to state-of-the-art video QA models show a substantial gap in performance (91.4% vs 46.2%), suggesting that there is significant room for improvement in multimodal video understanding.   Dataset, baseline code, and challenge server are available at https://github.com/deepmind/perception_test",
          "primary_category": "cs.CV",
          "Topic": "Multimodal Video Perception Benchmark"
        }
      },
      {
        "id": "360287970189639836",
        "label": "Paper",
        "properties": {
          "title": "Gorilla: Large Language Model Connected with Massive APIs",
          "authors": "['Shishir G Patil', 'Tianjun Zhang', 'Xin Wang', 'Joseph E Gonzalez']",
          "year": "2023",
          "summary": "Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu",
          "primary_category": "cs.CL",
          "Topic": "API Integration and Accuracy Enhancement"
        }
      },
      {
        "id": "360287970189639724",
        "label": "Paper",
        "properties": {
          "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models",
          "authors": "['Muhammad Maaz', 'Hanoona Rasheed', 'Salman Khan', 'Fahad Shahbaz Khan']",
          "year": "2023",
          "summary": "Conversation agents fueled by Large Language Models (LLMs) are providing a new way to interact with visual data. While there have been initial attempts for image-based conversation models, this work addresses the under-explored field of \\emph{video-based conversation} by introducing Video-ChatGPT. It is a multimodal model that merges a video-adapted visual encoder with an LLM. The resulting model is capable of understanding and generating detailed conversations about videos. We introduce a new dataset of 100,000 video-instruction pairs used to train Video-ChatGPT acquired via manual and semi-automated pipeline that is easily scalable and robust to label noise. We also develop a quantitative evaluation framework for video-based dialogue models to objectively analyze the strengths and weaknesses of video-based dialogue models. Code: https://github.com/mbzuai-oryx/Video-ChatGPT.",
          "primary_category": "cs.CV",
          "Topic": "Video-based conversational AI"
        }
      },
      {
        "id": "360287970189639897",
        "label": "Paper",
        "properties": {
          "title": "Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning",
          "authors": "['Rohit Girdhar', 'Mannat Singh', 'Andrew Brown', 'Quentin Duval', 'Samaneh Azadi', 'Sai Saketh Rambhatla', 'Akbar Shah', 'Xi Yin', 'Devi Parikh', 'Ishan Misra']",
          "year": "2023",
          "summary": "We present Emu Video, a text-to-video generation model that factorizes the generation into two steps: first generating an image conditioned on the text, and then generating a video conditioned on the text and the generated image. We identify critical design decisions--adjusted noise schedules for diffusion, and multi-stage training that enable us to directly generate high quality and high resolution videos, without requiring a deep cascade of models as in prior work. In human evaluations, our generated videos are strongly preferred in quality compared to all prior work--81% vs. Google's Imagen Video, 90% vs. Nvidia's PYOCO, and 96% vs. Meta's Make-A-Video. Our model outperforms commercial solutions such as RunwayML's Gen2 and Pika Labs. Finally, our factorizing approach naturally lends itself to animating images based on a user's text prompt, where our generations are preferred 96% over prior work.",
          "primary_category": "cs.CV",
          "Topic": "Factorized Text-to-Video Generation"
        }
      },
      {
        "id": "360287970189639700",
        "label": "Paper",
        "properties": {
          "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
          "authors": "['Alexey Dosovitskiy', 'Lucas Beyer', 'Alexander Kolesnikov', 'Dirk Weissenborn', 'Xiaohua Zhai', 'Thomas Unterthiner', 'Mostafa Dehghani', 'Matthias Minderer', 'Georg Heigold', 'Sylvain Gelly', 'Jakob Uszkoreit', 'Neil Houlsby']",
          "year": "2020",
          "summary": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
          "primary_category": "cs.CV",
          "Topic": "Vision Transformers for Image Recognition"
        }
      },
      {
        "id": "360287970189639899",
        "label": "Paper",
        "properties": {
          "title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM",
          "authors": "['Deepak Narayanan', 'Mohammad Shoeybi', 'Jared Casper', 'Patrick LeGresley', 'Mostofa Patwary', 'Vijay Anand Korthikanti', 'Dmitri Vainbrand', 'Prethvi Kashinkunti', 'Julie Bernauer', 'Bryan Catanzaro', 'Amar Phanishayee', 'Matei Zaharia']",
          "year": "2021",
          "summary": "Large language models have led to state-of-the-art accuracies across a range of tasks. However, training these models efficiently is challenging for two reasons: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required to train these models can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to fundamental scaling issues at thousands of GPUs, e.g., due to expensive cross-node communication or devices spending significant time waiting on other devices to make progress.   In this paper, we show how different types of parallelism methods (tensor, pipeline, and data parallelism) can be composed to scale to thousands of GPUs and models with trillions of parameters. We survey techniques for pipeline parallelism and propose a novel interleaved pipeline parallelism schedule that can improve throughput by 10+% with memory footprint comparable to existing approaches. We quantitatively study the trade-offs between tensor, pipeline, and data parallelism, and provide intuition as to how to configure distributed training of a large model. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs with achieved per-GPU throughput of 52% of theoretical peak. Our code is open sourced at https://github.com/nvidia/megatron-lm.",
          "primary_category": "cs.CL",
          "Topic": "Scalable Language Model Training"
        }
      },
      {
        "id": "360287970189639902",
        "label": "Paper",
        "properties": {
          "title": "AudioChatLlama: Towards General-Purpose Speech Abilities for LLMs",
          "authors": "['Yassir Fathullah', 'Chunyang Wu', 'Egor Lakomkin', 'Ke Li', 'Junteng Jia', 'Yuan Shangguan', 'Jay Mahadeokar', 'Ozlem Kalinli', 'Christian Fuegen', 'Mike Seltzer']",
          "year": "2023",
          "summary": "In this work, we extend the instruction-tuned Llama-2 model with end-to-end general-purpose speech processing and reasoning abilities while maintaining the wide range of original LLM capabilities, without using any carefully curated paired data. The resulting end-to-end model, named AudioChatLlama, can utilize audio prompts as a replacement for text and sustain a conversation. Such a model also has extended cross-modal capabilities such as being able to perform spoken question answering (QA), speech translation, and audio summarization amongst many other closed and open-domain tasks. This is unlike prior approaches in speech, in which LLMs are extended to handle audio for a limited number of pre-designated tasks. On both synthesized and recorded speech QA test sets, evaluations show that our end-to-end approach is on par with or outperforms cascaded systems (speech recognizer + LLM) in terms of modeling the response to a prompt. Furthermore, unlike cascades, our approach can interchange text and audio modalities and intrinsically utilize prior context in a conversation to provide better results.",
          "primary_category": "cs.CL",
          "Topic": "Cross-modal speech and text processing"
        }
      },
      {
        "id": "360287970189639909",
        "label": "Paper",
        "properties": {
          "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",
          "authors": "['Qinghao Ye', 'Haiyang Xu', 'Guohai Xu', 'Jiabo Ye', 'Ming Yan', 'Yiyang Zhou', 'Junyang Wang', 'Anwen Hu', 'Pengcheng Shi', 'Yaya Shi', 'Chenliang Li', 'Yuanhong Xu', 'Hehong Chen', 'Junfeng Tian', 'Qi Qian', 'Ji Zhang', 'Fei Huang', 'Jingren Zhou']",
          "year": "2023",
          "summary": "Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.",
          "primary_category": "cs.CL",
          "Topic": "Modular Multimodal Language Models"
        }
      }
    ]
  },
  {
    "category_id": 4,
    "name": "Evaluation and Benchmarking",
    "description": "Papers introducing new benchmarks or evaluating existing models to better understand their performance and limitations.",
    "children": [
      {
        "id": "360287970189639719",
        "label": "Paper",
        "properties": {
          "title": "Measuring Massive Multitask Language Understanding",
          "authors": "['Dan Hendrycks', 'Collin Burns', 'Steven Basart', 'Andy Zou', 'Mantas Mazeika', 'Dawn Song', 'Jacob Steinhardt']",
          "year": "2020",
          "summary": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",
          "primary_category": "cs.CY",
          "Topic": "Massive Multitask Language Understanding"
        }
      },
      {
        "id": "360287970189639720",
        "label": "Paper",
        "properties": {
          "title": "Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models",
          "authors": "['Gowthami Somepalli', 'Vasu Singla', 'Micah Goldblum', 'Jonas Geiping', 'Tom Goldstein']",
          "year": "2022",
          "summary": "Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data.",
          "primary_category": "cs.LG",
          "Topic": "Content Replication in Diffusion Models"
        }
      },
      {
        "id": "360287970189639722",
        "label": "Paper",
        "properties": {
          "title": "MLS: A Large-Scale Multilingual Dataset for Speech Research",
          "authors": "['Vineel Pratap', 'Qiantong Xu', 'Anuroop Sriram', 'Gabriel Synnaeve', 'Ronan Collobert']",
          "year": "2020",
          "summary": "This paper introduces Multilingual LibriSpeech (MLS) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages. Additionally, we provide Language Models (LM) and baseline Automatic Speech Recognition (ASR) models and for all the languages in our dataset. We believe such a large transcribed dataset will open new avenues in ASR and Text-To-Speech (TTS) research. The dataset will be made freely available for anyone at http://www.openslr.org.",
          "primary_category": "eess.AS",
          "Topic": "Multilingual speech dataset development"
        }
      },
      {
        "id": "360287970189639730",
        "label": "Paper",
        "properties": {
          "title": "VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation",
          "authors": "['Changhan Wang', 'Morgane Rivière', 'Ann Lee', 'Anne Wu', 'Chaitanya Talnikar', 'Daniel Haziza', 'Mary Williamson', 'Juan Pino', 'Emmanuel Dupoux']",
          "year": "2021",
          "summary": "We introduce VoxPopuli, a large-scale multilingual corpus providing 100K hours of unlabelled speech data in 23 languages. It is the largest open data to date for unsupervised representation learning as well as semi-supervised learning. VoxPopuli also contains 1.8K hours of transcribed speeches in 16 languages and their aligned oral interpretations into 5 other languages totaling 5.1K hours. We provide speech recognition baselines and validate the versatility of VoxPopuli unlabelled data in semi-supervised learning under challenging out-of-domain settings. We will release the corpus at https://github.com/facebookresearch/voxpopuli under an open license.",
          "primary_category": "cs.CL",
          "Topic": "Multilingual Speech Representation Learning"
        }
      },
      {
        "id": "360287970189639739",
        "label": "Paper",
        "properties": {
          "title": "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark",
          "authors": "['Yubo Wang', 'Xueguang Ma', 'Ge Zhang', 'Yuansheng Ni', 'Abhranil Chandra', 'Shiguang Guo', 'Weiming Ren', 'Aaran Arulraj', 'Xuan He', 'Ziyan Jiang', 'Tianle Li', 'Max Ku', 'Kai Wang', 'Alex Zhuang', 'Rongqi Fan', 'Xiang Yue', 'Wenhu Chen']",
          "year": "2024",
          "summary": "In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. However, as models continue to improve, their performance on these benchmarks has begun to plateau, making it increasingly difficult to discern differences in model capabilities. This paper introduces MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. Additionally, MMLU-Pro eliminates the trivial and noisy questions in MMLU. Our experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16% to 33% compared to MMLU but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5% in MMLU to just 2% in MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering, which is in stark contrast to the findings on the original MMLU, indicating that MMLU-Pro includes more complex reasoning questions. Our assessments confirm that MMLU-Pro is a more discriminative benchmark to better track progress in the field.",
          "primary_category": "cs.CL",
          "Topic": "Challenging Multi-Disciplinary Reasoning"
        }
      },
      {
        "id": "360287970189639690",
        "label": "Paper",
        "properties": {
          "title": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
          "authors": "['Mandar Joshi', 'Eunsol Choi', 'Daniel S Weld', 'Luke Zettlemoyer']",
          "year": "2017",
          "summary": "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- http://nlp.cs.washington.edu/triviaqa/",
          "primary_category": "cs.CL",
          "Topic": "Complex Reading Comprehension Challenges"
        }
      },
      {
        "id": "360287970189639757",
        "label": "Paper",
        "properties": {
          "title": "Quantifying Variance in Evaluation Benchmarks",
          "authors": "['Lovish Madaan', 'Aaditya K Singh', 'Rylan Schaeffer', 'Andrew Poulton', 'Sanmi Koyejo', 'Pontus Stenetorp', 'Sharan Narang', 'Dieuwke Hupkes']",
          "year": "2024",
          "summary": "Evaluation benchmarks are the cornerstone of measuring capabilities of large language models (LLMs), as well as driving progress in said capabilities. Originally designed to make claims about capabilities (or lack thereof) in fully pretrained models, evaluation benchmarks are now also extensively used to decide between various training choices. Despite this widespread usage, we rarely quantify the variance in our evaluation benchmarks, which dictates whether differences in performance are meaningful. Here, we define and measure a range of metrics geared towards measuring variance in evaluation benchmarks, including seed variance across initialisations, and monotonicity during training. By studying a large number of models -- both openly available and pretrained from scratch -- we provide empirical estimates for a variety of variance metrics, with considerations and recommendations for practitioners. We also evaluate the utility and tradeoffs of continuous versus discrete performance measures and explore options for better understanding and reducing this variance. We find that simple changes, such as framing choice tasks (like MMLU) as completion tasks, can often reduce variance for smaller scale ($\\sim$7B) models, while more involved methods inspired from human testing literature (such as item analysis and item response theory) struggle to meaningfully reduce variance. Overall, our work provides insights into variance in evaluation benchmarks, suggests LM-specific techniques to reduce variance, and more generally encourages practitioners to carefully factor in variance when comparing models.",
          "primary_category": "cs.LG",
          "Topic": "Variance in Evaluation Benchmarks"
        }
      },
      {
        "id": "360287970189639767",
        "label": "Paper",
        "properties": {
          "title": "Proximal Policy Optimization Algorithms",
          "authors": "['John Schulman', 'Filip Wolski', 'Prafulla Dhariwal', 'Alec Radford', 'Oleg Klimov']",
          "year": "2017",
          "summary": "We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\"\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.\"",
          "primary_category": "cs.LG",
          "Topic": "Proximal Policy Optimization"
        }
      },
      {
        "id": "360287970189639774",
        "label": "Paper",
        "properties": {
          "title": "Reducing Activation Recomputation in Large Transformer Models",
          "authors": "['Vijay Korthikanti', 'Jared Casper', 'Sangkug Lym', 'Lawrence McAfee', 'Michael Andersch', 'Mohammad Shoeybi', 'Bryan Catanzaro']",
          "year": "2022",
          "summary": "Training large transformer models is one of the most important computational challenges of modern AI. In this paper, we show how to significantly accelerate training of large transformer models by reducing activation recomputation. Activation recomputation is commonly used to work around memory capacity constraints. Rather than storing activations for backpropagation, they are traditionally recomputed, which saves memory but adds redundant compute. In this work, we show most of this redundant compute is unnecessary because we can reduce memory consumption sufficiently without it. We present two novel yet very simple techniques: sequence parallelism and selective activation recomputation. In conjunction with tensor parallelism, these techniques almost eliminate the need to recompute activations. We evaluate our approach on language models up to one trillion parameters in scale and show that our method reduces activation memory by 5x, while reducing execution time overhead from activation recomputation by over 90%. For example, when training a 530B parameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops Utilization of 54.2%, which is 29% faster than the 42.1% we achieve using recomputation. Our implementation will be available in both Megatron-LM and NeMo-Megatron.",
          "primary_category": "cs.LG",
          "Topic": "Selective Activation Recomputation"
        }
      },
      {
        "id": "360287970189639775",
        "label": "Paper",
        "properties": {
          "title": "SemDeDup: Data-efficient learning at web-scale through semantic deduplication",
          "authors": "['Amro Abbas', 'Kushal Tirumala', 'Dániel Simig', 'Surya Ganguli', 'Ari S Morcos']",
          "year": "2023",
          "summary": "Progress in machine learning has been driven in large part by massive increases in data. However, large web-scale datasets such as LAION are largely uncurated beyond searches for exact duplicates, potentially leaving much redundancy. Here, we introduce SemDeDup, a method which leverages embeddings from pre-trained models to identify and remove semantic duplicates: data pairs which are semantically similar, but not exactly identical. Removing semantic duplicates preserves performance and speeds up learning. Analyzing a subset of LAION, we show that SemDeDup can remove 50% of the data with minimal performance loss, effectively halving training time. Moreover, performance increases out of distribution. Also, analyzing language models trained on C4, a partially curated dataset, we show that SemDeDup improves over prior approaches while providing efficiency gains. SemDeDup provides an example of how simple ways of leveraging quality embeddings can be used to make models learn faster with less data.",
          "primary_category": "cs.LG",
          "Topic": "Semantic Deduplication in Web-Scale Data"
        }
      },
      {
        "id": "360287970189639776",
        "label": "Paper",
        "properties": {
          "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
          "authors": "['Victor Sanh', 'Lysandre Debut', 'Julien Chaumond', 'Thomas Wolf']",
          "year": "2019",
          "summary": "As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.",
          "primary_category": "cs.CL",
          "Topic": "Smaller, faster, distilled language models."
        }
      },
      {
        "id": "360287970189639780",
        "label": "Paper",
        "properties": {
          "title": "Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?",
          "authors": "['Aaditya K Singh', 'Muhammed Yusuf Kocyigit', 'Andrew Poulton', 'David Esiobu', 'Maria Lomeli', 'Gergely Szilvasy', 'Dieuwke Hupkes']",
          "year": "2024",
          "summary": "Hampering the interpretation of benchmark scores, evaluation data contamination has become a growing concern in the evaluation of LLMs, and an active area of research studies its effects. While evaluation data contamination is easily understood intuitively, it is surprisingly difficult to define precisely which samples should be considered contaminated and, consequently, how it impacts benchmark scores. We propose that these questions should be addressed together and that contamination metrics can be assessed based on whether models benefit from the examples they mark contaminated. We propose a novel analysis method called ConTAM, and show with a large scale survey of existing and novel n-gram based contamination metrics across 13 benchmarks and 7 models from 2 different families that ConTAM can be used to better understand evaluation data contamination and its effects. We find that contamination may have a much larger effect than reported in recent LLM releases and benefits models differently at different scales. We also find that considering only the longest contaminated substring provides a better signal than considering a union of all contaminated substrings, and that doing model and benchmark specific threshold analysis greatly increases the specificity of the results. Lastly, we investigate the impact of hyperparameter choices, finding that, among other things, both using larger values of n and disregarding matches that are infrequent in the pre-training data lead to many false negatives. With ConTAM, we provide a method to empirically ground evaluation data contamination metrics in downstream effects. With our exploration, we shed light on how evaluation data contamination can impact LLMs and provide insight into the considerations important when doing contamination analysis. We end our paper by discussing these in more detail and providing concrete suggestions for future work.",
          "primary_category": "cs.CL",
          "Topic": "Evaluation Data Contamination in LLMs"
        }
      },
      {
        "id": "360287970189639781",
        "label": "Paper",
        "properties": {
          "title": "Billion-scale similarity search with GPUs",
          "authors": "['Jeff Johnson', 'Matthijs Douze', 'Hervé Jégou']",
          "year": "2017",
          "summary": "Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy.   We propose a design for k-selection that operates at up to 55% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art. We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.",
          "primary_category": "cs.CV",
          "Topic": "Billion-scale similarity search with GPUs."
        }
      },
      {
        "id": "360287970189639783",
        "label": "Paper",
        "properties": {
          "title": "When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards",
          "authors": "['Norah Alzahrani', 'Hisham Abdullah Alyahya', 'Yazeed Alnumay', 'Sultan Alrashed', 'Shaykhah Alsubaie', 'Yusef Almushaykeh', 'Faisal Mirza', 'Nouf Alotaibi', 'Nora Altwairesh', 'Areeb Alowisheq', 'M Saiful Bari', 'Haidar Khan']",
          "year": "2024",
          "summary": "Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value - we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple-choice question benchmarks (e.g., MMLU), minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a hybrid scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust evaluation schemes on the existing benchmarks. The code for this paper is available at https://github.com/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness.",
          "primary_category": "cs.CL",
          "Topic": "Sensitivity of LLM Leaderboards"
        }
      },
      {
        "id": "360287970189639784",
        "label": "Paper",
        "properties": {
          "title": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models",
          "authors": "['Samyam Rajbhandari', 'Jeff Rasley', 'Olatunji Ruwase', 'Yuxiong He']",
          "year": "2019",
          "summary": "Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware.   We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create the world's largest language model (Turing-NLG, 17B parameters) with record breaking accuracy.",
          "primary_category": "cs.LG",
          "Topic": "Memory-efficient large model training"
        }
      },
      {
        "id": "360287970189639791",
        "label": "Paper",
        "properties": {
          "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data",
          "authors": "['Guillaume Wenzek', 'Marie-Anne Lachaux', 'Alexis Conneau', 'Vishrav Chaudhary', 'Francisco Guzmán', 'Armand Joulin', 'Edouard Grave']",
          "year": "2019",
          "summary": "Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia.",
          "primary_category": "cs.CL",
          "Topic": "High-quality web data extraction"
        }
      },
      {
        "id": "360287970189639795",
        "label": "Paper",
        "properties": {
          "title": "CoVoST 2 and Massively Multilingual Speech-to-Text Translation",
          "authors": "['Changhan Wang', 'Anne Wu', 'Juan Pino']",
          "year": "2020",
          "summary": "Speech translation has recently become an increasingly popular topic of research, partly due to the development of benchmark datasets. Nevertheless, current datasets cover a limited number of languages. With the aim to foster research in massive multilingual speech translation and speech translation for low resource language pairs, we release CoVoST 2, a large-scale multilingual speech translation corpus covering translations from 21 languages into English and from English into 15 languages. This represents the largest open dataset available to date from total volume and language coverage perspective. Data sanity checks provide evidence about the quality of the data, which is released under CC0 license. We also provide extensive speech recognition, bilingual and multilingual machine translation and speech translation baselines with open-source implementation.",
          "primary_category": "cs.CL",
          "Topic": "Massively Multilingual Speech Translation"
        }
      },
      {
        "id": "360287970189639798",
        "label": "Paper",
        "properties": {
          "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms",
          "authors": "['Aida Amini', 'Saadia Gabriel', 'Peter Lin', 'Rik Koncel-Kedziorski', 'Yejin Choi', 'Hannaneh Hajishirzi']",
          "year": "2019",
          "summary": "We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver that learns to map problems to operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model precise operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, our new dataset, MathQA, significantly enhances the AQuA dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model enhanced with automatic problem categorization. Our experiments show improvements over competitive baselines in our MathQA as well as the AQuA dataset. The results are still significantly lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at: https://math-qa.github.io/math-QA/",
          "primary_category": "cs.CL",
          "Topic": "Math Word Problem Solving"
        }
      },
      {
        "id": "360287970189639790",
        "label": "Paper",
        "properties": {
          "title": "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations",
          "authors": "['Hakan Inan', 'Kartikeya Upasani', 'Jianfeng Chi', 'Rashi Rungta', 'Krithika Iyer', 'Yuning Mao', 'Michael Tontchev', 'Qing Hu', 'Brian Fuller', 'Davide Testuggine', 'Madian Khabsa']",
          "year": "2023",
          "summary": "We introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI conversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing a specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is also instrumental in classifying the responses generated by LLMs to these prompts, a process we refer to as response classification. For the purpose of both prompt and response classification, we have meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is instruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on existing benchmarks such as the OpenAI Moderation Evaluation dataset and ToxicChat, where its performance matches or exceeds that of currently available content moderation tools. Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores. Furthermore, the instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats. This feature enhances the model's capabilities, such as enabling the adjustment of taxonomy categories to align with specific use cases, and facilitating zero-shot or few-shot prompting with diverse taxonomies at the input. We are making Llama Guard model weights available and we encourage researchers to further develop and adapt them to meet the evolving needs of the community for AI safety.",
          "primary_category": "cs.CL",
          "Topic": "LLM-based Safety Risk Classification"
        }
      },
      {
        "id": "360287970189639800",
        "label": "Paper",
        "properties": {
          "title": "Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale",
          "authors": "['Matthew Le', 'Apoorv Vyas', 'Bowen Shi', 'Brian Karrer', 'Leda Sari', 'Rashel Moritz', 'Mary Williamson', 'Vimal Manohar', 'Yossi Adi', 'Jay Mahadeokar', 'Wei-Ning Hsu']",
          "year": "2023",
          "summary": "Large-scale generative models such as GPT and DALL-E have revolutionized the research community. These models not only generate high fidelity outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are not filtered or enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox outperforms the state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs 1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to 20 times faster. Audio samples can be found in \\url{https://voicebox.metademolab.com}.",
          "primary_category": "eess.AS",
          "Topic": "Text-guided speech generation"
        }
      },
      {
        "id": "360287970189639803",
        "label": "Paper",
        "properties": {
          "title": "Training Deep Neural Networks with Joint Quantization and Pruning of Weights and Activations",
          "authors": "['Xinyu Zhang', 'Ian Colbert', 'Ken Kreutz-Delgado', 'Srinjoy Das']",
          "year": "2021",
          "summary": "Quantization and pruning are core techniques used to reduce the inference costs of deep neural networks. State-of-the-art quantization techniques are currently applied to both the weights and activations; however, pruning is most often applied to only the weights of the network. In this work, we jointly apply novel uniform quantization and unstructured pruning methods to both the weights and activations of deep neural networks during training. Using our methods, we empirically evaluate the currently accepted prune-then-quantize paradigm across a wide range of computer vision tasks and observe a non-commutative nature when applied to both the weights and activations of deep neural networks. Informed by these observations, we articulate the non-commutativity hypothesis: for a given deep neural network being trained for a specific task, there exists an exact training schedule in which quantization and pruning can be introduced to optimize network performance. We identify that this optimal ordering not only exists, but also varies across discriminative and generative tasks. Using the optimal training schedule within our training framework, we demonstrate increased performance per memory footprint over existing solutions.",
          "primary_category": "cs.LG",
          "Topic": "Joint quantization and pruning techniques"
        }
      },
      {
        "id": "360287970189639806",
        "label": "Paper",
        "properties": {
          "title": "Gender Bias in Machine Translation",
          "authors": "['Beatrice Savoldi', 'Marco Gaido', 'Luisa Bentivogli', 'Matteo Negri', 'Marco Turchi']",
          "year": "2021",
          "summary": "Machine translation (MT) technology has facilitated our daily tasks by providing accessible shortcuts for gathering, elaborating and communicating information. However, it can suffer from biases that harm users and society at large. As a relatively new field of inquiry, gender bias in MT still lacks internal cohesion, which advocates for a unified framework to ease future research. To this end, we: i) critically review current conceptualizations of bias in light of theoretical insights from related disciplines, ii) summarize previous analyses aimed at assessing gender bias in MT, iii) discuss the mitigating strategies proposed so far, and iv) point toward potential directions for future work.",
          "primary_category": "cs.CL",
          "Topic": "Gender bias in machine translation."
        }
      },
      {
        "id": "360287970189639808",
        "label": "Paper",
        "properties": {
          "title": "Mixture-of-Experts with Expert Choice Routing",
          "authors": "['Yanqi Zhou', 'Tao Lei', 'Hanxiao Liu', 'Nan Du', 'Yanping Huang', 'Vincent Zhao', 'Andrew Dai', 'Zhifeng Chen', 'Quoc Le', 'James Laudon']",
          "year": "2022",
          "summary": "Sparsely-activated Mixture-of-experts (MoE) models allow the number of parameters to greatly increase while keeping the amount of computation for a given token or a given sample unchanged. However, a poor expert routing strategy (e.g. one resulting in load imbalance) can cause certain experts to be under-trained, leading to an expert being under or over-specialized. Prior work allocates a fixed number of experts to each token using a top-k function regardless of the relative importance of different tokens. To address this, we propose a heterogeneous mixture-of-experts employing an expert choice method. Instead of letting tokens select the top-k experts, we have experts selecting the top-k tokens. As a result, each token can be routed to a variable number of experts and each expert can have a fixed bucket size. We systematically study pre-training speedups using the same computational resources of the Switch Transformer top-1 and GShard top-2 gating of prior work and find that our method improves training convergence time by more than 2x. For the same computational cost, our method demonstrates higher performance in fine-tuning 11 selected tasks in the GLUE and SuperGLUE benchmarks. For a smaller activation cost, our method outperforms the T5 dense model in 7 out of the 11 tasks.",
          "primary_category": "cs.LG",
          "Topic": "Expert Choice Routing in MoE"
        }
      },
      {
        "id": "360287970189639809",
        "label": "Paper",
        "properties": {
          "title": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models",
          "authors": "['Chenxin An', 'Shansan Gong', 'Ming Zhong', 'Xingjian Zhao', 'Mukai Li', 'Jun Zhang', 'Lingpeng Kong', 'Xipeng Qiu']",
          "year": "2023",
          "summary": "Recently, there has been growing interest in extending the context length of large language models (LLMs), aiming to effectively process long inputs of one turn or conversations with more extensive histories. While proprietary models such as GPT-4 and Claude can largely preserve the reasoning ability in an extended context, open-source models are still progressing through the early stages of development. To bridge this gap, we propose L-Eval to institute a more standardized evaluation for long context language models (LCLMs) addressing two key aspects: dataset construction and evaluation metrics. On the one hand, we build a new evaluation suite containing 20 sub-tasks, 508 long documents, and over 2,000 human-labeled query-response pairs encompassing diverse question styles, domains, and input length (3k$\\sim$200k tokens). On the other hand, we investigate the effectiveness in evalution metrics for LCLMs. Results show that popular n-gram matching metrics generally can not correlate well with human judgment, and thus we strongly advocate for length-instruction-enhanced (LIE) evaluation and employing LLM judges. We conducted a comprehensive study of 4 popular commercial LLMs and 12 open-source counterparts using the L-Eval benchmark. Our empirical findings offer useful insights into the study of LCLMs and lay the groundwork for the development of more principled evaluation of these models.",
          "primary_category": "cs.CL",
          "Topic": "Standardized Evaluation for LCLMs"
        }
      },
      {
        "id": "360287970189639811",
        "label": "Paper",
        "properties": {
          "title": "Learning From Mistakes Makes LLM Better Reasoner",
          "authors": "['Shengnan An', 'Zexiong Ma', 'Zeqi Lin', 'Nanning Zheng', 'Jian-Guang Lou', 'Weizhu Chen']",
          "year": "2023",
          "summary": "Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve their reasoning capabilities, this work explores whether LLMs can LEarn from MistAkes (LEMA), akin to the human learning process. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LEMA incorporates mistake-correction data pairs during fine-tuning LLMs. Specifically, we first collect inaccurate reasoning paths from various LLMs, and then employ GPT-4 as a ''corrector'' to identify the mistake step, explain the reason for the mistake, correct the mistake and generate the final answer. In addition, we apply a correction-centric evolution strategy that effectively expands the question set for generating correction data. Experiments across various LLMs and reasoning tasks show that LEMA effectively improves CoT-alone fine-tuning. Our further ablations shed light on the non-homogeneous effectiveness between CoT data and correction data. These results suggest a significant potential for LLMs to improve through learning from their mistakes. Our code, models and prompts are publicly available at https://github.com/microsoft/LEMA.",
          "primary_category": "cs.CL",
          "Topic": "Learning from Mistakes in LLMs"
        }
      },
      {
        "id": "360287970189639797",
        "label": "Paper",
        "properties": {
          "title": "Training Verifiers to Solve Math Word Problems",
          "authors": "['Karl Cobbe', 'Vineet Kosaraju', 'Mohammad Bavarian', 'Mark Chen', 'Heewoo Jun', 'Lukasz Kaiser', 'Matthias Plappert', 'Jerry Tworek', 'Jacob Hilton', 'Reiichiro Nakano', 'Christopher Hesse', 'John Schulman']",
          "year": "2021",
          "summary": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
          "primary_category": "cs.LG",
          "Topic": "Mathematical Reasoning with Verifiers"
        }
      },
      {
        "id": "360287970189639817",
        "label": "Paper",
        "properties": {
          "title": "Changing Answer Order Can Decrease MMLU Accuracy",
          "authors": "['Vipul Gupta', 'David Pantoja', 'Candace Ross', 'Adina Williams', 'Megan Ung']",
          "year": "2024",
          "summary": "As large language models (LLMs) have grown in prevalence, particular benchmarks have become essential for the evaluation of these models and for understanding model capabilities. Most commonly, we use test accuracy averaged across multiple subtasks in order to rank models on leaderboards, to determine which model is best for our purposes. In this paper, we investigate the robustness of the accuracy measurement on a widely used multiple choice question answering dataset, MMLU. When shuffling the answer label contents, we find that all explored models decrease in accuracy on MMLU, but not every model is equally sensitive. These findings suggest a possible adjustment to the standard practice of leaderboard testing, where we additionally consider the percentage of examples each model answers correctly by random chance.",
          "primary_category": "cs.CL",
          "Topic": "Robustness of Model Accuracy"
        }
      },
      {
        "id": "360287970189639821",
        "label": "Paper",
        "properties": {
          "title": "Robust Speech Recognition via Large-Scale Weak Supervision",
          "authors": "['Alec Radford', 'Jong Wook Kim', 'Tao Xu', 'Greg Brockman', 'Christine McLeavey', 'Ilya Sutskever']",
          "year": "2022",
          "summary": "We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",
          "primary_category": "eess.AS",
          "Topic": "Large-scale weakly supervised speech recognition"
        }
      },
      {
        "id": "360287970189639826",
        "label": "Paper",
        "properties": {
          "title": "PIQA: Reasoning about Physical Commonsense in Natural Language",
          "authors": "['Yonatan Bisk', 'Rowan Zellers', 'Ronan Le Bras', 'Jianfeng Gao', 'Yejin Choi']",
          "year": "2019",
          "summary": "To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to today's natural language understanding systems. While recent pretrained models (such as BERT) have made progress on question answering over more abstract domains - such as news articles and encyclopedia entries, where text is plentiful - in more physical domains, text is inherently limited due to reporting bias. Can AI systems learn to reliably answer physical common-sense questions without experiencing the physical world? In this paper, we introduce the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA. Though humans find the dataset easy (95% accuracy), large pretrained models struggle (77%). We provide analysis about the dimensions of knowledge that existing models lack, which offers significant opportunities for future research.",
          "primary_category": "cs.CL",
          "Topic": "Physical Commonsense Reasoning"
        }
      },
      {
        "id": "360287970189639827",
        "label": "Paper",
        "properties": {
          "title": "DocVQA: A Dataset for VQA on Document Images",
          "authors": "['Minesh Mathew', 'Dimosthenis Karatzas', 'C V Jawahar']",
          "year": "2020",
          "summary": "We present a new dataset for Visual Question Answering (VQA) on document images called DocVQA. The dataset consists of 50,000 questions defined on 12,000+ document images. Detailed analysis of the dataset in comparison with similar datasets for VQA and reading comprehension is presented. We report several baseline results by adopting existing VQA and reading comprehension models. Although the existing models perform reasonably well on certain types of questions, there is large performance gap compared to human performance (94.36% accuracy). The models need to improve specifically on questions where understanding structure of the document is crucial. The dataset, code and leaderboard are available at docvqa.org",
          "primary_category": "cs.CV",
          "Topic": "Document Visual Question Answering"
        }
      },
      {
        "id": "360287970189639828",
        "label": "Paper",
        "properties": {
          "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism",
          "authors": "['Yanping Huang', 'Youlong Cheng', 'Ankur Bapna', 'Orhan Firat', 'Mia Xu Chen', 'Dehao Chen', 'HyoukJoong Lee', 'Jiquan Ngiam', 'Quoc V Le', 'Yonghui Wu', 'Zhifeng Chen']",
          "year": "2018",
          "summary": "Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.",
          "primary_category": "cs.CV",
          "Topic": "Scalable Deep Learning Infrastructure"
        }
      },
      {
        "id": "360287970189639835",
        "label": "Paper",
        "properties": {
          "title": "Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models",
          "authors": "['Manish Bhatt', 'Sahana Chennabasappa', 'Cyrus Nikolaidis', 'Shengye Wan', 'Ivan Evtimov', 'Dominik Gabi', 'Daniel Song', 'Faizan Ahmad', 'Cornelius Aschermann', 'Lorenzo Fontana', 'Sasha Frolov', 'Ravi Prakash Giri', 'Dhaval Kapil', 'Yiannis Kozyrakis', 'David LeBlanc', 'James Milazzo', 'Aleksandar Straumann', 'Gabriel Synnaeve', 'Varun Vontimitta', 'Spencer Whitman', 'Joshua Saxe']",
          "year": "2023",
          "summary": "This paper presents CyberSecEval, a comprehensive benchmark developed to help bolster the cybersecurity of Large Language Models (LLMs) employed as coding assistants. As what we believe to be the most extensive unified cybersecurity safety benchmark to date, CyberSecEval provides a thorough evaluation of LLMs in two crucial security domains: their propensity to generate insecure code and their level of compliance when asked to assist in cyberattacks. Through a case study involving seven models from the Llama 2, Code Llama, and OpenAI GPT large language model families, CyberSecEval effectively pinpointed key cybersecurity risks. More importantly, it offered practical insights for refining these models. A significant observation from the study was the tendency of more advanced models to suggest insecure code, highlighting the critical need for integrating security considerations in the development of sophisticated LLMs. CyberSecEval, with its automated test case generation and evaluation pipeline covers a broad scope and equips LLM designers and researchers with a tool to broadly measure and enhance the cybersecurity safety properties of LLMs, contributing to the development of more secure AI systems.",
          "primary_category": "cs.CR",
          "Topic": "LLM cybersecurity evaluation"
        }
      },
      {
        "id": "360287970189639799",
        "label": "Paper",
        "properties": {
          "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering",
          "authors": "['Todor Mihaylov', 'Peter Clark', 'Tushar Khot', 'Ashish Sabharwal']",
          "year": "2018",
          "summary": "We present a new kind of question answering dataset, OpenBookQA, modeled after open book exams for assessing human understanding of a subject. The open book that comes with our questions is a set of 1329 elementary level science facts. Roughly 6000 questions probe an understanding of these facts and their application to novel situations. This requires combining an open book fact (e.g., metals conduct electricity) with broad common knowledge (e.g., a suit of armor is made of metal) obtained from other sources. While existing QA datasets over documents or knowledge bases, being generally self-contained, focus on linguistic understanding, OpenBookQA probes a deeper understanding of both the topic---in the context of common knowledge---and the language it is expressed in. Human performance on OpenBookQA is close to 92%, but many state-of-the-art pre-trained QA methods perform surprisingly poorly, worse than several simple neural baselines we develop. Our oracle experiments designed to circumvent the knowledge retrieval bottleneck demonstrate the value of both the open book and additional facts. We leave it as a challenge to solve the retrieval problem in this multi-hop setting and to close the large gap to human performance.",
          "primary_category": "cs.CL",
          "Topic": "Open Book Question Answering"
        }
      },
      {
        "id": "360287970189639841",
        "label": "Paper",
        "properties": {
          "title": "Deduplicating Training Data Makes Language Models Better",
          "authors": "['Katherine Lee', 'Daphne Ippolito', 'Andrew Nystrom', 'Chiyuan Zhang', 'Douglas Eck', 'Chris Callison-Burch', 'Nicholas Carlini']",
          "year": "2021",
          "summary": "We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings. As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data. We develop two tools that allow us to deduplicate training datasets -- for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times. Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer train steps to achieve the same or better accuracy. We can also reduce train-test overlap, which affects over 4% of the validation set of standard datasets, thus allowing for more accurate evaluation. We release code for reproducing our work and performing dataset deduplication at https://github.com/google-research/deduplicate-text-datasets.",
          "primary_category": "cs.CL",
          "Topic": "Deduplication in Language Model Datasets"
        }
      },
      {
        "id": "360287970189639842",
        "label": "Paper",
        "properties": {
          "title": "Orca-Math: Unlocking the potential of SLMs in Grade School Math",
          "authors": "['Arindam Mitra', 'Hamed Khanpour', 'Corby Rosset', 'Ahmed Awadallah']",
          "year": "2024",
          "summary": "Mathematical word problem-solving has long been recognized as a complex task for small language models (SLMs). A recent study hypothesized that the smallest model size, needed to achieve over 80% accuracy on the GSM8K benchmark, is 34 billion parameters. To reach this level of performance with smaller models, researcher often train SLMs to generate Python code or use tools to help avoid calculation errors. Additionally, they employ ensembling, where outputs of up to 100 model runs are combined to arrive at a more accurate result. Result selection is done using consensus, majority vote or a separate a verifier model used in conjunction with the SLM. Ensembling provides a substantial boost in accuracy but at a significant cost increase with multiple calls to the model (e.g., Phi-GSM uses top-48 to boost the performance from 68.2 to 81.5).   In this work, we present Orca-Math, a 7-billion-parameter SLM based on the Mistral-7B, which achieves 86.81% on GSM8k without the need for multiple model calls or the use of verifiers, code execution or any other external tools. Our approach has the following key elements: (1) A high quality synthetic dataset of 200K math problems created using a multi-agent setup where agents collaborate to create the data, (2) An iterative learning techniques that enables the SLM to practice solving problems, receive feedback on its solutions and learn from preference pairs incorporating the SLM solutions and the feedback. When trained with Supervised Fine-Tuning alone, Orca-Math achieves 81.50% on GSM8k pass@1 metric. With iterative preference learning, Orca-Math achieves 86.81% pass@1. Orca-Math surpasses the performance of significantly larger models such as LLAMA-2-70B, WizardMath-70B, Gemini-Pro, ChatGPT-3.5. It also significantly outperforms other smaller models while using much smaller data (hundreds of thousands vs. millions of problems).",
          "primary_category": "cs.CL",
          "Topic": "Mathematical Reasoning in SLMs"
        }
      },
      {
        "id": "360287970189639844",
        "label": "Paper",
        "properties": {
          "title": "Emergent Abilities of Large Language Models",
          "authors": "['Jason Wei', 'Yi Tay', 'Rishi Bommasani', 'Colin Raffel', 'Barret Zoph', 'Sebastian Borgeaud', 'Dani Yogatama', 'Maarten Bosma', 'Denny Zhou', 'Donald Metzler', 'Ed H Chi', 'Tatsunori Hashimoto', 'Oriol Vinyals', 'Percy Liang', 'Jeff Dean', 'William Fedus']",
          "year": "2022",
          "summary": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.",
          "primary_category": "cs.CL",
          "Topic": "Emergent abilities in scaling."
        }
      },
      {
        "id": "360287970189639846",
        "label": "Paper",
        "properties": {
          "title": "Dynabench: Rethinking Benchmarking in NLP",
          "authors": "['Douwe Kiela', 'Max Bartolo', 'Yixin Nie', 'Divyansh Kaushik', 'Atticus Geiger', 'Zhengxuan Wu', 'Bertie Vidgen', 'Grusha Prasad', 'Amanpreet Singh', 'Pratik Ringshia', 'Zhiyi Ma', 'Tristan Thrush', 'Sebastian Riedel', 'Zeerak Waseem', 'Pontus Stenetorp', 'Robin Jia', 'Mohit Bansal', 'Christopher Potts', 'Adina Williams']",
          "year": "2021",
          "summary": "We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.",
          "primary_category": "cs.CL",
          "Topic": "Dynamic NLP Benchmarking Platform"
        }
      },
      {
        "id": "360287970189639857",
        "label": "Paper",
        "properties": {
          "title": "Bilateral Multi-Perspective Matching for Natural Language Sentences",
          "authors": "['Zhiguo Wang', 'Wael Hamza', 'Radu Florian']",
          "year": "2017",
          "summary": "Natural language sentence matching is a fundamental technology for a variety of tasks. Previous approaches either match sentences from a single direction or only apply single granular (word-by-word or sentence-by-sentence) matching. In this work, we propose a bilateral multi-perspective matching (BiMPM) model under the \"matching-aggregation\"\" framework. Given two sentences $P$ and $Q$, our model first encodes them with a BiLSTM encoder. Next, we match the two encoded sentences in two directions $P \\rightarrow Q$ and $P \\leftarrow Q$. In each matching direction, each time step of one sentence is matched against all time-steps of the other sentence from multiple perspectives. Then, another BiLSTM layer is utilized to aggregate the matching results into a fix-length matching vector. Finally, based on the matching vector, the decision is made through a fully connected layer. We evaluate our model on three tasks: paraphrase identification, natural language inference and answer sentence selection. Experimental results on standard benchmark datasets show that our model achieves the state-of-the-art performance on all tasks.\"",
          "primary_category": "cs.AI",
          "Topic": "Bilateral Multi-Perspective Matching"
        }
      },
      {
        "id": "360287970189639744",
        "label": "Paper",
        "properties": {
          "title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
          "authors": "['Wenhu Chen', 'Xueguang Ma', 'Xinyi Wang', 'William W Cohen']",
          "year": "2022",
          "summary": "Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\\% across all the evaluated datasets. By combining PoT with self-consistency decoding, we can achieve SoTA performance on all math problem datasets and near-SoTA performance on financial datasets. All of our data and code are released in Github https://github.com/wenhuchen/Program-of-Thoughts",
          "primary_category": "cs.CL",
          "Topic": "Program of Thoughts for Numerical Reasoning"
        }
      },
      {
        "id": "360287970189639869",
        "label": "Paper",
        "properties": {
          "title": "Scaling Instruction-Finetuned Language Models",
          "authors": "['Hyung Won Chung', 'Le Hou', 'Shayne Longpre', 'Barret Zoph', 'Yi Tay', 'William Fedus', 'Yunxuan Li', 'Xuezhi Wang', 'Mostafa Dehghani', 'Siddhartha Brahma', 'Albert Webson', 'Shixiang Shane Gu', 'Zhuyun Dai', 'Mirac Suzgun', 'Xinyun Chen', 'Aakanksha Chowdhery', 'Alex Castro-Ros', 'Marie Pellat', 'Kevin Robinson', 'Dasha Valter', 'Sharan Narang', 'Gaurav Mishra', 'Adams Yu', 'Vincent Zhao', 'Yanping Huang', 'Andrew Dai', 'Hongkun Yu', 'Slav Petrov', 'Ed H Chi', 'Jeff Dean', 'Jacob Devlin', 'Adam Roberts', 'Denny Zhou', 'Quoc V Le', 'Jason Wei']",
          "year": "2022",
          "summary": "Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.",
          "primary_category": "cs.LG",
          "Topic": "Instruction Finetuning Scaling"
        }
      },
      {
        "id": "360287970189639880",
        "label": "Paper",
        "properties": {
          "title": "Leveraging Large Language Models for Multiple Choice Question Answering",
          "authors": "['Joshua Robinson', 'Christopher Michael Rytting', 'David Wingate']",
          "year": "2022",
          "summary": "While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g., \"A\"\") associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the LLM it is used with must be able to associate answer options with the symbols that represent them. The LLM needs what we term multiple choice symbol binding (MCSB) ability. This ability varies greatly by model. We show that a model with high MCSB ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been previously underestimated.\"",
          "primary_category": "cs.CL",
          "Topic": "Multiple Choice Question Answering"
        }
      },
      {
        "id": "360287970189639885",
        "label": "Paper",
        "properties": {
          "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
          "authors": "['Pranav Rajpurkar', 'Jian Zhang', 'Konstantin Lopyrev', 'Percy Liang']",
          "year": "2016",
          "summary": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research.   The dataset is freely available at https://stanford-qa.com",
          "primary_category": "cs.CL",
          "Topic": "Machine Reading Comprehension"
        }
      },
      {
        "id": "360287970189639888",
        "label": "Paper",
        "properties": {
          "title": "PaLM: Scaling Language Modeling with Pathways",
          "authors": "['Aakanksha Chowdhery', 'Sharan Narang', 'Jacob Devlin', 'Maarten Bosma', 'Gaurav Mishra', 'Adam Roberts', 'Paul Barham', 'Hyung Won Chung', 'Charles Sutton', 'Sebastian Gehrmann', 'Parker Schuh', 'Kensen Shi', 'Sasha Tsvyashchenko', 'Joshua Maynez', 'Abhishek Rao', 'Parker Barnes', 'Yi Tay', 'Noam Shazeer', 'Vinodkumar Prabhakaran', 'Emily Reif', 'Nan Du', 'Ben Hutchinson', 'Reiner Pope', 'James Bradbury', 'Jacob Austin', 'Michael Isard', 'Guy Gur-Ari', 'Pengcheng Yin', 'Toju Duke', 'Anselm Levskaya', 'Sanjay Ghemawat', 'Sunipa Dev', 'Henryk Michalewski', 'Xavier Garcia', 'Vedant Misra', 'Kevin Robinson', 'Liam Fedus', 'Denny Zhou', 'Daphne Ippolito', 'David Luan', 'Hyeontaek Lim', 'Barret Zoph', 'Alexander Spiridonov', 'Ryan Sepassi', 'David Dohan', 'Shivani Agrawal', 'Mark Omernick', 'Andrew M Dai', 'Thanumalayan Sankaranarayana Pillai', 'Marie Pellat', 'Aitor Lewkowycz', 'Erica Moreira', 'Rewon Child', 'Oleksandr Polozov', 'Katherine Lee', 'Zongwei Zhou', 'Xuezhi Wang', 'Brennan Saeta', 'Mark Diaz', 'Orhan Firat', 'Michele Catasta', 'Jason Wei', 'Kathy Meier-Hellstern', 'Douglas Eck', 'Jeff Dean', 'Slav Petrov', 'Noah Fiedel']",
          "year": "2022",
          "summary": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",
          "primary_category": "cs.CL",
          "Topic": "Foundational Language Modeling"
        }
      },
      {
        "id": "360287970189639820",
        "label": "Paper",
        "properties": {
          "title": "EXAMS: A Multi-Subject High School Examinations Dataset for Cross-Lingual and Multilingual Question Answering",
          "authors": "['Momchil Hardalov', 'Todor Mihaylov', 'Dimitrina Zlatkova', 'Yoan Dinkov', 'Ivan Koychev', 'Preslav Nakov']",
          "year": "2020",
          "summary": "We propose EXAMS -- a new benchmark dataset for cross-lingual and multilingual question answering for high school examinations. We collected more than 24,000 high-quality high school exam questions in 16 languages, covering 8 language families and 24 school subjects from Natural Sciences and Social Sciences, among others.   EXAMS offers a fine-grained evaluation framework across multiple languages and subjects, which allows precise analysis and comparison of various models. We perform various experiments with existing top-performing multilingual pre-trained models and we show that EXAMS offers multiple challenges that require multilingual knowledge and reasoning in multiple domains. We hope that EXAMS will enable researchers to explore challenging reasoning and knowledge transfer methods and pre-trained models for school question answering in various languages which was not possible before. The data, code, pre-trained models, and evaluation are available at https://github.com/mhardalov/exams-qa.",
          "primary_category": "cs.CL",
          "Topic": "Cross-lingual School Question Answering"
        }
      },
      {
        "id": "360287970189639893",
        "label": "Paper",
        "properties": {
          "title": "Adversarial Examples for Evaluating Reading Comprehension Systems",
          "authors": "[]",
          "year": "2017",
          "summary": "",
          "primary_category": "",
          "Topic": "Adversarial evaluation in NLP"
        }
      },
      {
        "id": "360287970189639877",
        "label": "Paper",
        "properties": {
          "title": "CyberSecEval 2: A Wide-Ranging Cybersecurity Evaluation Suite for Large Language Models",
          "authors": "['Manish Bhatt', 'Sahana Chennabasappa', 'Yue Li', 'Cyrus Nikolaidis', 'Daniel Song', 'Shengye Wan', 'Faizan Ahmad', 'Cornelius Aschermann', 'Yaohui Chen', 'Dhaval Kapil', 'David Molnar', 'Spencer Whitman', 'Joshua Saxe']",
          "year": "2024",
          "summary": "Large language models (LLMs) introduce new security risks, but there are few comprehensive evaluation suites to measure and reduce these risks. We present BenchmarkName, a novel benchmark to quantify LLM security risks and capabilities. We introduce two new areas for testing: prompt injection and code interpreter abuse. We evaluated multiple state-of-the-art (SOTA) LLMs, including GPT-4, Mistral, Meta Llama 3 70B-Instruct, and Code Llama. Our results show that conditioning away risk of attack remains an unsolved problem; for example, all tested models showed between 26% and 41% successful prompt injection tests. We further introduce the safety-utility tradeoff: conditioning an LLM to reject unsafe prompts can cause the LLM to falsely reject answering benign prompts, which lowers utility. We propose quantifying this tradeoff using False Refusal Rate (FRR). As an illustration, we introduce a novel test set to quantify FRR for cyberattack helpfulness risk. We find many LLMs able to successfully comply with \"borderline\"\" benign requests while still rejecting most unsafe requests. Finally, we quantify the utility of LLMs for automating a core cybersecurity task, that of exploiting software vulnerabilities. This is important because the offensive capabilities of LLMs are of intense interest; we quantify this by creating novel test sets for four representative problems. We find that models with coding capabilities perform better than those without, but that further work is needed for LLMs to become proficient at exploit generation. Our code is open source and can be used to evaluate other LLMs.\"",
          "primary_category": "cs.CR",
          "Topic": "Cybersecurity Evaluation for LLMs"
        }
      },
      {
        "id": "360287970189639898",
        "label": "Paper",
        "properties": {
          "title": "Demystifying CLIP Data",
          "authors": "['Hu Xu', 'Saining Xie', 'Xiaoqing Ellen Tan', 'Po-Yao Huang', 'Russell Howes', 'Vasu Sharma', 'Shang-Wen Li', 'Gargi Ghosh', 'Luke Zettlemoyer', 'Christoph Feichtenhofer']",
          "year": "2023",
          "summary": "Contrastive Language-Image Pre-training (CLIP) is an approach that has advanced research and applications in computer vision, fueling modern recognition systems and generative models. We believe that the main ingredient to the success of CLIP is its data and not the model architecture or pre-training objective. However, CLIP only provides very limited information about its data and how it has been collected, leading to works that aim to reproduce CLIP's data by filtering with its model parameters. In this work, we intend to reveal CLIP's data curation approach and in our pursuit of making it open to the community introduce Metadata-Curated Language-Image Pre-training (MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's concepts) and yields a balanced subset over the metadata distribution. Our experimental study rigorously isolates the model and training settings, concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M image-text data pairs outperforms CLIP's data on multiple standard benchmarks. In zero-shot ImageNet classification, MetaCLIP achieves 70.8% accuracy, surpassing CLIP's 68.3% on ViT-B models. Scaling to 1B data, while maintaining the same training budget, attains 72.4%. Our observations hold across various model sizes, exemplified by ViT-H achieving 80.5%, without any bells-and-whistles. Curation code and training data distribution on metadata is made available at https://github.com/facebookresearch/MetaCLIP.",
          "primary_category": "cs.CV",
          "Topic": "Metadata Curated Image-Text Data"
        }
      },
      {
        "id": "360287970189639900",
        "label": "Paper",
        "properties": {
          "title": "RACE: Large-scale ReAding Comprehension Dataset From Examinations",
          "authors": "['Guokun Lai', 'Qizhe Xie', 'Hanxiao Liu', 'Yiming Yang', 'Eduard Hovy']",
          "year": "2017",
          "summary": "We present RACE, a new dataset for benchmark evaluation of methods in the reading comprehension task. Collected from the English exams for middle and high school Chinese students in the age range between 12 to 18, RACE consists of near 28,000 passages and near 100,000 questions generated by human experts (English instructors), and covers a variety of topics which are carefully designed for evaluating the students' ability in understanding and reasoning. In particular, the proportion of questions that requires reasoning is much larger in RACE than that in other benchmark datasets for reading comprehension, and there is a significant gap between the performance of the state-of-the-art models (43%) and the ceiling human performance (95%). We hope this new dataset can serve as a valuable resource for research and evaluation in machine comprehension. The dataset is freely available at http://www.cs.cmu.edu/~glai1/data/race/ and the code is available at https://github.com/qizhex/RACE_AR_baselines.",
          "primary_category": "cs.CL",
          "Topic": "Reading comprehension evaluation"
        }
      },
      {
        "id": "360287970189639878",
        "label": "Paper",
        "properties": {
          "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge",
          "authors": "['Peter Clark', 'Isaac Cowhey', 'Oren Etzioni', 'Tushar Khot', 'Ashish Sabharwal', 'Carissa Schoenick', 'Oyvind Tafjord']",
          "year": "2018",
          "summary": "We present a new question set, text corpus, and baselines assembled to encourage AI research in advanced question answering. Together, these constitute the AI2 Reasoning Challenge (ARC), which requires far more powerful knowledge and reasoning than previous challenges such as SQuAD or SNLI. The ARC question set is partitioned into a Challenge Set and an Easy Set, where the Challenge Set contains only questions answered incorrectly by both a retrieval-based algorithm and a word co-occurence algorithm. The dataset contains only natural, grade-school science questions (authored for human tests), and is the largest public-domain set of this kind (7,787 questions). We test several baselines on the Challenge Set, including leading neural models from the SQuAD and SNLI tasks, and find that none are able to significantly outperform a random baseline, reflecting the difficult nature of this task. We are also releasing the ARC Corpus, a corpus of 14M science sentences relevant to the task, and implementations of the three neural baseline models tested. Can your model perform better? We pose ARC as a challenge to the community.",
          "primary_category": "cs.AI",
          "Topic": "Advanced Question Answering Challenges"
        }
      },
      {
        "id": "360287970189639905",
        "label": "Paper",
        "properties": {
          "title": "Crosslingual Generalization through Multitask Finetuning",
          "authors": "['Niklas Muennighoff', 'Thomas Wang', 'Lintang Sutawika', 'Adam Roberts', 'Stella Biderman', 'Teven Le Scao', 'M Saiful Bari', 'Sheng Shen', 'Zheng-Xin Yong', 'Hailey Schoelkopf', 'Xiangru Tang', 'Dragomir Radev', 'Alham Fikri Aji', 'Khalid Almubarak', 'Samuel Albanie', 'Zaid Alyafeai', 'Albert Webson', 'Edward Raff', 'Colin Raffel']",
          "year": "2022",
          "summary": "Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.",
          "primary_category": "cs.CL",
          "Topic": "Crosslingual Multitask Finetuning"
        }
      },
      {
        "id": "360287970189639759",
        "label": "Paper",
        "properties": {
          "title": "GAIA: a benchmark for General AI Assistants",
          "authors": "['Grégoire Mialon', 'Clémentine Fourrier', 'Craig Swift', 'Thomas Wolf', 'Yann LeCun', 'Thomas Scialom']",
          "year": "2023",
          "summary": "We introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a milestone in AI research. GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA questions are conceptually simple for humans yet challenging for most advanced AIs: we show that human respondents obtain 92\\% vs. 15\\% for GPT-4 equipped with plugins. This notable performance disparity contrasts with the recent trend of LLMs outperforming humans on tasks requiring professional skills in e.g. law or chemistry. GAIA's philosophy departs from the current trend in AI benchmarks suggesting to target tasks that are ever more difficult for humans. We posit that the advent of Artificial General Intelligence (AGI) hinges on a system's capability to exhibit similar robustness as the average human does on such questions. Using GAIA's methodology, we devise 466 questions and their answer. We release our questions while retaining answers to 300 of them to power a leader-board available at https://huggingface.co/gaia-benchmark.",
          "primary_category": "cs.CL",
          "Topic": "Challenging real-world AI tasks"
        }
      },
      {
        "id": "360287970189639913",
        "label": "Paper",
        "properties": {
          "title": "A Self-Supervised Descriptor for Image Copy Detection",
          "authors": "['Ed Pizzi', 'Sreya Dutta Roy', 'Sugosh Nagavara Ravindra', 'Priya Goyal', 'Matthijs Douze']",
          "year": "2022",
          "summary": "Image copy detection is an important task for content moderation. We introduce SSCD, a model that builds on a recent self-supervised contrastive training objective. We adapt this method to the copy detection task by changing the architecture and training objective, including a pooling operator from the instance matching literature, and adapting contrastive learning to augmentations that combine images.   Our approach relies on an entropy regularization term, promoting consistent separation between descriptor vectors, and we demonstrate that this significantly improves copy detection accuracy. Our method produces a compact descriptor vector, suitable for real-world web scale applications. Statistical information from a background image distribution can be incorporated into the descriptor.   On the recent DISC2021 benchmark, SSCD is shown to outperform both baseline copy detection models and self-supervised architectures designed for image classification by huge margins, in all settings. For example, SSCD out-performs SimCLR descriptors by 48% absolute. Code is available at https://github.com/facebookresearch/sscd-copy-detection.",
          "primary_category": "cs.CV",
          "Topic": "Image Copy Detection"
        }
      },
      {
        "id": "360287970189639907",
        "label": "Paper",
        "properties": {
          "title": "AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models",
          "authors": "['Wanjun Zhong', 'Ruixiang Cui', 'Yiduo Guo', 'Yaobo Liang', 'Shuai Lu', 'Yanlin Wang', 'Amin Saied', 'Weizhu Chen', 'Nan Duan']",
          "year": "2023",
          "summary": "Evaluating the general abilities of foundation models to tackle human-level tasks is a vital aspect of their development and application in the pursuit of Artificial General Intelligence (AGI). Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities. In this paper, we introduce AGIEval, a novel benchmark specifically designed to assess foundation model in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5% accuracy on the English test of the Chinese national college entrance exam. This demonstrates the extraordinary performance of contemporary foundation models. In contrast, we also find that GPT-4 is less proficient in tasks that require complex reasoning or specific domain knowledge. Our comprehensive analyses of model capabilities (understanding, knowledge, reasoning, and calculation) reveal these models' strengths and limitations, providing valuable insights into future directions for enhancing their general capabilities. By concentrating on tasks pertinent to human cognition and decision-making, our benchmark delivers a more meaningful and robust evaluation of foundation models' performance in real-world scenarios. The data, code, and all model outputs are released in https://github.com/ruixiangcui/AGIEval.",
          "primary_category": "cs.CL",
          "Topic": "Human-centric AI evaluation benchmark"
        }
      }
    ]
  },
  {
    "category_id": 5,
    "name": "Safety and Robustness",
    "description": "Papers addressing safety concerns, adversarial attacks, and robustness of AI models.",
    "children": [
      {
        "id": "360287970189639703",
        "label": "Paper",
        "properties": {
          "title": "Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning",
          "authors": "['Lucas Weber', 'Elia Bruni', 'Dieuwke Hupkes']",
          "year": "2023",
          "summary": "Finding the best way of adapting pre-trained language models to a task is a big challenge in current NLP. Just like the previous generation of task-tuned models (TT), models that are adapted to tasks via in-context-learning (ICL) are robust in some setups but not in others. Here, we present a detailed analysis of which design choices cause instabilities and inconsistencies in LLM predictions. First, we show how spurious correlations between input distributions and labels -- a known issue in TT models -- form only a minor problem for prompted models. Then, we engage in a systematic, holistic evaluation of different factors that have been found to influence predictions in a prompting setup. We test all possible combinations of a range of factors on both vanilla and instruction-tuned (IT) LLMs of different scale and statistically analyse the results to show which factors are the most influential, interactive or stable. Our results show which factors can be used without precautions and which should be avoided or handled with care in most settings.",
          "primary_category": "cs.CL",
          "Topic": "In-context learning instabilities"
        }
      },
      {
        "id": "360287970189639723",
        "label": "Paper",
        "properties": {
          "title": "Jailbreaking Black Box Large Language Models in Twenty Queries",
          "authors": "['Patrick Chao', 'Alexander Robey', 'Edgar Dobriban', 'Hamed Hassani', 'George J Pappas', 'Eric Wong']",
          "year": "2023",
          "summary": "There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and Gemini.",
          "primary_category": "cs.LG",
          "Topic": "Automated LLM jailbreak generation"
        }
      },
      {
        "id": "360287970189639726",
        "label": "Paper",
        "properties": {
          "title": "Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts",
          "authors": "['Mikayel Samvelyan', 'Sharath Chandra Raparthy', 'Andrei Lupu', 'Eric Hambro', 'Aram H Markosyan', 'Manish Bhatt', 'Yuning Mao', 'Minqi Jiang', 'Jack Parker-Holder', 'Jakob Foerster', 'Tim Rocktäschel', 'Roberta Raileanu']",
          "year": "2024",
          "summary": "As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to adversarial attacks is of paramount importance. Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel black-box approach for producing a diverse collection of adversarial prompts. Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem and uses open-ended search to generate prompts that are both effective and diverse. Focusing on the safety domain, we use Rainbow Teaming to target various state-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach reveals hundreds of effective adversarial prompts, with an attack success rate exceeding 90% across all tested models. Furthermore, we demonstrate that prompts generated by Rainbow Teaming are highly transferable and that fine-tuning models with synthetic data generated by our method significantly enhances their safety without sacrificing general performance or helpfulness. We additionally explore the versatility of Rainbow Teaming by applying it to question answering and cybersecurity, showcasing its potential to drive robust open-ended self-improvement in a wide range of applications.",
          "primary_category": "cs.CL",
          "Topic": "Diverse Adversarial Prompt Generation"
        }
      },
      {
        "id": "360287970189639753",
        "label": "Paper",
        "properties": {
          "title": "Large Language Models Are Not Robust Multiple Choice Selectors",
          "authors": "['Chujie Zheng', 'Hao Zhou', 'Fandong Meng', 'Jie Zhou', 'Minlie Huang']",
          "year": "2023",
          "summary": "Multiple choice questions (MCQs) serve as a common yet important task format in the evaluation of large language models (LLMs). This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent \"selection bias\"\", namely, they prefer to select specific option IDs as answers (like \"\"Option A\"\"). Through extensive empirical analyses with 20 LLMs on three benchmarks, we pinpoint that this behavioral bias primarily stems from LLMs' token bias, where the model a priori assigns more probabilistic mass to specific option ID tokens (e.g., A/B/C/D) when predicting answers from the option IDs. To mitigate selection bias, we propose a label-free, inference-time debiasing method, called PriDe, which separates the model's prior bias for option IDs from the overall prediction distribution. PriDe first estimates the prior by permutating option contents on a small number of test samples, and then applies the estimated prior to debias the remaining samples. We demonstrate that it achieves interpretable and transferable debiasing with high computational efficiency. We hope this work can draw broader research attention to the bias and robustness of modern LLMs.\"",
          "primary_category": "cs.CL",
          "Topic": "Selection Bias in LLMs"
        }
      },
      {
        "id": "360287970189639694",
        "label": "Paper",
        "properties": {
          "title": "Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning",
          "authors": "['Yuxi Xie', 'Anirudh Goyal', 'Wenyue Zheng', 'Min-Yen Kan', 'Timothy P Lillicrap', 'Kenji Kawaguchi', 'Michael Shieh']",
          "year": "2024",
          "summary": "We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to $81.8\\%$ (+$5.9\\%$), $34.7\\%$ (+$5.8\\%$), and $76.4\\%$ (+$15.8\\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains. Our code is publicly available at https://github.com/YuxiXie/MCTS-DPO.",
          "primary_category": "cs.AI",
          "Topic": "Iterative Preference Learning"
        }
      },
      {
        "id": "360287970189639713",
        "label": "Paper",
        "properties": {
          "title": "ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning",
          "authors": "['Ahmed Masry', 'Do Xuan Long', 'Jia Qing Tan', 'Shafiq Joty', 'Enamul Hoque']",
          "year": "2022",
          "summary": "Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.",
          "primary_category": "cs.CL",
          "Topic": "Complex Visual and Logical Reasoning"
        }
      },
      {
        "id": "360287970189639813",
        "label": "Paper",
        "properties": {
          "title": "Measuring Mathematical Problem Solving With the MATH Dataset",
          "authors": "['Dan Hendrycks', 'Collin Burns', 'Saurav Kadavath', 'Akul Arora', 'Steven Basart', 'Eric Tang', 'Dawn Song', 'Jacob Steinhardt']",
          "year": "2021",
          "summary": "Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",
          "primary_category": "cs.LG",
          "Topic": "Mathematical Problem Solving with MATH"
        }
      },
      {
        "id": "360287970189639814",
        "label": "Paper",
        "properties": {
          "title": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection",
          "authors": "['Thomas Hartvigsen', 'Saadia Gabriel', 'Hamid Palangi', 'Maarten Sap', 'Dipankar Ray', 'Ece Kamar']",
          "year": "2022",
          "summary": "Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language. To help mitigate these issues, we create ToxiGen, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model. Controlling machine generation in this way allows ToxiGen to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of ToxiGen and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5% of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that ToxiGen can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset. Our code and data can be found at https://github.com/microsoft/ToxiGen.",
          "primary_category": "cs.CL",
          "Topic": "Implicit hate speech detection"
        }
      },
      {
        "id": "360287970189639819",
        "label": "Paper",
        "properties": {
          "title": "Scalable Extraction of Training Data from (Production) Language Models",
          "authors": "['Milad Nasr', 'Nicholas Carlini', 'Jonathan Hayase', 'Matthew Jagielski', 'A Feder Cooper', 'Daphne Ippolito', 'Christopher A Choquette-Choo', 'Eric Wallace', 'Florian Tramèr', 'Katherine Lee']",
          "year": "2023",
          "summary": "This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.",
          "primary_category": "cs.LG",
          "Topic": "Extractable Memorization in LLMs"
        }
      },
      {
        "id": "360287970189639802",
        "label": "Paper",
        "properties": {
          "title": "The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions",
          "authors": "['Eric Wallace', 'Kai Xiao', 'Reimar Leike', 'Lilian Weng', 'Johannes Heidecke', 'Alex Beutel']",
          "year": "2024",
          "summary": "Today's LLMs are susceptible to prompt injections, jailbreaks, and other attacks that allow adversaries to overwrite a model's original instructions with their own malicious prompts. In this work, we argue that one of the primary vulnerabilities underlying these attacks is that LLMs often consider system prompts (e.g., text from an application developer) to be the same priority as text from untrusted users and third parties. To address this, we propose an instruction hierarchy that explicitly defines how models should behave when instructions of different priorities conflict. We then propose a data generation method to demonstrate this hierarchical instruction following behavior, which teaches LLMs to selectively ignore lower-privileged instructions. We apply this method to GPT-3.5, showing that it drastically increases robustness -- even for attack types not seen during training -- while imposing minimal degradations on standard capabilities.",
          "primary_category": "cs.CR",
          "Topic": "Hierarchical Instruction Prioritization"
        }
      },
      {
        "id": "360287970189639849",
        "label": "Paper",
        "properties": {
          "title": "Illuminating search spaces by mapping elites",
          "authors": "['Jean-Baptiste Mouret', 'Jeff Clune']",
          "year": "2015",
          "summary": "Many fields use search algorithms, which automatically explore a search space to find high-performing solutions: chemists search through the space of molecules to discover new drugs; engineers search for stronger, cheaper, safer designs, scientists search for models that best explain data, etc. The goal of search algorithms has traditionally been to return the single highest-performing solution in a search space. Here we describe a new, fundamentally different type of algorithm that is more useful because it provides a holistic view of how high-performing solutions are distributed throughout a search space. It creates a map of high-performing solutions at each point in a space defined by dimensions of variation that a user gets to choose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites) algorithm illuminates search spaces, allowing researchers to understand how interesting attributes of solutions combine to affect performance, either positively or, equally of interest, negatively. For example, a drug company may wish to understand how performance changes as the size of molecules and their cost-to-produce vary. MAP-Elites produces a large diversity of high-performing, yet qualitatively different solutions, which can be more helpful than a single, high-performing solution. Interestingly, because MAP-Elites explores more of the search space, it also tends to find a better overall solution than state-of-the-art search algorithms. We demonstrate the benefits of this new algorithm in three different problem domains ranging from producing modular neural networks to designing simulated and real soft robots. Because MAP- Elites (1) illuminates the relationship between performance and dimensions of interest in solutions, (2) returns a set of high-performing, yet diverse solutions, and (3) improves finding a single, best solution, it will advance science and engineering.",
          "primary_category": "cs.AI",
          "Topic": "Illuminating high-dimensional search spaces."
        }
      },
      {
        "id": "360287970189639850",
        "label": "Paper",
        "properties": {
          "title": "Learning Video Representations from Large Language Models",
          "authors": "['Yue Zhao', 'Ishan Misra', 'Philipp Krähenbühl', 'Rohit Girdhar']",
          "year": "2022",
          "summary": "We introduce LaViLa, a new approach to learning video-language representations by leveraging Large Language Models (LLMs). We repurpose pre-trained LLMs to be conditioned on visual input, and finetune them to create automatic video narrators. Our auto-generated narrations offer a number of advantages, including dense coverage of long videos, better temporal synchronization of the visual information and text, and much higher diversity of text. The video-text embedding learned contrastively with these additional auto-generated narrations outperforms the previous state-of-the-art on multiple first-person and third-person video tasks, both in zero-shot and finetuned setups. Most notably, LaViLa obtains an absolute gain of 10.1% on EGTEA classification and 5.9% Epic-Kitchens-100 multi-instance retrieval benchmarks. Furthermore, LaViLa trained with only half the narrations from the Ego4D dataset outperforms baseline models trained on the full set, and shows positive scaling behavior on increasing pre-training data and model size.",
          "primary_category": "cs.CV",
          "Topic": "Dense Video Narration Learning"
        }
      },
      {
        "id": "360287970189639861",
        "label": "Paper",
        "properties": {
          "title": "Verb Semantics and Lexical Selection",
          "authors": "['Zhibiao Wu', 'Martha Palmer']",
          "year": "1994",
          "summary": "This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT). Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentence as well as selection restrictions placed on the verb arguments. A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT. We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems. Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection.",
          "primary_category": "cmp-lg",
          "Topic": "Verb semantics in translation"
        }
      },
      {
        "id": "360287970189639871",
        "label": "Paper",
        "properties": {
          "title": "WinoGrande: An Adversarial Winograd Schema Challenge at Scale",
          "authors": "['Keisuke Sakaguchi', 'Ronan Le Bras', 'Chandra Bhagavatula', 'Yejin Choi']",
          "year": "2019",
          "summary": "The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations. However, recent advances in neural language models have already reached around 90% accuracy on variants of WSC. This raises an important question whether these models have truly acquired robust commonsense capabilities or whether they rely on spurious biases in the datasets that lead to an overestimation of the true capabilities of machine commonsense. To investigate this question, we introduce WinoGrande, a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel AfLite algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. The best state-of-the-art methods on WinoGrande achieve 59.4-79.1%, which are 15-35% below human performance of 94.0%, depending on the amount of the training data allowed. Furthermore, we establish new state-of-the-art results on five related benchmarks - WSC (90.1%), DPR (93.1%), COPA (90.6%), KnowRef (85.6%), and Winogender (97.1%). These results have dual implications: on one hand, they demonstrate the effectiveness of WinoGrande when used as a resource for transfer learning. On the other hand, they raise a concern that we are likely to be overestimating the true capabilities of machine commonsense across all these benchmarks. We emphasize the importance of algorithmic bias reduction in existing and future benchmarks to mitigate such overestimation.",
          "primary_category": "cs.CL",
          "Topic": "Mitigating biases in commonsense reasoning."
        }
      },
      {
        "id": "360287970189639879",
        "label": "Paper",
        "properties": {
          "title": "TVQA: Localized, Compositional Video Question Answering",
          "authors": "['Jie Lei', 'Licheng Yu', 'Mohit Bansal', 'Tamara L Berg']",
          "year": "2018",
          "summary": "Recent years have witnessed an increasing interest in image-based question-answering (QA) tasks. However, due to data limitations, there has been much less work on video-based QA. In this paper, we present TVQA, a large-scale video QA dataset based on 6 popular TV shows. TVQA consists of 152,545 QA pairs from 21,793 clips, spanning over 460 hours of video. Questions are designed to be compositional in nature, requiring systems to jointly localize relevant moments within a clip, comprehend subtitle-based dialogue, and recognize relevant visual concepts. We provide analyses of this new dataset as well as several baselines and a multi-stream end-to-end trainable neural network framework for the TVQA task. The dataset is publicly available at http://tvqa.cs.unc.edu.",
          "primary_category": "cs.CL",
          "Topic": "Multimodal Video Question Answering"
        }
      },
      {
        "id": "360287970189639901",
        "label": "Paper",
        "properties": {
          "title": "Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs",
          "authors": "['Aly M Kassem', 'Omar Mahmoud', 'Niloofar Mireshghallah', 'Hyunwoo Kim', 'Yulia Tsvetkov', 'Yejin Choi', 'Sherif Saad', 'Santu Rana']",
          "year": "2024",
          "summary": "In this paper, we introduce a black-box prompt optimization method that uses an attacker LLM agent to uncover higher levels of memorization in a victim agent, compared to what is revealed by prompting the target model with the training data directly, which is the dominant approach of quantifying memorization in LLMs. We use an iterative rejection-sampling optimization process to find instruction-based prompts with two main characteristics: (1) minimal overlap with the training data to avoid presenting the solution directly to the model, and (2) maximal overlap between the victim model's output and the training data, aiming to induce the victim to spit out training data. We observe that our instruction-based prompts generate outputs with 23.7% higher overlap with training data compared to the baseline prefix-suffix measurements. Our findings show that (1) instruction-tuned models can expose pre-training data as much as their base-models, if not more so, (2) contexts other than the original training data can lead to leakage, and (3) using instructions proposed by other LLMs can open a new avenue of automated attacks that we should further study and explore. The code can be found at https://github.com/Alymostafa/Instruction_based_attack .",
          "primary_category": "cs.CL",
          "Topic": "Instruction-based Memorization Uncovering"
        }
      },
      {
        "id": "360287970189639714",
        "label": "Paper",
        "properties": {
          "title": "Quantifying Memorization Across Neural Language Models",
          "authors": "['Nicholas Carlini', 'Daphne Ippolito', 'Matthew Jagielski', 'Katherine Lee', 'Florian Tramer', 'Chiyuan Zhang']",
          "year": "2022",
          "summary": "Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others).   We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes more complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.",
          "primary_category": "cs.LG",
          "Topic": "Memorization in Language Models"
        }
      },
      {
        "id": "360287970189639840",
        "label": "Paper",
        "properties": {
          "title": "MuTox: Universal MUltilingual Audio-based TOXicity Dataset and Zero-shot Detector",
          "authors": "['Marta R Costa-jussà', 'Mariano Coria Meglioli', 'Pierre Andrews', 'David Dale', 'Prangthip Hansanti', 'Elahe Kalbassi', 'Alex Mourachko', 'Christophe Ropers', 'Carleigh Wood']",
          "year": "2024",
          "summary": "Research in toxicity detection in natural language processing for the speech modality (audio-based) is quite limited, particularly for languages other than English. To address these limitations and lay the groundwork for truly multilingual audio-based toxicity detection, we introduce MuTox, the first highly multilingual audio-based dataset with toxicity labels. The dataset comprises 20,000 audio utterances for English and Spanish, and 4,000 for the other 19 languages. To demonstrate the quality of this dataset, we trained the MuTox audio-based toxicity classifier, which enables zero-shot toxicity detection across a wide range of languages. This classifier outperforms existing text-based trainable classifiers by more than 1% AUC, while expanding the language coverage more than tenfold. When compared to a wordlist-based classifier that covers a similar number of languages, MuTox improves precision and recall by approximately 2.5 times. This significant improvement underscores the potential of MuTox in advancing the field of audio-based toxicity detection.",
          "primary_category": "cs.SD",
          "Topic": "Multilingual Audio Toxicity Detection"
        }
      }
    ]
  }
]